{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05b284b",
   "metadata": {},
   "source": [
    "# Commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1f9136d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0166ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_month_info(input='1949-01'):\n",
    "    year, month = input.split('-')\n",
    "    diff = (int(year) - 1949)*12 + int(month)\n",
    "    return torch.tensor(diff).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdcb2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirlinePassengersDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super().__init__()\n",
    "        URL = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\"\n",
    "        self.data = pd.read_csv(URL, index_col=False)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        passengers_frame = self.data.Passengers.to_frame()\n",
    "        self.scaler.fit(passengers_frame)\n",
    "        scaled_passengers = self.scaler.transform(passengers_frame)\n",
    "        self.passengers = scaled_passengers\n",
    "        self.transform = transform\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        month, passenger = tokenize_month_info(self.data.Month[index]), torch.tensor(self.passengers[index]).to(torch.float32)\n",
    "        return (month, passenger)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "818d546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AirlinePassengersDataset()\n",
    "split = int(len(dataset) * 0.9)\n",
    "train_data, test_data = Subset(dataset, list(range(0, split))), Subset(dataset, list(range(split, len(dataset))))\n",
    "loader = DataLoader(train_data, batch_size=12, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=len(dataset)-split, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa4bd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "tensor([0.0154, 0.0154])\n"
     ]
    }
   ],
   "source": [
    "print(len(loader))\n",
    "for word in loader:\n",
    "    print(torch.concat((word[1][0],word[1][0])))\n",
    "    # print(word)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e45e4",
   "metadata": {},
   "source": [
    "<img src=\"../images/LSTM_figure4_v3a.png\" style=\"width:500;height:400px;\">\n",
    "\n",
    "$$\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)\\tag{1} $$\n",
    "\n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right) \\tag{3}$$\n",
    "\n",
    "$$\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2} $$ \n",
    "\n",
    "$$ \\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle} \\tag{4} $$\n",
    "\n",
    "$$ \\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})\\tag{5}$$ \n",
    "\n",
    "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c9277",
   "metadata": {},
   "source": [
    "# Basic LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c24f48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64):\n",
    "        super().__init__()\n",
    "        combined_size = input_size + hidden_size\n",
    "        # forget gate\n",
    "        self.forget = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # update gate\n",
    "        self.update = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # tanh\n",
    "        self.candidate = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # output gate\n",
    "        self.output = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "\n",
    "        # final fc\n",
    "        self.final = nn.Linear(hidden_size, input_size, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, prev_a, prev_c): # hidden state and long-term memory\n",
    "        # print(\"x, prev_a, prev_c\", x, prev_a, prev_c)\n",
    "        input_vector = torch.concat((x, prev_a))\n",
    "        # print(\"input_vector\", input_vector)\n",
    "        forget = torch.sigmoid(self.forget(input_vector))\n",
    "        # print(\"forget\", forget)\n",
    "        update = torch.sigmoid(self.update(input_vector))\n",
    "        # print(\"update\", update)\n",
    "        candidate = torch.tanh(self.candidate(input_vector))\n",
    "        # print(\"candidate\", candidate)\n",
    "        output = torch.sigmoid(self.output(input_vector))\n",
    "\n",
    "        # print(\"output\", output)\n",
    "        c = (forget * prev_c) + (update * candidate)\n",
    "        # print(\"c\", c)\n",
    "\n",
    "        a = output * torch.tanh(c)\n",
    "        # print(\"a\", a)\n",
    "\n",
    "        x = self.final(a)\n",
    "        # print(\"x\", x)\n",
    "\n",
    "        return x, a, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3184c2",
   "metadata": {},
   "source": [
    "# Non Connected LSTM with no depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e62355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, cells=12, input_size=1, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.cells = nn.Sequential(*(LSTMCell(input_size, hidden_size) for _ in range(cells)))\n",
    "        # final fc\n",
    "        # self.final = nn.Linear(hidden_size, input_size, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, seq, prev_a, prev_c): # hidden state and long-term memory\n",
    "        output = []\n",
    "        for c in range(len(seq)):\n",
    "            x, prev_a, prev_c = self.cells[c](seq[c], prev_a, prev_c)\n",
    "            output.append(x)\n",
    "        # x = self.final(prev_a)\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output, prev_a, prev_c\n",
    "\n",
    "    def predict(self, x, prev_a, prev_c): # hidden state and long-term memory\n",
    "        output = []\n",
    "        for c in range(12):\n",
    "            x, prev_a, prev_c = self.cells[c](x, prev_a, prev_c)\n",
    "            output.append(x)\n",
    "        # x = self.final(prev_a)\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output, prev_a, prev_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919213b7",
   "metadata": {},
   "source": [
    "## Batch 12, Sequence Loop inside the Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "993f1ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH_LOSS 0 0.11366112953559919\n",
      "EPOCH_LOSS 1 0.09986566855910826\n",
      "EPOCH_LOSS 2 0.08855088884857568\n",
      "EPOCH_LOSS 3 0.07927130039950664\n",
      "EPOCH_LOSS 4 0.07166145437143066\n",
      "EPOCH_LOSS 5 0.06542149630629203\n",
      "EPOCH_LOSS 6 0.06030538786118003\n",
      "EPOCH_LOSS 7 0.05611120105128397\n",
      "EPOCH_LOSS 8 0.052673244044523344\n",
      "EPOCH_LOSS 9 0.0498555169694803\n",
      "EPOCH_LOSS 10 0.04754642495589161\n",
      "EPOCH_LOSS 11 0.04565438430290669\n",
      "EPOCH_LOSS 12 0.04410423557485708\n",
      "EPOCH_LOSS 13 0.04283431022089313\n",
      "EPOCH_LOSS 14 0.04179401134288954\n",
      "EPOCH_LOSS 15 0.04094182598849081\n",
      "EPOCH_LOSS 16 0.040243695955723524\n",
      "EPOCH_LOSS 17 0.039671691848938775\n",
      "EPOCH_LOSS 18 0.0392029130439782\n",
      "EPOCH_LOSS 19 0.03881857034072957\n",
      "EPOCH_LOSS 20 0.03850326271177354\n",
      "EPOCH_LOSS 21 0.03824437809684737\n",
      "EPOCH_LOSS 22 0.03803156971910291\n",
      "EPOCH_LOSS 23 0.03785636536882852\n",
      "EPOCH_LOSS 24 0.037711826121349906\n",
      "EPOCH_LOSS 25 0.03759227144870569\n",
      "EPOCH_LOSS 26 0.03749305524185977\n",
      "EPOCH_LOSS 27 0.03741037769412452\n",
      "EPOCH_LOSS 28 0.03734112033535811\n",
      "EPOCH_LOSS 29 0.037282756053503945\n",
      "EPOCH_LOSS 30 0.03723319845316424\n",
      "EPOCH_LOSS 31 0.03719076058107682\n",
      "EPOCH_LOSS 32 0.037154062153686856\n",
      "EPOCH_LOSS 33 0.03712198082145981\n",
      "EPOCH_LOSS 34 0.03709359758067876\n",
      "EPOCH_LOSS 35 0.03706817128907212\n",
      "EPOCH_LOSS 36 0.03704509777228602\n",
      "EPOCH_LOSS 37 0.037023889779282566\n",
      "EPOCH_LOSS 38 0.03700414731759916\n",
      "EPOCH_LOSS 39 0.03698555526154285\n",
      "EPOCH_LOSS 40 0.03696785100989721\n",
      "EPOCH_LOSS 41 0.03695083506913348\n",
      "EPOCH_LOSS 42 0.03693433264693753\n",
      "EPOCH_LOSS 43 0.03691821924241429\n",
      "EPOCH_LOSS 44 0.03690238591198894\n",
      "EPOCH_LOSS 45 0.036886747693642974\n",
      "EPOCH_LOSS 46 0.03687123983929103\n",
      "EPOCH_LOSS 47 0.03685581796294586\n",
      "EPOCH_LOSS 48 0.036840435710142956\n",
      "EPOCH_LOSS 49 0.03682506147941405\n",
      "EPOCH_LOSS 50 0.036809676834805446\n",
      "EPOCH_LOSS 51 0.03679426452568309\n",
      "EPOCH_LOSS 52 0.03677880941805514\n",
      "EPOCH_LOSS 53 0.03676330649547956\n",
      "EPOCH_LOSS 54 0.036747744899581776\n",
      "EPOCH_LOSS 55 0.036732125117189506\n",
      "EPOCH_LOSS 56 0.03671644613231448\n",
      "EPOCH_LOSS 57 0.036700706082311546\n",
      "EPOCH_LOSS 58 0.03668490401469171\n",
      "EPOCH_LOSS 59 0.03666904297741977\n",
      "EPOCH_LOSS 60 0.036653127119114455\n",
      "EPOCH_LOSS 61 0.03663715070367537\n",
      "EPOCH_LOSS 62 0.03662112674845213\n",
      "EPOCH_LOSS 63 0.03660504858602177\n",
      "EPOCH_LOSS 64 0.03658892212181606\n",
      "EPOCH_LOSS 65 0.036572752689773384\n",
      "EPOCH_LOSS 66 0.03655654247003523\n",
      "EPOCH_LOSS 67 0.03654028989628635\n",
      "EPOCH_LOSS 68 0.036524002186276695\n",
      "EPOCH_LOSS 69 0.03650767800652168\n",
      "EPOCH_LOSS 70 0.036491322331130505\n",
      "EPOCH_LOSS 71 0.03647493219680407\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(passengers)<\u001b[32m2\u001b[39m:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m logits, a, c = \u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpassengers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_prev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m loss = criterion(logits, passengers)\n\u001b[32m     28\u001b[39m batch_loss = loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mosesopposite/andrej/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mosesopposite/andrej/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, seq, prev_a, prev_c)\u001b[39m\n\u001b[32m     10\u001b[39m output = []\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq)):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     x, prev_a, prev_c = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcells\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     output.append(x)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# x = self.final(prev_a)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mosesopposite/andrej/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mosesopposite/andrej/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mLSTMCell.forward\u001b[39m\u001b[34m(self, x, prev_a, prev_c)\u001b[39m\n\u001b[32m     20\u001b[39m input_vector = torch.concat((x, prev_a))\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print(\"input_vector\", input_vector)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m forget = torch.sigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_vector\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# print(\"forget\", forget)\u001b[39;00m\n\u001b[32m     24\u001b[39m update = torch.sigmoid(\u001b[38;5;28mself\u001b[39m.update(input_vector))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/mosesopposite/andrej/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "initial_lr = 5e-2\n",
    "cell = LSTM()\n",
    "optimizer = optim.SGD(cell.parameters(), lr=initial_lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50, threshold=0.0005)\n",
    "a0 = torch.zeros((64), requires_grad=True).to(torch.float32)\n",
    "c0 = torch.zeros((64), requires_grad=True).to(torch.float32)\n",
    "\n",
    "EPOCH=10000\n",
    "\n",
    "for ep in range(EPOCH):\n",
    "    epoch_loss = 0\n",
    "    a_prev = a0\n",
    "    c_prev = c0\n",
    "    for batch in loader:\n",
    "        batch_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        passengers = torch.FloatTensor(batch[1])\n",
    "        \n",
    "        if len(passengers)<2:\n",
    "            continue\n",
    "\n",
    "        logits, a, c = cell(passengers, a_prev, c_prev)\n",
    "        loss = criterion(logits, passengers)\n",
    "        batch_loss = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        a_prev = a.detach()\n",
    "        c_prev = c.detach()\n",
    "        # print(\"Batch avg loss\", batch_loss.item() / 4)\n",
    "        epoch_loss += batch_loss #/ 12\n",
    "\n",
    "    print(\"EPOCH_LOSS\", ep,  epoch_loss / len(loader))\n",
    "    scheduler.step(epoch_loss / len(loader))\n",
    "    if initial_lr != optimizer.param_groups[0]['lr']:\n",
    "        print(f\"LR Changed {optimizer.param_groups[0]['lr']}\")\n",
    "        initial_lr = optimizer.param_groups[0]['lr']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f93d3c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[289.25415367]\n",
      " [283.39729691]\n",
      " [302.91521275]\n",
      " [298.85973901]\n",
      " [302.68631941]\n",
      " [320.0727191 ]\n",
      " [337.41569281]\n",
      " [338.76422918]\n",
      " [310.05142999]\n",
      " [282.24129832]\n",
      " [257.17725378]\n",
      " [275.70526421]]\n",
      "[[361.99999839 405.00000584 416.99999428 391.00001419 418.9999975\n",
      "  461.00000334 472.0000056  534.99999893 622.         606.00000513\n",
      "  508.00000179 461.00000334]]\n",
      "tensor(0.3481, dtype=torch.float64)\n",
      "tensor(171.7050, dtype=torch.float64)\n",
      "tensor(189.5110, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    output = []\n",
    "    labels = []\n",
    "    input, a_prev, c_prev = torch.tensor([test_loader.dataset[0][1]]), torch.zeros(64).to(torch.float32), torch.zeros(64).to(torch.float32)\n",
    "\n",
    "    logits, act, c = cell.predict(input, a_prev, c_prev)\n",
    "    output = dataset.scaler.inverse_transform(logits)\n",
    "    labels = dataset.scaler.inverse_transform(torch.FloatTensor(test_loader.dataset)[1:13,1].reshape(1, -1))\n",
    "    \n",
    "    print(output)\n",
    "    print(labels)\n",
    "\n",
    "    toutput = torch.tensor(output)\n",
    "    tlabels = torch.tensor(labels)\n",
    "    # mape (mean absolute percentage error)\n",
    "    # mean((actual-forecast) / actual)\n",
    "    mape = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)) / torch.squeeze(tlabels))) # 0.1580, 0.1418(12-batch), 0.1343(sliding window)\n",
    "    print(mape)\n",
    "\n",
    "    # mae (mean absolute error)\n",
    "    mae = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)))) # 82.5248, 71.5019, 69.0149\n",
    "    print(mae)\n",
    "\n",
    "    # rmse (root mean square error)\n",
    "    rmse = torch.sqrt(torch.mean(torch.square((torch.squeeze(toutput)-torch.squeeze(tlabels))))) # 112.3071, 94.6528, 91.1919\n",
    "    print(rmse)\n",
    "\n",
    "finally:\n",
    "    torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca3926",
   "metadata": {},
   "source": [
    "# Stacked LSTM with depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4adfbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64):\n",
    "        super().__init__()\n",
    "        combined_size = input_size + hidden_size\n",
    "        # forget gate\n",
    "        self.forget = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # update gate\n",
    "        self.update = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # tanh\n",
    "        self.candidate = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # output gate\n",
    "        self.output = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "\n",
    "        # final fc\n",
    "        self.final = nn.Linear(hidden_size, input_size, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, prev_a, prev_c): # hidden state and long-term memory\n",
    "        # print(\"x, prev_a, prev_c\", x, prev_a, prev_c)\n",
    "        input_vector = torch.concat((x, prev_a))\n",
    "        # print(\"input_vector\", input_vector)\n",
    "        forget = torch.sigmoid(self.forget(input_vector))\n",
    "        # print(\"forget\", forget)\n",
    "        update = torch.sigmoid(self.update(input_vector))\n",
    "        # print(\"update\", update)\n",
    "        candidate = torch.tanh(self.candidate(input_vector))\n",
    "        # print(\"candidate\", candidate)\n",
    "        output = torch.sigmoid(self.output(input_vector))\n",
    "\n",
    "        # print(\"output\", output)\n",
    "        c = (forget * prev_c) + (update * candidate)\n",
    "        # print(\"c\", c)\n",
    "\n",
    "        a = output * torch.tanh(c)\n",
    "        # print(\"a\", a)\n",
    "\n",
    "        # x = self.final(a)\n",
    "        # print(\"x\", x)\n",
    "\n",
    "        return x, a, c\n",
    "\n",
    "\n",
    "class LSTMStacked(nn.Module):\n",
    "\n",
    "    def __init__(self, cells=12, input_size=1, hidden_size=64, output_size=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = cells\n",
    "        self.cells = nn.Sequential(LSTMCell(input_size, hidden_size), *(LSTMCell(hidden_size, hidden_size) for _ in range(cells)))\n",
    "        self.final = nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x, prev_a_c_list): # hidden state and long-term memory\n",
    "        new_a_c_list = []\n",
    "        for c in range(self.num_layers):\n",
    "            prev_a, prev_c = prev_a_c_list[c]\n",
    "            _, a, c = self.cells[c](x, prev_a, prev_c)\n",
    "            new_a_c_list.append((a, c))\n",
    "            x = a\n",
    "        x = self.final(x)\n",
    "        return x, new_a_c_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4535bd",
   "metadata": {},
   "source": [
    "## 12 in 1 out Sliding Window Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9115a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH_LOSS 0 0.18668344166352707\n",
      "EPOCH_LOSS 1 0.17321746981440064\n",
      "EPOCH_LOSS 2 0.15971209474393508\n",
      "EPOCH_LOSS 3 0.14399932232052565\n",
      "EPOCH_LOSS 4 0.12450047377838679\n",
      "EPOCH_LOSS 5 0.09952798854707605\n",
      "EPOCH_LOSS 6 0.06923974128249061\n",
      "EPOCH_LOSS 7 0.04239686497321911\n",
      "EPOCH_LOSS 8 0.032829049885961456\n",
      "EPOCH_LOSS 9 0.03347955252379468\n",
      "EPOCH_LOSS 10 0.03392386715313478\n",
      "EPOCH_LOSS 11 0.033614963194978885\n",
      "EPOCH_LOSS 12 0.03311926538997955\n",
      "EPOCH_LOSS 13 0.03260644481508054\n",
      "EPOCH_LOSS 14 0.032110505073211615\n",
      "EPOCH_LOSS 15 0.03163348084030869\n",
      "EPOCH_LOSS 16 0.031171253289632005\n",
      "EPOCH_LOSS 17 0.030719364309294783\n",
      "EPOCH_LOSS 18 0.030274183572048516\n",
      "EPOCH_LOSS 19 0.029832931494570596\n",
      "EPOCH_LOSS 20 0.029393524895511444\n",
      "EPOCH_LOSS 21 0.02895438196605003\n",
      "EPOCH_LOSS 22 0.028514343273048248\n",
      "EPOCH_LOSS 23 0.02807250502607611\n",
      "EPOCH_LOSS 24 0.027628188051593474\n",
      "EPOCH_LOSS 25 0.02718089830107652\n",
      "EPOCH_LOSS 26 0.0267302714226323\n",
      "EPOCH_LOSS 27 0.026276073509524604\n",
      "EPOCH_LOSS 28 0.025818170945536296\n",
      "EPOCH_LOSS 29 0.025356521702812263\n",
      "EPOCH_LOSS 30 0.024891193553279862\n",
      "EPOCH_LOSS 31 0.02442231219309455\n",
      "EPOCH_LOSS 32 0.023950122808545464\n",
      "EPOCH_LOSS 33 0.02347491560262008\n",
      "EPOCH_LOSS 34 0.02299707535846509\n",
      "EPOCH_LOSS 35 0.022517078123620686\n",
      "EPOCH_LOSS 36 0.02203544586669552\n",
      "EPOCH_LOSS 37 0.021552812030613027\n",
      "EPOCH_LOSS 38 0.021069895457254136\n",
      "EPOCH_LOSS 39 0.020587477227445174\n",
      "EPOCH_LOSS 40 0.020106442261471277\n",
      "EPOCH_LOSS 41 0.01962775587089884\n",
      "EPOCH_LOSS 42 0.01915246267410946\n",
      "EPOCH_LOSS 43 0.018681678022751363\n",
      "EPOCH_LOSS 44 0.018216576271410573\n",
      "EPOCH_LOSS 45 0.017758361039771707\n",
      "EPOCH_LOSS 46 0.01730826636793649\n",
      "EPOCH_LOSS 47 0.01686751651747035\n",
      "EPOCH_LOSS 48 0.016437304746460525\n",
      "EPOCH_LOSS 49 0.016018786810785713\n",
      "EPOCH_LOSS 50 0.015613052980753862\n",
      "EPOCH_LOSS 51 0.015221101467687473\n",
      "EPOCH_LOSS 52 0.014843840831014395\n",
      "EPOCH_LOSS 53 0.014482042818319595\n",
      "EPOCH_LOSS 54 0.014136363425133558\n",
      "EPOCH_LOSS 55 0.013807298569051413\n",
      "EPOCH_LOSS 56 0.013495220584138582\n",
      "EPOCH_LOSS 57 0.013200319648071009\n",
      "EPOCH_LOSS 58 0.012922662225400538\n",
      "EPOCH_LOSS 59 0.012662151792484658\n",
      "EPOCH_LOSS 60 0.012418567259312013\n",
      "EPOCH_LOSS 61 0.012191546340968437\n",
      "EPOCH_LOSS 62 0.011980624839121644\n",
      "EPOCH_LOSS 63 0.01178524377197874\n",
      "EPOCH_LOSS 64 0.01160475784024341\n",
      "EPOCH_LOSS 65 0.011438468242525807\n",
      "EPOCH_LOSS 66 0.011285618635856035\n",
      "EPOCH_LOSS 67 0.011145425775311819\n",
      "EPOCH_LOSS 68 0.011017094918000882\n",
      "EPOCH_LOSS 69 0.010899823286614336\n",
      "EPOCH_LOSS 70 0.010792814621310237\n",
      "EPOCH_LOSS 71 0.01069529873548775\n",
      "EPOCH_LOSS 72 0.010606523887589023\n",
      "EPOCH_LOSS 73 0.01052577335298159\n",
      "EPOCH_LOSS 74 0.01045236149025432\n",
      "EPOCH_LOSS 75 0.010385645369321272\n",
      "EPOCH_LOSS 76 0.010325021249800167\n",
      "EPOCH_LOSS 77 0.010269927258916518\n",
      "EPOCH_LOSS 78 0.01021984371273119\n",
      "EPOCH_LOSS 79 0.010174288041669678\n",
      "EPOCH_LOSS 80 0.010132821819368223\n",
      "EPOCH_LOSS 81 0.010095043301954995\n",
      "EPOCH_LOSS 82 0.0100605816243194\n",
      "EPOCH_LOSS 83 0.010029102830182256\n",
      "EPOCH_LOSS 84 0.010000306750065196\n",
      "EPOCH_LOSS 85 0.009973916274637442\n",
      "EPOCH_LOSS 86 0.009949685243403656\n",
      "EPOCH_LOSS 87 0.00992738971301795\n",
      "EPOCH_LOSS 88 0.00990683318965573\n",
      "EPOCH_LOSS 89 0.009887830037019023\n",
      "EPOCH_LOSS 90 0.009870220026812634\n",
      "EPOCH_LOSS 91 0.009853859378225988\n",
      "EPOCH_LOSS 92 0.00983861625739407\n",
      "EPOCH_LOSS 93 0.00982437524388897\n",
      "EPOCH_LOSS 94 0.00981103283058287\n",
      "EPOCH_LOSS 95 0.009798495374183341\n",
      "EPOCH_LOSS 96 0.009786680162257195\n",
      "EPOCH_LOSS 97 0.009775512493247217\n",
      "EPOCH_LOSS 98 0.009764925478435528\n",
      "EPOCH_LOSS 99 0.009754860732098476\n",
      "EPOCH_LOSS 100 0.009745264289972317\n",
      "EPOCH_LOSS 101 0.009736089222377247\n",
      "EPOCH_LOSS 102 0.009727293278812686\n",
      "EPOCH_LOSS 103 0.009718837495123473\n",
      "EPOCH_LOSS 104 0.009710689894689525\n",
      "EPOCH_LOSS 105 0.009702819891377954\n",
      "EPOCH_LOSS 106 0.009695197942744278\n",
      "EPOCH_LOSS 107 0.009687804059654016\n",
      "EPOCH_LOSS 108 0.009680613186320543\n",
      "EPOCH_LOSS 109 0.009673605969240263\n",
      "EPOCH_LOSS 110 0.009666766305520403\n",
      "EPOCH_LOSS 111 0.00966007893101883\n",
      "EPOCH_LOSS 112 0.009653527660511703\n",
      "EPOCH_LOSS 113 0.009647103940769147\n",
      "EPOCH_LOSS 114 0.009640791870717915\n",
      "EPOCH_LOSS 115 0.009634584430835638\n",
      "EPOCH_LOSS 116 0.009628470347009206\n",
      "EPOCH_LOSS 117 0.009622442469667302\n",
      "EPOCH_LOSS 118 0.009616493064296775\n",
      "EPOCH_LOSS 119 0.009610615673173395\n",
      "EPOCH_LOSS 120 0.009604802323940561\n",
      "EPOCH_LOSS 121 0.009599050647327344\n",
      "EPOCH_LOSS 122 0.009593354001708698\n",
      "EPOCH_LOSS 123 0.009587707889602042\n",
      "EPOCH_LOSS 124 0.009582107055344367\n",
      "EPOCH_LOSS 125 0.009576551044330795\n",
      "EPOCH_LOSS 126 0.009571033164984907\n",
      "EPOCH_LOSS 127 0.009565552843964972\n",
      "EPOCH_LOSS 128 0.009560103339643336\n",
      "EPOCH_LOSS 129 0.009554685332470217\n",
      "EPOCH_LOSS 130 0.009549296205393316\n",
      "EPOCH_LOSS 131 0.009543933042298483\n",
      "EPOCH_LOSS 132 0.009538594653869812\n",
      "EPOCH_LOSS 133 0.00953327712024259\n",
      "EPOCH_LOSS 134 0.009527981974248253\n",
      "EPOCH_LOSS 135 0.009522704083180959\n",
      "EPOCH_LOSS 136 0.009517446118990573\n",
      "EPOCH_LOSS 137 0.00951220366468723\n",
      "EPOCH_LOSS 138 0.009506977825529811\n",
      "EPOCH_LOSS 139 0.009501764047689822\n",
      "EPOCH_LOSS 140 0.009496563815246599\n",
      "EPOCH_LOSS 141 0.009491375718066351\n",
      "EPOCH_LOSS 142 0.009486198081485023\n",
      "EPOCH_LOSS 143 0.00948103102248689\n",
      "EPOCH_LOSS 144 0.009475874452345627\n",
      "EPOCH_LOSS 145 0.009470726203047076\n",
      "EPOCH_LOSS 146 0.009465585148031802\n",
      "EPOCH_LOSS 147 0.00946045394870558\n",
      "EPOCH_LOSS 148 0.009455328281496488\n",
      "EPOCH_LOSS 149 0.009450208780803113\n",
      "EPOCH_LOSS 150 0.00944509720964328\n",
      "EPOCH_LOSS 151 0.009439989956596236\n",
      "EPOCH_LOSS 152 0.009434887902018137\n",
      "EPOCH_LOSS 153 0.009429790412102394\n",
      "EPOCH_LOSS 154 0.009424694797283062\n",
      "EPOCH_LOSS 155 0.009419606452120172\n",
      "EPOCH_LOSS 156 0.009414520435517921\n",
      "EPOCH_LOSS 157 0.00940943793636178\n",
      "EPOCH_LOSS 158 0.009404358171176772\n",
      "EPOCH_LOSS 159 0.009399281175808507\n",
      "EPOCH_LOSS 160 0.009394206168610659\n",
      "EPOCH_LOSS 161 0.009389133829564977\n",
      "EPOCH_LOSS 162 0.009384062550518242\n",
      "EPOCH_LOSS 163 0.009378994271782726\n",
      "EPOCH_LOSS 164 0.00937392480193697\n",
      "EPOCH_LOSS 165 0.009368859948002805\n",
      "EPOCH_LOSS 166 0.009363794560372944\n",
      "EPOCH_LOSS 167 0.009358730350690708\n",
      "EPOCH_LOSS 168 0.009353665613263354\n",
      "EPOCH_LOSS 169 0.00934860249688993\n",
      "EPOCH_LOSS 170 0.009343539319105034\n",
      "EPOCH_LOSS 171 0.009338477740576789\n",
      "EPOCH_LOSS 172 0.009333415779457491\n",
      "EPOCH_LOSS 173 0.009328351386854644\n",
      "EPOCH_LOSS 174 0.009323287607482272\n",
      "EPOCH_LOSS 175 0.009318227100727635\n",
      "EPOCH_LOSS 176 0.00931316286310591\n",
      "EPOCH_LOSS 177 0.0093080989503691\n",
      "EPOCH_LOSS 178 0.009303032288597832\n",
      "EPOCH_LOSS 179 0.00929796706996874\n",
      "EPOCH_LOSS 180 0.009292899712186732\n",
      "EPOCH_LOSS 181 0.009287834792224137\n",
      "EPOCH_LOSS 182 0.009282764404265223\n",
      "EPOCH_LOSS 183 0.00927769764167705\n",
      "EPOCH_LOSS 184 0.009272624892544542\n",
      "EPOCH_LOSS 185 0.009267551275582101\n",
      "EPOCH_LOSS 186 0.009262477038874811\n",
      "EPOCH_LOSS 187 0.009257402420490685\n",
      "EPOCH_LOSS 188 0.009252325403817615\n",
      "EPOCH_LOSS 189 0.009247245218746793\n",
      "EPOCH_LOSS 190 0.009242165528624736\n",
      "EPOCH_LOSS 191 0.009237082832786904\n",
      "EPOCH_LOSS 192 0.00923199837918003\n",
      "EPOCH_LOSS 193 0.009226911055589749\n",
      "EPOCH_LOSS 194 0.009221824210445664\n",
      "EPOCH_LOSS 195 0.009216731584118915\n",
      "EPOCH_LOSS 196 0.009211639058536922\n",
      "EPOCH_LOSS 197 0.009206544118167305\n",
      "EPOCH_LOSS 198 0.009201444506060821\n",
      "EPOCH_LOSS 199 0.009196345491189556\n",
      "EPOCH_LOSS 200 0.009191241963647286\n",
      "EPOCH_LOSS 201 0.009186137183805024\n",
      "EPOCH_LOSS 202 0.009181028174680979\n",
      "EPOCH_LOSS 203 0.00917591633988537\n",
      "EPOCH_LOSS 204 0.009170803590328465\n",
      "EPOCH_LOSS 205 0.009165686892492166\n",
      "EPOCH_LOSS 206 0.009160565758784621\n",
      "EPOCH_LOSS 207 0.009155443948402751\n",
      "EPOCH_LOSS 208 0.009150319137986881\n",
      "EPOCH_LOSS 209 0.009145189696435\n",
      "EPOCH_LOSS 210 0.00914005832932334\n",
      "EPOCH_LOSS 211 0.0091349229185277\n",
      "EPOCH_LOSS 212 0.00912978486994752\n",
      "EPOCH_LOSS 213 0.009124642683841524\n",
      "EPOCH_LOSS 214 0.009119498439479056\n",
      "EPOCH_LOSS 215 0.00911434717969969\n",
      "EPOCH_LOSS 216 0.009109197086063062\n",
      "EPOCH_LOSS 217 0.009104042282020254\n",
      "EPOCH_LOSS 218 0.009098881293629674\n",
      "EPOCH_LOSS 219 0.009093718196295054\n",
      "EPOCH_LOSS 220 0.00908855016972968\n",
      "EPOCH_LOSS 221 0.009083379307383878\n",
      "EPOCH_LOSS 222 0.00907820345440732\n",
      "EPOCH_LOSS 223 0.009073024226645245\n",
      "EPOCH_LOSS 224 0.009067842505694987\n",
      "EPOCH_LOSS 225 0.009062655280290705\n",
      "EPOCH_LOSS 226 0.009057462095982413\n",
      "EPOCH_LOSS 227 0.009052266243838786\n",
      "EPOCH_LOSS 228 0.009047067476066059\n",
      "EPOCH_LOSS 229 0.009041862975891811\n",
      "EPOCH_LOSS 230 0.009036651598274187\n",
      "EPOCH_LOSS 231 0.009031439536134523\n",
      "EPOCH_LOSS 232 0.009026220881718033\n",
      "EPOCH_LOSS 233 0.009020997618291058\n",
      "EPOCH_LOSS 234 0.009015767686739949\n",
      "EPOCH_LOSS 235 0.009010536924490564\n",
      "EPOCH_LOSS 236 0.00900529907532121\n",
      "EPOCH_LOSS 237 0.009000057074850987\n",
      "EPOCH_LOSS 238 0.008994809599108486\n",
      "EPOCH_LOSS 239 0.008989555953426804\n",
      "EPOCH_LOSS 240 0.008984297342300497\n",
      "EPOCH_LOSS 241 0.008979033897254305\n",
      "EPOCH_LOSS 242 0.008973765906265488\n",
      "EPOCH_LOSS 243 0.008968492325205954\n",
      "EPOCH_LOSS 244 0.008963213367992058\n",
      "EPOCH_LOSS 245 0.00895792908451192\n",
      "EPOCH_LOSS 246 0.008952639317452592\n",
      "EPOCH_LOSS 247 0.008947340650239412\n",
      "EPOCH_LOSS 248 0.008942039633616799\n",
      "EPOCH_LOSS 249 0.008936731422371244\n",
      "EPOCH_LOSS 250 0.008931419262043334\n",
      "EPOCH_LOSS 251 0.008926097371230807\n",
      "EPOCH_LOSS 252 0.00892077235950956\n",
      "EPOCH_LOSS 253 0.008915440561854006\n",
      "EPOCH_LOSS 254 0.008910101553489308\n",
      "EPOCH_LOSS 255 0.008904758277666642\n",
      "EPOCH_LOSS 256 0.008899406115406391\n",
      "EPOCH_LOSS 257 0.008894047650514988\n",
      "EPOCH_LOSS 258 0.008888683651181194\n",
      "EPOCH_LOSS 259 0.00888331356537282\n",
      "EPOCH_LOSS 260 0.008877934131628263\n",
      "EPOCH_LOSS 261 0.008872550054353565\n",
      "EPOCH_LOSS 262 0.008867159143897475\n",
      "EPOCH_LOSS 263 0.008861760673740858\n",
      "EPOCH_LOSS 264 0.008856353587944819\n",
      "EPOCH_LOSS 265 0.0088509401538641\n",
      "EPOCH_LOSS 266 0.008845520849504244\n",
      "EPOCH_LOSS 267 0.008840092039735414\n",
      "EPOCH_LOSS 268 0.008834659210987233\n",
      "EPOCH_LOSS 269 0.008829216059684068\n",
      "EPOCH_LOSS 270 0.008823765045919696\n",
      "EPOCH_LOSS 271 0.008818307445489752\n",
      "EPOCH_LOSS 272 0.008812842118867336\n",
      "EPOCH_LOSS 273 0.008807370002400564\n",
      "EPOCH_LOSS 274 0.00880188985706226\n",
      "EPOCH_LOSS 275 0.008796398863516018\n",
      "EPOCH_LOSS 276 0.008790904012417958\n",
      "EPOCH_LOSS 277 0.008785398214703857\n",
      "EPOCH_LOSS 278 0.00877988528782614\n",
      "EPOCH_LOSS 279 0.008774365637656536\n",
      "EPOCH_LOSS 280 0.008768836851989667\n",
      "EPOCH_LOSS 281 0.008763299288637847\n",
      "EPOCH_LOSS 282 0.008757755683637099\n",
      "EPOCH_LOSS 283 0.008752200200425662\n",
      "EPOCH_LOSS 284 0.008746640360462533\n",
      "EPOCH_LOSS 285 0.008741071993980744\n",
      "EPOCH_LOSS 286 0.008735492477119177\n",
      "EPOCH_LOSS 287 0.00872990583458622\n",
      "EPOCH_LOSS 288 0.008724310338666483\n",
      "EPOCH_LOSS 289 0.008718708414326452\n",
      "EPOCH_LOSS 290 0.008713097808745804\n",
      "EPOCH_LOSS 291 0.008707480455704615\n",
      "EPOCH_LOSS 292 0.008701854334526226\n",
      "EPOCH_LOSS 293 0.00869622056734734\n",
      "EPOCH_LOSS 294 0.008690580028031553\n",
      "EPOCH_LOSS 295 0.008684927932691145\n",
      "EPOCH_LOSS 296 0.008679269523149276\n",
      "EPOCH_LOSS 297 0.008673605477127436\n",
      "EPOCH_LOSS 298 0.008667935030760361\n",
      "EPOCH_LOSS 299 0.008662254724048543\n",
      "EPOCH_LOSS 300 0.008656567479542125\n",
      "EPOCH_LOSS 301 0.008650874712977418\n",
      "EPOCH_LOSS 302 0.008645174394127146\n",
      "EPOCH_LOSS 303 0.008639466754632254\n",
      "EPOCH_LOSS 304 0.00863375405656903\n",
      "EPOCH_LOSS 305 0.008628035564841277\n",
      "EPOCH_LOSS 306 0.008622306946449741\n",
      "EPOCH_LOSS 307 0.008616575108162744\n",
      "EPOCH_LOSS 308 0.008610835208650046\n",
      "EPOCH_LOSS 309 0.008605089071127505\n",
      "EPOCH_LOSS 310 0.0085993379046929\n",
      "EPOCH_LOSS 311 0.008593580300439836\n",
      "EPOCH_LOSS 312 0.008587816122141868\n",
      "EPOCH_LOSS 313 0.008582047609145087\n",
      "EPOCH_LOSS 314 0.008576272041169382\n",
      "EPOCH_LOSS 315 0.008570490213494712\n",
      "EPOCH_LOSS 316 0.008564703795074055\n",
      "EPOCH_LOSS 317 0.008558910438988111\n",
      "EPOCH_LOSS 318 0.008553112768727897\n",
      "EPOCH_LOSS 319 0.008547309104933913\n",
      "EPOCH_LOSS 320 0.008541499664434533\n",
      "EPOCH_LOSS 321 0.00853568531018476\n",
      "EPOCH_LOSS 322 0.008529865703780535\n",
      "EPOCH_LOSS 323 0.008524040097547378\n",
      "EPOCH_LOSS 324 0.008518209052167394\n",
      "EPOCH_LOSS 325 0.008512371691934677\n",
      "EPOCH_LOSS 326 0.008506531051622437\n",
      "EPOCH_LOSS 327 0.008500683899125536\n",
      "EPOCH_LOSS 328 0.008494832985214586\n",
      "EPOCH_LOSS 329 0.00848897564780977\n",
      "EPOCH_LOSS 330 0.008483114269930045\n",
      "EPOCH_LOSS 331 0.008477247435158873\n",
      "EPOCH_LOSS 332 0.008471375986908778\n",
      "EPOCH_LOSS 333 0.008465500831516499\n",
      "EPOCH_LOSS 334 0.008459617302473544\n",
      "EPOCH_LOSS 335 0.008453731107741459\n",
      "EPOCH_LOSS 336 0.008447839446425394\n",
      "EPOCH_LOSS 337 0.008441944856524728\n",
      "EPOCH_LOSS 338 0.008436044061586745\n",
      "EPOCH_LOSS 339 0.008430137256472623\n",
      "EPOCH_LOSS 340 0.008424227512176734\n",
      "EPOCH_LOSS 341 0.00841831104101511\n",
      "EPOCH_LOSS 342 0.008412394774633602\n",
      "EPOCH_LOSS 343 0.008406469329614075\n",
      "EPOCH_LOSS 344 0.008400540577230668\n",
      "EPOCH_LOSS 345 0.008394610368175867\n",
      "EPOCH_LOSS 346 0.008388672534552964\n",
      "EPOCH_LOSS 347 0.008382730500117359\n",
      "EPOCH_LOSS 348 0.00837678635820131\n",
      "EPOCH_LOSS 349 0.008370835569839362\n",
      "EPOCH_LOSS 350 0.008364881967152214\n",
      "EPOCH_LOSS 351 0.008358924675331189\n",
      "EPOCH_LOSS 352 0.008352962242098508\n",
      "EPOCH_LOSS 353 0.008346995875560082\n",
      "EPOCH_LOSS 354 0.00834102582912643\n",
      "EPOCH_LOSS 355 0.008335051854489031\n",
      "EPOCH_LOSS 356 0.00832907461197899\n",
      "EPOCH_LOSS 357 0.008323092085811707\n",
      "EPOCH_LOSS 358 0.008317108175071133\n",
      "EPOCH_LOSS 359 0.008311118761389383\n",
      "EPOCH_LOSS 360 0.008305127924545675\n",
      "EPOCH_LOSS 361 0.008299129539327857\n",
      "EPOCH_LOSS 362 0.00829313234929199\n",
      "EPOCH_LOSS 363 0.008287130110600505\n",
      "EPOCH_LOSS 364 0.008281122149660832\n",
      "EPOCH_LOSS 365 0.008275113601516011\n",
      "EPOCH_LOSS 366 0.008269101104084785\n",
      "EPOCH_LOSS 367 0.008263085034811341\n",
      "EPOCH_LOSS 368 0.008257065177729554\n",
      "EPOCH_LOSS 369 0.008251045741300728\n",
      "EPOCH_LOSS 370 0.008245018173732742\n",
      "EPOCH_LOSS 371 0.00823899161327467\n",
      "EPOCH_LOSS 372 0.00823296007177064\n",
      "EPOCH_LOSS 373 0.008226926079640911\n",
      "EPOCH_LOSS 374 0.008220891209013146\n",
      "EPOCH_LOSS 375 0.00821485178040993\n",
      "EPOCH_LOSS 376 0.008208810609603538\n",
      "EPOCH_LOSS 377 0.008202766982384107\n",
      "EPOCH_LOSS 378 0.00819672001554677\n",
      "EPOCH_LOSS 379 0.00819067126913924\n",
      "EPOCH_LOSS 380 0.008184620978425254\n",
      "EPOCH_LOSS 381 0.008178566352670846\n",
      "EPOCH_LOSS 382 0.008172511272254596\n",
      "EPOCH_LOSS 383 0.008166454424386974\n",
      "EPOCH_LOSS 384 0.008160393540780422\n",
      "EPOCH_LOSS 385 0.008154332399060053\n",
      "EPOCH_LOSS 386 0.008148269758566262\n",
      "EPOCH_LOSS 387 0.008142205162802672\n",
      "EPOCH_LOSS 388 0.008136137651785964\n",
      "EPOCH_LOSS 389 0.008130068107121611\n",
      "EPOCH_LOSS 390 0.00812399926545391\n",
      "EPOCH_LOSS 391 0.008117928056709646\n",
      "EPOCH_LOSS 392 0.00811185488399125\n",
      "EPOCH_LOSS 393 0.008105779242916866\n",
      "EPOCH_LOSS 394 0.00809970440509732\n",
      "EPOCH_LOSS 395 0.008093628581149398\n",
      "EPOCH_LOSS 396 0.008087550837932509\n",
      "EPOCH_LOSS 397 0.008081471978750072\n",
      "EPOCH_LOSS 398 0.00807539328843826\n",
      "EPOCH_LOSS 399 0.008069313276293715\n",
      "EPOCH_LOSS 400 0.00806323169632715\n",
      "EPOCH_LOSS 401 0.008057151853593483\n",
      "EPOCH_LOSS 402 0.008051070853628925\n",
      "EPOCH_LOSS 403 0.008044987570381532\n",
      "EPOCH_LOSS 404 0.008038904516633943\n",
      "EPOCH_LOSS 405 0.008032822156042688\n",
      "EPOCH_LOSS 406 0.00802674227480222\n",
      "EPOCH_LOSS 407 0.008020657136501644\n",
      "EPOCH_LOSS 408 0.008014575732905148\n",
      "EPOCH_LOSS 409 0.008008493531500455\n",
      "EPOCH_LOSS 410 0.008002412622562797\n",
      "EPOCH_LOSS 411 0.00799633208784067\n",
      "EPOCH_LOSS 412 0.007990250345066173\n",
      "EPOCH_LOSS 413 0.00798417129397088\n",
      "EPOCH_LOSS 414 0.007978091323689693\n",
      "EPOCH_LOSS 415 0.007972012598810644\n",
      "EPOCH_LOSS 416 0.007965935390450516\n",
      "EPOCH_LOSS 417 0.007959859233088949\n",
      "EPOCH_LOSS 418 0.007953785991691192\n",
      "EPOCH_LOSS 419 0.007947710767262984\n",
      "EPOCH_LOSS 420 0.007941640206267992\n",
      "EPOCH_LOSS 421 0.007935568771686677\n",
      "EPOCH_LOSS 422 0.007929500303033476\n",
      "EPOCH_LOSS 423 0.007923434031791896\n",
      "EPOCH_LOSS 424 0.007917368218438142\n",
      "EPOCH_LOSS 425 0.007911306809268758\n",
      "EPOCH_LOSS 426 0.007905247609029529\n",
      "EPOCH_LOSS 427 0.007899188211714652\n",
      "EPOCH_LOSS 428 0.007893133296386265\n",
      "EPOCH_LOSS 429 0.007887079886869952\n",
      "EPOCH_LOSS 430 0.007881030188567876\n",
      "EPOCH_LOSS 431 0.00787498253343712\n",
      "EPOCH_LOSS 432 0.007868937172051654\n",
      "EPOCH_LOSS 433 0.00786289796637351\n",
      "EPOCH_LOSS 434 0.00785685959140205\n",
      "EPOCH_LOSS 435 0.00785082485497261\n",
      "EPOCH_LOSS 436 0.00784479442920068\n",
      "EPOCH_LOSS 437 0.007838768214821618\n",
      "EPOCH_LOSS 438 0.007832744913962434\n",
      "EPOCH_LOSS 439 0.007826726552274557\n",
      "EPOCH_LOSS 440 0.007820709244495731\n",
      "EPOCH_LOSS 441 0.00781469964066028\n",
      "EPOCH_LOSS 442 0.007808693773839932\n",
      "EPOCH_LOSS 443 0.007802694248255679\n",
      "EPOCH_LOSS 444 0.0077966966896723475\n",
      "EPOCH_LOSS 445 0.007790704304061675\n",
      "EPOCH_LOSS 446 0.007784718785385724\n",
      "EPOCH_LOSS 447 0.0077787356254434715\n",
      "EPOCH_LOSS 448 0.007772759814084317\n",
      "EPOCH_LOSS 449 0.007766789005540692\n",
      "EPOCH_LOSS 450 0.007760823323488328\n",
      "EPOCH_LOSS 451 0.007754864773434949\n",
      "EPOCH_LOSS 452 0.007748910870017208\n",
      "EPOCH_LOSS 453 0.007742963510378865\n",
      "EPOCH_LOSS 454 0.007737022044098575\n",
      "EPOCH_LOSS 455 0.007731088033576978\n",
      "EPOCH_LOSS 456 0.007725159041616293\n",
      "EPOCH_LOSS 457 0.0077192385900684765\n",
      "EPOCH_LOSS 458 0.007713323225620342\n",
      "EPOCH_LOSS 459 0.0077074174305812836\n",
      "EPOCH_LOSS 460 0.0077015169101824444\n",
      "EPOCH_LOSS 461 0.007695624091792345\n",
      "EPOCH_LOSS 462 0.007689736627847407\n",
      "EPOCH_LOSS 463 0.007683858750157115\n",
      "EPOCH_LOSS 464 0.007677990002803607\n",
      "EPOCH_LOSS 465 0.007672126842577835\n",
      "EPOCH_LOSS 466 0.007666272427482859\n",
      "EPOCH_LOSS 467 0.00766042769509422\n",
      "EPOCH_LOSS 468 0.00765459078885306\n",
      "EPOCH_LOSS 469 0.007648759517737201\n",
      "EPOCH_LOSS 470 0.007642939601156083\n",
      "EPOCH_LOSS 471 0.007637129116648488\n",
      "EPOCH_LOSS 472 0.007631325426207027\n",
      "EPOCH_LOSS 473 0.00762553161104513\n",
      "EPOCH_LOSS 474 0.007619748454936437\n",
      "EPOCH_LOSS 475 0.007613973888704466\n",
      "EPOCH_LOSS 476 0.007608208256818636\n",
      "EPOCH_LOSS 477 0.007602453552882283\n",
      "EPOCH_LOSS 478 0.007596709170782239\n",
      "EPOCH_LOSS 479 0.007590972687926918\n",
      "EPOCH_LOSS 480 0.007585248295485623\n",
      "EPOCH_LOSS 481 0.007579534060653866\n",
      "EPOCH_LOSS 482 0.007573827740935202\n",
      "EPOCH_LOSS 483 0.007568133803100364\n",
      "EPOCH_LOSS 484 0.007562452729506571\n",
      "EPOCH_LOSS 485 0.007556779845161215\n",
      "EPOCH_LOSS 486 0.0075511177841579595\n",
      "EPOCH_LOSS 487 0.007545467679785018\n",
      "EPOCH_LOSS 488 0.007539827259410441\n",
      "EPOCH_LOSS 489 0.00753420054202621\n",
      "EPOCH_LOSS 490 0.007528583818069679\n",
      "EPOCH_LOSS 491 0.007522978714994166\n",
      "EPOCH_LOSS 492 0.007517385959177913\n",
      "EPOCH_LOSS 493 0.007511805157793057\n",
      "EPOCH_LOSS 494 0.00750623410325498\n",
      "EPOCH_LOSS 495 0.0075006780399180725\n",
      "EPOCH_LOSS 496 0.007495132551921259\n",
      "EPOCH_LOSS 497 0.007489599158109033\n",
      "EPOCH_LOSS 498 0.00748407922503605\n",
      "EPOCH_LOSS 499 0.007478570376674608\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "initial_lr = 1e-5\n",
    "layers = 3\n",
    "cell = LSTMStacked(cells=layers)\n",
    "optimizer = optim.Adam(cell.parameters(), lr=initial_lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50, threshold=0.0005)\n",
    "prev_a_c_0 = [(torch.zeros((64)).to(torch.float32), torch.zeros((64)).to(torch.float32)) for _ in range(layers)]\n",
    "\n",
    "EPOCH=1000\n",
    "sliding_window_length = 12\n",
    "\n",
    "for ep in range(EPOCH):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, split-sliding_window_length):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        passengers = torch.tensor(np.array(dataset.passengers[i:i+sliding_window_length])).to(torch.float32)\n",
    "        prev_a_c = prev_a_c_0\n",
    "        # print(\"passengers\", passengers)\n",
    "        if len(passengers)<12:\n",
    "            continue\n",
    "\n",
    "        # logits, a_c = cell(torch.squeeze(passengers), prev_a_c)\n",
    "        for p in range(sliding_window_length):\n",
    "            logit, a_c = cell(passengers[p], prev_a_c)\n",
    "            prev_a_c = a_c\n",
    "\n",
    "        target = torch.FloatTensor(dataset.passengers[i+sliding_window_length])\n",
    "        loss = criterion(logit, target)\n",
    "        prev_a_c = a_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prev_a_c = [(a.detach(), c.detach()) for a, c in prev_a_c]\n",
    "\n",
    "        # print(\"Batch avg loss\", batch_loss.item() / 4)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(\"EPOCH_LOSS\", ep, epoch_loss / (split-sliding_window_length))\n",
    "    scheduler.step(epoch_loss / (split-sliding_window_length))\n",
    "    if initial_lr != optimizer.param_groups[0]['lr']:\n",
    "        print(f\"LR Changed {optimizer.param_groups[0]['lr']}\")\n",
    "        initial_lr = optimizer.param_groups[0]['lr']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba46028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    output = []\n",
    "    labels = []\n",
    "    # input = torch.FloatTensor(test_loader.dataset)[i:i+sliding_window_length,1]\n",
    "    for i in range(len(dataset.passengers) - sliding_window_length):\n",
    "        input = torch.FloatTensor(dataset.passengers)[i:i+sliding_window_length]\n",
    "        prev_a_c = [(torch.zeros((64)).to(torch.float32), torch.zeros((64)).to(torch.float32)) for _ in range(layers)]\n",
    "        # logits, a_c_prev = cell(input, a_c_prev)\n",
    "        for p in range(sliding_window_length):\n",
    "            logit, a_c = cell(input[p], prev_a_c)\n",
    "            prev_a_c = a_c\n",
    "        output.append( dataset.scaler.inverse_transform(logit.reshape(-1, 1)))\n",
    "        labels.append(dataset.scaler.inverse_transform(torch.FloatTensor(dataset.passengers)[sliding_window_length+i].reshape(-1, 1)))\n",
    "    \n",
    "    print(output)\n",
    "    print(labels)\n",
    "\n",
    "    toutput = torch.tensor(output)\n",
    "    tlabels = torch.tensor(labels)\n",
    "    # mape (mean absolute percentage error)\n",
    "    # mean((actual-forecast) / actual)\n",
    "    mape = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)) / torch.squeeze(tlabels))) # 0.1580, 0.1418(12-batch), 0.1343(sliding window)\n",
    "    print(mape)\n",
    "\n",
    "    # mae (mean absolute error)\n",
    "    mae = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)))) # 82.5248, 71.5019, 69.0149\n",
    "    print(mae)\n",
    "\n",
    "    # rmse (root mean square error)\n",
    "    rmse = torch.sqrt(torch.mean(torch.square((torch.squeeze(toutput)-torch.squeeze(tlabels))))) # 112.3071, 94.6528, 91.1919\n",
    "    print(rmse)\n",
    "\n",
    "finally:\n",
    "    torch.set_grad_enabled(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(np.array(toutput).squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67db0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cell.state_dict, './airlines_lstm_cell.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cba000",
   "metadata": {},
   "source": [
    "# 12 in 1 out Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7a00e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, output_size=1):\n",
    "        super().__init__()\n",
    "        combined_size = input_size + hidden_size\n",
    "        # forget gate\n",
    "        self.forget = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # update gate\n",
    "        self.update = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # tanh\n",
    "        self.candidate = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # output gate\n",
    "        self.output = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "\n",
    "        # final fc\n",
    "        self.final = nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, prev_a, prev_c): # hidden state and long-term memory\n",
    "        # print(\"x, prev_a, prev_c\", x, prev_a, prev_c)\n",
    "        input_vector = torch.concat((x, prev_a))\n",
    "        # print(\"input_vector\", input_vector)\n",
    "        forget = torch.sigmoid(self.forget(input_vector))\n",
    "        # print(\"forget\", forget)\n",
    "        update = torch.sigmoid(self.update(input_vector))\n",
    "        # print(\"update\", update)\n",
    "        candidate = torch.tanh(self.candidate(input_vector))\n",
    "        # print(\"candidate\", candidate)\n",
    "        output = torch.sigmoid(self.output(input_vector))\n",
    "\n",
    "        # print(\"output\", output)\n",
    "        c = (forget * prev_c) + (update * candidate)\n",
    "        # print(\"c\", c)\n",
    "\n",
    "        a = output * torch.tanh(c)\n",
    "        # print(\"a\", a)\n",
    "\n",
    "        x = self.final(a)\n",
    "        # print(\"x\", x)\n",
    "\n",
    "        return x, a, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030d3c2",
   "metadata": {},
   "source": [
    "## 12 in 1 out Sliding Window Cell Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f3282da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH_LOSS 0 0.22664204862318996\n",
      "EPOCH_LOSS 1 0.21507030247877806\n",
      "EPOCH_LOSS 2 0.20474915380756825\n",
      "EPOCH_LOSS 3 0.19455377157363626\n",
      "EPOCH_LOSS 4 0.18421390451268\n",
      "EPOCH_LOSS 5 0.1735410915257839\n",
      "EPOCH_LOSS 6 0.16234282857905596\n",
      "EPOCH_LOSS 7 0.15042401825746474\n",
      "EPOCH_LOSS 8 0.13761123606903303\n",
      "EPOCH_LOSS 9 0.12376708166527514\n",
      "EPOCH_LOSS 10 0.10881292676705738\n",
      "EPOCH_LOSS 11 0.09281256556735173\n",
      "EPOCH_LOSS 12 0.07617385050851334\n",
      "EPOCH_LOSS 13 0.05992426128327621\n",
      "EPOCH_LOSS 14 0.04578862492916158\n",
      "EPOCH_LOSS 15 0.03557685723269689\n",
      "EPOCH_LOSS 16 0.02988024821798488\n",
      "EPOCH_LOSS 17 0.027531523711048098\n",
      "EPOCH_LOSS 18 0.02677875230012385\n",
      "EPOCH_LOSS 19 0.02649951751968294\n",
      "EPOCH_LOSS 20 0.026273000614502787\n",
      "EPOCH_LOSS 21 0.0260068916673907\n",
      "EPOCH_LOSS 22 0.0257030569907492\n",
      "EPOCH_LOSS 23 0.025377394278838724\n",
      "EPOCH_LOSS 24 0.025041852638666897\n",
      "EPOCH_LOSS 25 0.024703306882929318\n",
      "EPOCH_LOSS 26 0.024365279689569964\n",
      "EPOCH_LOSS 27 0.024029310511876123\n",
      "EPOCH_LOSS 28 0.023695976447802528\n",
      "EPOCH_LOSS 29 0.023365336164294606\n",
      "EPOCH_LOSS 30 0.023037233119766902\n",
      "EPOCH_LOSS 31 0.022711423840301618\n",
      "EPOCH_LOSS 32 0.022387648358472557\n",
      "EPOCH_LOSS 33 0.022065655090310065\n",
      "EPOCH_LOSS 34 0.021745235459307007\n",
      "EPOCH_LOSS 35 0.02142621887029162\n",
      "EPOCH_LOSS 36 0.0211084491636034\n",
      "EPOCH_LOSS 37 0.020791803473922014\n",
      "EPOCH_LOSS 38 0.02047619840857011\n",
      "EPOCH_LOSS 39 0.020161574368780102\n",
      "EPOCH_LOSS 40 0.019847901667614614\n",
      "EPOCH_LOSS 41 0.01953517149614992\n",
      "EPOCH_LOSS 42 0.0192234202586799\n",
      "EPOCH_LOSS 43 0.018912723548537465\n",
      "EPOCH_LOSS 44 0.018603147904970807\n",
      "EPOCH_LOSS 45 0.018294809775013094\n",
      "EPOCH_LOSS 46 0.017987820890347877\n",
      "EPOCH_LOSS 47 0.017682355408112714\n",
      "EPOCH_LOSS 48 0.01737855114191403\n",
      "EPOCH_LOSS 49 0.017076595070928027\n",
      "EPOCH_LOSS 50 0.01677667051026375\n",
      "EPOCH_LOSS 51 0.016478975947284508\n",
      "EPOCH_LOSS 52 0.01618375284797522\n",
      "EPOCH_LOSS 53 0.015891203861794302\n",
      "EPOCH_LOSS 54 0.015601566333159029\n",
      "EPOCH_LOSS 55 0.015315103037736838\n",
      "EPOCH_LOSS 56 0.015032051892765004\n",
      "EPOCH_LOSS 57 0.014752672113820512\n",
      "EPOCH_LOSS 58 0.014477234622928447\n",
      "EPOCH_LOSS 59 0.014205994368163676\n",
      "EPOCH_LOSS 60 0.013939229202458233\n",
      "EPOCH_LOSS 61 0.013677196983595562\n",
      "EPOCH_LOSS 62 0.013420166215514928\n",
      "EPOCH_LOSS 63 0.013168389737360578\n",
      "EPOCH_LOSS 64 0.012922131651595187\n",
      "EPOCH_LOSS 65 0.012681629893817305\n",
      "EPOCH_LOSS 66 0.012447126538075119\n",
      "EPOCH_LOSS 67 0.012218838954026007\n",
      "EPOCH_LOSS 68 0.01199696246513451\n",
      "EPOCH_LOSS 69 0.011781695978060645\n",
      "EPOCH_LOSS 70 0.011573193710771036\n",
      "EPOCH_LOSS 71 0.011371608096991755\n",
      "EPOCH_LOSS 72 0.011177046092009578\n",
      "EPOCH_LOSS 73 0.010989602274850522\n",
      "EPOCH_LOSS 74 0.010809356955396152\n",
      "EPOCH_LOSS 75 0.010636339066941928\n",
      "EPOCH_LOSS 76 0.010470573072413895\n",
      "EPOCH_LOSS 77 0.010312042969642665\n",
      "EPOCH_LOSS 78 0.010160724820969391\n",
      "EPOCH_LOSS 79 0.01001653739567205\n",
      "EPOCH_LOSS 80 0.009879409495745202\n",
      "EPOCH_LOSS 81 0.009749231060578893\n",
      "EPOCH_LOSS 82 0.009625871460551357\n",
      "EPOCH_LOSS 83 0.009509174047417785\n",
      "EPOCH_LOSS 84 0.009398976929993747\n",
      "EPOCH_LOSS 85 0.00929509023587584\n",
      "EPOCH_LOSS 86 0.009197311349367722\n",
      "EPOCH_LOSS 87 0.009105434373384451\n",
      "EPOCH_LOSS 88 0.009019231924031552\n",
      "EPOCH_LOSS 89 0.008938471009739438\n",
      "EPOCH_LOSS 90 0.008862913179179763\n",
      "EPOCH_LOSS 91 0.008792317061754621\n",
      "EPOCH_LOSS 92 0.008726440471911281\n",
      "EPOCH_LOSS 93 0.008665037621975564\n",
      "EPOCH_LOSS 94 0.008607863743881719\n",
      "EPOCH_LOSS 95 0.008554680341016774\n",
      "EPOCH_LOSS 96 0.008505252931161078\n",
      "EPOCH_LOSS 97 0.008459348757688457\n",
      "EPOCH_LOSS 98 0.008416745176265245\n",
      "EPOCH_LOSS 99 0.008377226414416869\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "initial_lr = 1e-5\n",
    "cell = LSTMCell()\n",
    "optimizer = optim.Adam(cell.parameters(), lr=initial_lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50, threshold=0.0005)\n",
    "a0 = torch.zeros((64)).to(torch.float32)\n",
    "c0 = torch.zeros((64)).to(torch.float32)\n",
    "\n",
    "EPOCH=100\n",
    "sliding_window_length = 12\n",
    "\n",
    "for ep in range(EPOCH):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, split-sliding_window_length):\n",
    "        batch_loss = 0.0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        passengers = torch.tensor(np.array(dataset.passengers[i:i+sliding_window_length])).to(torch.float32)\n",
    "        target = torch.FloatTensor(dataset.passengers[i+sliding_window_length])\n",
    "        prev_a, prev_c = a0, c0\n",
    "        # print(\"passengers\", passengers)\n",
    "        if len(passengers)<12:\n",
    "            continue\n",
    "\n",
    "        # logits, a, c = cell(torch.squeeze(passengers), prev_a, prev_c)\n",
    "        for p in range(sliding_window_length):\n",
    "            logit, a, c = cell(passengers[p], prev_a, prev_c)\n",
    "            prev_a, prev_c = a, c\n",
    "\n",
    "        loss = criterion(logit, target)\n",
    "        # batch_loss += loss\n",
    "        # prev_a = a.detach()\n",
    "        # prev_c = c.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(\"Batch avg loss\", batch_loss.item() / 4)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(\"EPOCH_LOSS\", ep, epoch_loss / (split-sliding_window_length))\n",
    "    scheduler.step(epoch_loss / (split-sliding_window_length))\n",
    "    if initial_lr != optimizer.param_groups[0]['lr']:\n",
    "        print(f\"LR Changed {optimizer.param_groups[0]['lr']}\")\n",
    "        initial_lr = optimizer.param_groups[0]['lr']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be6b0896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[171.10841805]]), array([[170.83052576]]), array([[170.75727433]]), array([[170.94295782]]), array([[171.11468571]]), array([[171.16956639]]), array([[171.88006663]]), array([[173.34486359]]), array([[175.18663138]]), array([[177.14405769]]), array([[178.49620646]]), array([[179.14737999]]), array([[180.11388201]]), array([[181.21513885]]), array([[182.08057088]]), array([[183.785191]]), array([[185.19364071]]), array([[187.46415704]]), array([[189.16022474]]), array([[191.27834105]]), array([[193.75380701]]), array([[196.2103619]]), array([[198.48379594]]), array([[200.29755992]]), array([[201.5310244]]), array([[202.51773423]]), array([[203.59987932]]), array([[204.18147558]]), array([[204.94877076]]), array([[205.45561814]]), array([[207.49971116]]), array([[209.62604028]]), array([[212.90639198]]), array([[215.62342554]]), array([[218.34649521]]), array([[220.37422436]]), array([[222.05150449]]), array([[223.2643289]]), array([[223.86556178]]), array([[225.809403]]), array([[228.46902412]]), array([[230.89759064]]), array([[232.51114434]]), array([[235.0052126]]), array([[237.95512241]]), array([[241.18707722]]), array([[243.85593003]]), array([[245.24969858]]), array([[245.7604208]]), array([[245.69862407]]), array([[244.33035845]]), array([[242.9471029]]), array([[241.41874933]]), array([[240.83635032]]), array([[241.41058284]]), array([[243.62198859]]), array([[246.1483832]]), array([[249.52483398]]), array([[252.74388295]]), array([[255.63443518]]), array([[257.97114795]]), array([[260.27681571]]), array([[262.58527768]]), array([[263.73439109]]), array([[265.49972731]]), array([[267.03190941]]), array([[269.47963399]]), array([[273.32537246]]), array([[278.35528314]]), array([[284.51819038]]), array([[290.53623116]]), array([[295.4414832]]), array([[299.92209309]]), array([[303.21385318]]), array([[306.17540294]]), array([[308.58996755]]), array([[310.59741169]]), array([[312.80060464]]), array([[315.21090847]]), array([[317.76586264]]), array([[323.06549913]]), array([[329.44138354]]), array([[335.96460444]]), array([[342.2391873]]), array([[346.2332657]]), array([[349.28463143]]), array([[351.23131317]]), array([[352.39175779]]), array([[353.23897344]]), array([[354.00788957]]), array([[354.47169691]]), array([[356.15328413]]), array([[361.31938535]]), array([[368.91081119]]), array([[377.61169124]]), array([[385.83678436]]), array([[391.5973568]]), array([[395.55203843]]), array([[398.1328969]]), array([[397.10147977]]), array([[395.132043]]), array([[392.60953867]]), array([[388.17341995]]), array([[385.16358137]]), array([[386.29920232]]), array([[389.94942355]]), array([[396.49036682]]), array([[402.8767488]]), array([[406.74399185]]), array([[410.22356582]]), array([[413.36894655]]), array([[415.02663636]]), array([[416.76509571]]), array([[417.97222364]]), array([[415.73991537]]), array([[415.00440621]]), array([[417.9172349]]), array([[427.55054617]]), array([[439.01250327]]), array([[451.54511988]]), array([[462.67961454]]), array([[471.07056928]]), array([[478.38308394]]), array([[479.42728341]]), array([[482.59078789]]), array([[483.09877765]]), array([[481.87704575]]), array([[480.060256]]), array([[481.13008189]]), array([[489.9337455]]), array([[502.80237699]])]\n",
      "[array([[115.00000034]]), array([[126.00000067]]), array([[141.00000165]]), array([[134.99999972]]), array([[124.99999906]]), array([[148.99999909]]), array([[169.99999815]]), array([[169.99999815]]), array([[157.99999814]]), array([[133.00000036]]), array([[113.99999969]]), array([[140.00000004]]), array([[145.00000037]]), array([[150.0000007]]), array([[178.00000331]]), array([[162.99999847]]), array([[172.00000137]]), array([[178.00000331]]), array([[198.99999851]]), array([[198.99999851]]), array([[183.99999753]]), array([[162.00000072]]), array([[145.99999812]]), array([[165.99999943]]), array([[170.99999976]]), array([[179.99999881]]), array([[192.99999657]]), array([[181.00000042]]), array([[183.00000364]]), array([[217.99999821]]), array([[230.00000209]]), array([[242.00000596]]), array([[208.99999917]]), array([[191.00000107]]), array([[172.00000137]]), array([[193.99999818]]), array([[196.0000014]]), array([[196.0000014]]), array([[235.9999963]]), array([[234.9999947]]), array([[229.00000048]]), array([[243.00000757]]), array([[263.99999505]]), array([[271.99999249]]), array([[236.99999791]]), array([[211.00000238]]), array([[179.99999881]]), array([[201.00000173]]), array([[203.99999884]]), array([[187.99999624]]), array([[234.9999947]]), array([[226.99999726]]), array([[233.99999309]]), array([[263.99999505]]), array([[301.99999446]]), array([[292.99999541]]), array([[259.00000244]]), array([[229.00000048]]), array([[202.99999723]]), array([[229.00000048]]), array([[242.00000596]]), array([[232.9999992]]), array([[266.99999988]]), array([[269.0000031]]), array([[270.00000471]]), array([[314.99999994]]), array([[363.99998617]]), array([[347.00000513]]), array([[311.99999511]]), array([[273.99999571]]), array([[236.99999791]]), array([[278.00000215]]), array([[283.99999636]]), array([[277.00000054]]), array([[317.00000316]]), array([[312.99999672]]), array([[318.00000477]]), array([[374.00000226]]), array([[412.99998784]]), array([[405.00000584]]), array([[355.00000256]]), array([[306.00000089]]), array([[271.00000632]]), array([[306.00000089]]), array([[314.99999994]]), array([[300.99999285]]), array([[356.00000417]]), array([[348.00000674]]), array([[355.00000256]]), array([[421.99998689]]), array([[465.00000978]]), array([[467.00001299]]), array([[403.99998879]]), array([[347.00000513]]), array([[304.99999928]]), array([[336.00000286]]), array([[339.99999386]]), array([[318.00000477]]), array([[361.99999839]]), array([[348.00000674]]), array([[363.]]), array([[434.99999237]]), array([[490.99998987]]), array([[505.0000124]]), array([[403.99998879]]), array([[358.99999356]]), array([[310.00000733]]), array([[337.00000447]]), array([[359.99999517]]), array([[341.99999708]]), array([[405.99999201]]), array([[396.00000679]]), array([[420.00001454]]), array([[472.0000056]]), array([[548.00000441]]), array([[559.00000668]]), array([[463.00000656]]), array([[407.00000906]]), array([[361.99999839]]), array([[405.00000584]]), array([[416.99999428]]), array([[391.00001419]]), array([[418.9999975]]), array([[461.00000334]]), array([[472.0000056]]), array([[534.99999893]]), array([[622.]]), array([[606.00000513]]), array([[508.00000179]]), array([[461.00000334]]), array([[389.99999714]])]\n",
      "tensor(0.1390, dtype=torch.float64)\n",
      "tensor(39.0482, dtype=torch.float64)\n",
      "tensor(50.3357, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    output = []\n",
    "    labels = []\n",
    "    #input, a_prev, c_prev = torch.FloatTensor(test_loader.dataset)[0:sliding_window_length,1], torch.zeros((64)).to(torch.float32), torch.zeros((64)).to(torch.float32)\n",
    "    \n",
    "    for i in range(len(dataset.passengers) - sliding_window_length - 1):\n",
    "        input = torch.FloatTensor(dataset.passengers)[i:i+sliding_window_length]\n",
    "        prev_a, prev_c = torch.zeros((64)).to(torch.float32), torch.zeros((64)).to(torch.float32)\n",
    "        for p in range(sliding_window_length):\n",
    "            logit, a, c = cell(input[p], prev_a, prev_c)\n",
    "            prev_a, prev_c = a, c\n",
    "        # logits, a_prev, c_prev = cell(input, a_prev, c_prev)\n",
    "        output.append( dataset.scaler.inverse_transform(logit.reshape(-1, 1)))\n",
    "        labels.append(dataset.scaler.inverse_transform(torch.FloatTensor(dataset.passengers)[sliding_window_length+i].reshape(-1, 1)))\n",
    "    \n",
    "    print(output)\n",
    "    print(labels)\n",
    "\n",
    "    toutput = torch.tensor(output)\n",
    "    tlabels = torch.tensor(labels)\n",
    "    # mape (mean absolute percentage error)\n",
    "    # mean((actual-forecast) / actual)\n",
    "    mape = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)) / torch.squeeze(tlabels))) # 0.1580, 0.1418(12-batch), 0.1343(sliding window)\n",
    "    print(mape)\n",
    "\n",
    "    # mae (mean absolute error)\n",
    "    mae = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)))) # 82.5248, 71.5019, 69.0149\n",
    "    print(mae)\n",
    "\n",
    "    # rmse (root mean square error)\n",
    "    rmse = torch.sqrt(torch.mean(torch.square((torch.squeeze(toutput)-torch.squeeze(tlabels))))) # 112.3071, 94.6528, 91.1919\n",
    "    print(rmse)\n",
    "\n",
    "finally:\n",
    "    torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1e967",
   "metadata": {},
   "source": [
    "# Sliding Window 1 in 1 out Cell Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9687f78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH_LOSS 0 tensor(0.0150, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 1 tensor(0.0216, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 2 tensor(0.0205, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 3 tensor(0.0196, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 4 tensor(0.0186, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 5 tensor(0.0177, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 6 tensor(0.0169, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 7 tensor(0.0161, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 8 tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 9 tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 10 tensor(0.0141, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 11 tensor(0.0136, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 12 tensor(0.0132, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 13 tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 14 tensor(0.0124, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 15 tensor(0.0120, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 16 tensor(0.0117, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 17 tensor(0.0115, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 18 tensor(0.0112, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 19 tensor(0.0110, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 20 tensor(0.0107, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 21 tensor(0.0105, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 22 tensor(0.0104, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 23 tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 24 tensor(0.0100, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 25 tensor(0.0099, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 26 tensor(0.0097, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 27 tensor(0.0096, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 28 tensor(0.0095, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 29 tensor(0.0094, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 30 tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 31 tensor(0.0092, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 32 tensor(0.0091, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 33 tensor(0.0090, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 34 tensor(0.0089, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 35 tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 36 tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 37 tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 38 tensor(0.0087, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 39 tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 40 tensor(0.0086, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 41 tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 42 tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 43 tensor(0.0085, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 44 tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 45 tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 46 tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 47 tensor(0.0084, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 48 tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 49 tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 50 tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 51 tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 52 tensor(0.0083, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 53 tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 54 tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 55 tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 56 tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 57 tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 58 tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 59 tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 60 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 61 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 62 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 63 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 64 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 65 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 66 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 67 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 68 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 69 tensor(0.0081, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 70 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 71 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 72 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 73 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 74 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 75 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 76 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 77 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 78 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 79 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 80 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 81 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 82 tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 83 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 84 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 85 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 86 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 87 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 88 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 89 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 90 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 91 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 92 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 93 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 94 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 95 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 96 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 97 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 98 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 99 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 100 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 101 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 102 tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 103 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 104 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 105 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 106 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 107 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 108 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 109 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 110 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 111 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 112 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 113 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 114 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 115 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 116 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 117 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 118 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 119 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 120 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 121 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 122 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 123 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 124 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 125 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 126 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 127 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 128 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 129 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 130 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 131 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 132 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 133 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 134 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 135 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 136 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 137 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 138 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 139 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 140 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 141 tensor(0.0078, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 142 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 143 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 144 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 145 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 146 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 147 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 148 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 149 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 150 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 151 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 152 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 153 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 154 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 155 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 156 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 157 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 158 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 159 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 160 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 161 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 162 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 163 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 164 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 165 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 166 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 167 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 168 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 169 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 170 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 171 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 172 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 173 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 174 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 175 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 176 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 177 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 178 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 179 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 180 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 181 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 182 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 183 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 184 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 185 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 186 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 187 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 188 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 189 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 190 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 191 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 192 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 193 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 194 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 195 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 196 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 197 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 198 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 199 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 200 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 201 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 202 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 203 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 204 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 205 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 206 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 207 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 208 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 209 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 210 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 211 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 212 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 213 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 214 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 215 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 216 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 217 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 218 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 219 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 220 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 221 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 222 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 223 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 224 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 225 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 226 tensor(0.0077, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 227 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 228 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 229 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 230 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 231 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 232 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 233 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 234 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 235 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 236 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 237 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 238 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 239 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 240 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 241 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 242 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 243 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 244 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 245 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 246 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 247 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 248 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 249 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 250 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 251 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 252 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 253 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 254 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 255 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 256 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 257 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 258 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 259 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 260 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 261 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 262 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 263 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 264 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 265 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 266 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 267 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 268 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 269 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 270 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 271 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 272 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 273 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 274 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 275 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 276 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 277 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 278 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 279 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 280 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 281 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 282 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 283 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 284 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 285 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 286 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 287 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 288 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 289 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 290 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 291 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 292 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 293 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 294 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 295 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 296 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 297 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 298 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 299 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 300 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 301 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 302 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 303 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 304 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 305 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 306 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 307 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 308 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 309 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 310 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 311 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 312 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 313 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 314 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 315 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 316 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 317 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 318 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 319 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 320 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 321 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 322 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 323 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 324 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 325 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 326 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 327 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 328 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 329 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 330 tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 331 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 332 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 333 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 334 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 335 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 336 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 337 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 338 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 339 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 340 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 341 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 342 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 343 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 344 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 345 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 346 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 347 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 348 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 349 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 350 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 351 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 352 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 353 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 354 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 355 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 356 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 357 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 358 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 359 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 360 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 361 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 362 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 363 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 364 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 365 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 366 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 367 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 368 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 369 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 370 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 371 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 372 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 373 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 374 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 375 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 376 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 377 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 378 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 379 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 380 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 381 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 382 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 383 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 384 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 385 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 386 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 387 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 388 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 389 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 390 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 391 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 392 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 393 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 394 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 395 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 396 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 397 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 398 tensor(0.0075, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 399 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 400 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 401 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 402 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 403 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 404 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 405 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 406 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 407 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 408 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 409 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 410 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 411 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 412 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 413 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 414 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 415 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 416 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 417 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 418 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 419 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 420 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 421 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 422 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 423 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 424 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 425 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 426 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 427 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 428 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 429 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 430 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 431 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 432 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 433 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 434 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 435 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 436 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 437 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 438 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 439 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 440 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 441 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 442 tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 443 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 444 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 445 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 446 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 447 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 448 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 449 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 450 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 451 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 452 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 453 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 454 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 455 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 456 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 457 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 458 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 459 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 460 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 461 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 462 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 463 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 464 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 465 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 466 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 467 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 468 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 469 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 470 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 471 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 472 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 473 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 474 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 475 tensor(0.0073, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 476 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 477 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 478 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 479 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 480 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 481 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 482 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 483 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 484 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 485 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 486 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 487 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 488 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 489 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 490 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 491 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 492 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 493 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 494 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 495 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 496 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 497 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 498 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 499 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 500 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 501 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 502 tensor(0.0072, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 503 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 504 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 505 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 506 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 507 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 508 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 509 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 510 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 511 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 512 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 513 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 514 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 515 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 516 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 517 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 518 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 519 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 520 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 521 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 522 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 523 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 524 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 525 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 526 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 527 tensor(0.0071, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 528 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 529 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 530 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 531 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 532 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 533 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 534 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 535 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 536 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 537 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 538 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 539 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 540 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 541 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 542 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 543 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 544 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 545 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 546 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 547 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 548 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 549 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 550 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 551 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 552 tensor(0.0070, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 553 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 554 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 555 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 556 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 557 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 558 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 559 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 560 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 561 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 562 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 563 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 564 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 565 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 566 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 567 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 568 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 569 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 570 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 571 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 572 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 573 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 574 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 575 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 576 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 577 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 578 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 579 tensor(0.0069, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 580 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 581 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 582 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 583 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 584 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 585 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 586 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 587 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 588 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 589 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 590 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 591 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 592 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 593 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 594 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 595 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 596 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 597 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 598 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 599 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 600 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 601 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 602 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 603 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 604 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 605 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 606 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 607 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 608 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 609 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 610 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 611 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 612 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 613 tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 614 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 615 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 616 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 617 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 618 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 619 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 620 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 621 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 622 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 623 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 624 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 625 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 626 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 627 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 628 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 629 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 630 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 631 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 632 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 633 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 634 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 635 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 636 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 637 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 638 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 639 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 640 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 641 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 642 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 643 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 644 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 645 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 646 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 647 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 648 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 649 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 650 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 651 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 652 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 653 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 654 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 655 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 656 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 657 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 658 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 659 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 660 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 661 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 662 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 663 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 664 tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 665 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 666 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 667 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 668 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 669 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 670 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 671 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 672 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 673 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 674 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 675 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 676 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 677 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 678 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 679 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 680 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 681 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 682 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 683 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 684 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 685 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 686 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 687 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 688 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 689 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 690 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 691 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 692 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 693 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 694 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 695 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 696 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 697 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 698 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 699 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 700 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 701 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 702 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 703 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 704 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 705 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 706 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 707 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 708 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 709 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 710 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 711 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 712 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 713 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 714 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 715 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 716 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 717 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 718 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 719 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 720 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 721 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 722 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 723 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 724 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 725 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 726 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 727 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 728 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 729 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 730 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 731 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 732 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 733 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 734 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 735 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 736 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 737 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 738 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 739 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 740 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 741 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 742 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 743 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 744 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 745 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 746 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 747 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 748 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 749 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 750 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 751 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 752 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 753 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 754 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 755 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 756 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 757 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 758 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 759 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 760 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 761 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 762 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 763 tensor(0.0066, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 764 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 765 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 766 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 767 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 768 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 769 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 770 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 771 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 772 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 773 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 774 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 775 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 776 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 777 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 778 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 779 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 780 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 781 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 782 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 783 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 784 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 785 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 786 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 787 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 788 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 789 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 790 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 791 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 792 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 793 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 794 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 795 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 796 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 797 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 798 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 799 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 800 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 801 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 802 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 803 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 804 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 805 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 806 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 807 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 808 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 809 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 810 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 811 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 812 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 813 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 814 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 815 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 816 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 817 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 818 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 819 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 820 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 821 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 822 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 823 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 824 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 825 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 826 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 827 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 828 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 829 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 830 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 831 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 832 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 833 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 834 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 835 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 836 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 837 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 838 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 839 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 840 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 841 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 842 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 843 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 844 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 845 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 846 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 847 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 848 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 849 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 850 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 851 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 852 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 853 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 854 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 855 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 856 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 857 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 858 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 859 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 860 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 861 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 862 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 863 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 864 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 865 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 866 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 867 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 868 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 869 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 870 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 871 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 872 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 873 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 874 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 875 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 876 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 877 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 878 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 879 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 880 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 881 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 882 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 883 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 884 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 885 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 886 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 887 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 888 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 889 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 890 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 891 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 892 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 893 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 894 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 895 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 896 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 897 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 898 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 899 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 900 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 901 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 902 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 903 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 904 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 905 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 906 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 907 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 908 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 909 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 910 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 911 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 912 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 913 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 914 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 915 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 916 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 917 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 918 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 919 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 920 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 921 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 922 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 923 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 924 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 925 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 926 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 927 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 928 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 929 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 930 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 931 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 932 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 933 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 934 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 935 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 936 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 937 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 938 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 939 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 940 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 941 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 942 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 943 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 944 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 945 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 946 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 947 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 948 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 949 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 950 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 951 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 952 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 953 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 954 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 955 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 956 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 957 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 958 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 959 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 960 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 961 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 962 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 963 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 964 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 965 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 966 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 967 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 968 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 969 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 970 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 971 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 972 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 973 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 974 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 975 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 976 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 977 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 978 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 979 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 980 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 981 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 982 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 983 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 984 tensor(0.0065, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 985 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 986 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 987 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 988 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 989 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 990 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 991 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 992 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 993 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 994 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 995 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 996 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 997 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 998 tensor(0.0064, grad_fn=<DivBackward0>)\n",
      "EPOCH_LOSS 999 tensor(0.0064, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "initial_lr = 5e-2\n",
    "cell = LSTMCell()\n",
    "optimizer = optim.SGD(cell.parameters(), lr=initial_lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50, threshold=0.0005)\n",
    "a0 = torch.zeros((64), requires_grad=True).to(torch.float32)\n",
    "c0 = torch.zeros((64), requires_grad=True).to(torch.float32)\n",
    "\n",
    "EPOCH=1000\n",
    "sliding_window_length = 12\n",
    "\n",
    "for ep in range(EPOCH):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, split-sliding_window_length):\n",
    "        batch_loss = 0.0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        a_prev = a0\n",
    "        c_prev = c0\n",
    "        passengers = torch.tensor(np.array(dataset.passengers[i:i+sliding_window_length])).to(torch.float32)\n",
    "        \n",
    "        if len(passengers)<2:\n",
    "            continue\n",
    "\n",
    "        for p in range(len(passengers) - 1):\n",
    "            logits, a, c = cell(passengers[p], a_prev, c_prev)\n",
    "            target = passengers[p+1].to(dtype=logits.dtype, device=logits.device).view_as(logits)\n",
    "            loss = criterion(logits, target)\n",
    "            batch_loss += loss\n",
    "            a_prev = a.detach()\n",
    "            c_prev = c.detach()\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(\"Batch avg loss\", batch_loss.item() / 4)\n",
    "        epoch_loss += batch_loss / 4\n",
    "\n",
    "    print(\"EPOCH_LOSS\", ep, epoch_loss / (split-sliding_window_length))\n",
    "    scheduler.step(epoch_loss / (split-sliding_window_length))\n",
    "    if initial_lr != optimizer.param_groups[0]['lr']:\n",
    "        print(f\"LR Changed {optimizer.param_groups[0]['lr']}\")\n",
    "        initial_lr = optimizer.param_groups[0]['lr']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843899ed",
   "metadata": {},
   "source": [
    "# First ever Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "772413f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]]), array([[297.22126901]])]\n",
      "[array([[361.99999839]]), array([[405.00000584]]), array([[416.99999428]]), array([[391.00001419]]), array([[418.9999975]]), array([[461.00000334]]), array([[472.0000056]]), array([[534.99999893]]), array([[622.]]), array([[606.00000513]]), array([[508.00000179]]), array([[461.00000334]]), array([[389.99999714]])]\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    output = []\n",
    "    labels = []\n",
    "    input, a_prev, c_prev = torch.tensor([test_loader.dataset[0][1]]), torch.zeros(64).to(torch.float32), torch.zeros(64).to(torch.float32)\n",
    "\n",
    "    for i in range(1,14):\n",
    "        logits, act, c = cell(input, a_prev, c_prev)\n",
    "        output.append(dataset.scaler.inverse_transform(logits.reshape(-1, 1)))\n",
    "        labels.append(dataset.scaler.inverse_transform(test_loader.dataset[i][1].reshape(-1, 1)))\n",
    "        input = logits\n",
    "        a_prev = act\n",
    "        c_prev = c\n",
    "    print(output)\n",
    "    print(labels)\n",
    "finally:\n",
    "    torch.set_grad_enabled(True)\n",
    "# (tensor([0.0154]), tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "48f90beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3440, dtype=torch.float64)\n",
      "tensor(168.0864, dtype=torch.float64)\n",
      "tensor(185.7166, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "toutput = torch.tensor(output)\n",
    "tlabels = torch.tensor(labels)\n",
    "# mape (mean absolute percentage error)\n",
    "# mean((actual-forecast) / actual)\n",
    "mape = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)) / torch.squeeze(tlabels))) # 0.1580, 0.1418(12-batch), 0.1343(sliding window)\n",
    "print(mape)\n",
    "\n",
    "# mae (mean absolute error)\n",
    "mae = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)))) # 82.5248, 71.5019, 69.0149\n",
    "print(mae)\n",
    "\n",
    "# rmse (root mean square error)\n",
    "rmse = torch.sqrt(torch.mean(torch.square((torch.squeeze(toutput)-torch.squeeze(tlabels))))) # 112.3071, 94.6528, 91.1919\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f66fd",
   "metadata": {},
   "source": [
    "# LSTM Stacked with Month Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "209c3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64):\n",
    "        super().__init__()\n",
    "        combined_size = input_size + hidden_size\n",
    "        # forget gate\n",
    "        self.forget = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # update gate\n",
    "        self.update = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # tanh\n",
    "        self.candidate = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        # output gate\n",
    "        self.output = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "\n",
    "        # final fc\n",
    "        self.final = nn.Linear(hidden_size, input_size, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x, prev_a, prev_c): # hidden state and long-term memory\n",
    "        # print(\"x, prev_a, prev_c\", x, prev_a, prev_c)\n",
    "        input_vector = torch.concat((x, prev_a))\n",
    "        # print(\"input_vector\", input_vector)\n",
    "        forget = torch.sigmoid(self.forget(input_vector))\n",
    "        # print(\"forget\", forget)\n",
    "        update = torch.sigmoid(self.update(input_vector))\n",
    "        # print(\"update\", update)\n",
    "        candidate = torch.tanh(self.candidate(input_vector))\n",
    "        # print(\"candidate\", candidate)\n",
    "        output = torch.sigmoid(self.output(input_vector))\n",
    "\n",
    "        # print(\"output\", output)\n",
    "        c = (forget * prev_c) + (update * candidate)\n",
    "        # print(\"c\", c)\n",
    "\n",
    "        a = output * torch.tanh(c)\n",
    "        # print(\"a\", a)\n",
    "\n",
    "        # x = self.final(a)\n",
    "        # print(\"x\", x)\n",
    "\n",
    "        return x, a, c\n",
    "\n",
    "\n",
    "class LSTMStacked(nn.Module):\n",
    "\n",
    "    def __init__(self, cells=12, input_size=1, hidden_size=64, output_size=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = cells\n",
    "        self.cells = nn.Sequential(LSTMCell(input_size, hidden_size), *(LSTMCell(hidden_size, hidden_size) for _ in range(cells)))\n",
    "        self.final = nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x, prev_a_c_list): # hidden state and long-term memory\n",
    "        new_a_c_list = []\n",
    "        for c in range(self.num_layers):\n",
    "            prev_a, prev_c = prev_a_c_list[c]\n",
    "            _, a, c = self.cells[c](x, prev_a, prev_c)\n",
    "            new_a_c_list.append((a, c))\n",
    "            x = a\n",
    "        x = self.final(x)\n",
    "        return x, new_a_c_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "340e798e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131    12\n",
       "132     1\n",
       "133     2\n",
       "134     3\n",
       "135     4\n",
       "136     5\n",
       "137     6\n",
       "138     7\n",
       "139     8\n",
       "140     9\n",
       "141    10\n",
       "142    11\n",
       "Name: Month, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passengers_with_month = torch.tensor(np.array(dataset)).to(torch.float32)\n",
    "# passengers_with_month\n",
    "month_indexes[i:i+sliding_window_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9011b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH_LOSS 0 0.0017335556437689196\n",
      "EPOCH_LOSS 1 0.0017770389773648259\n",
      "EPOCH_LOSS 2 0.0017699037877047087\n",
      "EPOCH_LOSS 3 0.0017718729914622793\n",
      "EPOCH_LOSS 4 0.0017739763188796848\n",
      "EPOCH_LOSS 5 0.001775815128223943\n",
      "EPOCH_LOSS 6 0.0017773960257034056\n",
      "EPOCH_LOSS 7 0.001778746501036783\n",
      "EPOCH_LOSS 8 0.0017798815458411014\n",
      "EPOCH_LOSS 9 0.0017808134634712393\n",
      "EPOCH_LOSS 10 0.0017815558775939725\n",
      "EPOCH_LOSS 11 0.0017821128397699833\n",
      "EPOCH_LOSS 12 0.0017824973417750562\n",
      "EPOCH_LOSS 13 0.0017827199506369074\n",
      "EPOCH_LOSS 14 0.0017827822903873176\n",
      "EPOCH_LOSS 15 0.0017826997854787638\n",
      "EPOCH_LOSS 16 0.001782477886660811\n",
      "EPOCH_LOSS 17 0.0017821248465779756\n",
      "EPOCH_LOSS 18 0.0017816501047649349\n",
      "EPOCH_LOSS 19 0.0017810610687556633\n",
      "EPOCH_LOSS 20 0.001780366352070899\n",
      "EPOCH_LOSS 21 0.0017795705370112285\n",
      "EPOCH_LOSS 22 0.001778685709670373\n",
      "EPOCH_LOSS 23 0.0017777118130497785\n",
      "EPOCH_LOSS 24 0.0017766615994129864\n",
      "EPOCH_LOSS 25 0.0017755385063948953\n",
      "EPOCH_LOSS 26 0.0017743486454761485\n",
      "EPOCH_LOSS 27 0.0017730976636639233\n",
      "EPOCH_LOSS 28 0.0017717895938801818\n",
      "EPOCH_LOSS 29 0.001770431875261623\n",
      "EPOCH_LOSS 30 0.001769023686127867\n",
      "EPOCH_LOSS 31 0.0017675777907615205\n",
      "EPOCH_LOSS 32 0.001766089953913352\n",
      "EPOCH_LOSS 33 0.0017645666040164976\n",
      "EPOCH_LOSS 34 0.0017630139744944438\n",
      "EPOCH_LOSS 35 0.0017614288503266181\n",
      "EPOCH_LOSS 36 0.0017598213065080335\n",
      "EPOCH_LOSS 37 0.0017581919528629952\n",
      "EPOCH_LOSS 38 0.0017565386224022862\n",
      "EPOCH_LOSS 39 0.0017548676177658968\n",
      "EPOCH_LOSS 40 0.0017531821644868073\n",
      "EPOCH_LOSS 41 0.0017514827059692216\n",
      "EPOCH_LOSS 42 0.0017497700161837485\n",
      "EPOCH_LOSS 43 0.0017480483785399807\n",
      "EPOCH_LOSS 44 0.0017463164820613068\n",
      "EPOCH_LOSS 45 0.001744574111511336\n",
      "EPOCH_LOSS 46 0.001742829557609541\n",
      "EPOCH_LOSS 47 0.001741076853506389\n",
      "EPOCH_LOSS 48 0.0017393226388260237\n",
      "EPOCH_LOSS 49 0.0017375631614802555\n",
      "EPOCH_LOSS 50 0.0017358033192492938\n",
      "EPOCH_LOSS 51 0.001734040940452283\n",
      "LR Changed 1.0000000000000002e-06\n",
      "EPOCH_LOSS 52 0.0019085248670553754\n",
      "EPOCH_LOSS 53 0.001758862721552469\n",
      "EPOCH_LOSS 54 0.0017529454192134846\n",
      "EPOCH_LOSS 55 0.001752942512325717\n",
      "EPOCH_LOSS 56 0.00175162522041795\n",
      "EPOCH_LOSS 57 0.0017497724247963324\n",
      "EPOCH_LOSS 58 0.001747817865594755\n",
      "EPOCH_LOSS 59 0.0017458986988434016\n",
      "EPOCH_LOSS 60 0.0017440524227572216\n",
      "EPOCH_LOSS 61 0.0017422824947257271\n",
      "EPOCH_LOSS 62 0.0017405888706454168\n",
      "EPOCH_LOSS 63 0.001738967304569889\n",
      "EPOCH_LOSS 64 0.0017374125003310884\n",
      "EPOCH_LOSS 65 0.0017359240721897496\n",
      "EPOCH_LOSS 66 0.001734491904062646\n",
      "EPOCH_LOSS 67 0.0017331190567144868\n",
      "EPOCH_LOSS 68 0.0017317972700594902\n",
      "EPOCH_LOSS 69 0.0017305298798466857\n",
      "EPOCH_LOSS 70 0.001729305347179906\n",
      "EPOCH_LOSS 71 0.0017281230509311133\n",
      "EPOCH_LOSS 72 0.0017269870742240574\n",
      "EPOCH_LOSS 73 0.0017258896641797388\n",
      "EPOCH_LOSS 74 0.0017248267026935987\n",
      "EPOCH_LOSS 75 0.0017238011322127443\n",
      "EPOCH_LOSS 76 0.0017228076986679042\n",
      "EPOCH_LOSS 77 0.0017218441387946309\n",
      "EPOCH_LOSS 78 0.0017209073899618263\n",
      "EPOCH_LOSS 79 0.0017200020555744938\n",
      "EPOCH_LOSS 80 0.0017191228479266343\n",
      "EPOCH_LOSS 81 0.0017182664891313244\n",
      "EPOCH_LOSS 82 0.001717437477819487\n",
      "EPOCH_LOSS 83 0.001716624839215724\n",
      "EPOCH_LOSS 84 0.0017158384033185332\n",
      "EPOCH_LOSS 85 0.0017150662624864249\n",
      "EPOCH_LOSS 86 0.0017143099882776594\n",
      "EPOCH_LOSS 87 0.0017135748915733467\n",
      "EPOCH_LOSS 88 0.0017128569232063922\n",
      "EPOCH_LOSS 89 0.0017121532944197185\n",
      "EPOCH_LOSS 90 0.001711465329449272\n",
      "EPOCH_LOSS 91 0.0017107903784399708\n",
      "EPOCH_LOSS 92 0.0017101276481076525\n",
      "EPOCH_LOSS 93 0.0017094770873738903\n",
      "EPOCH_LOSS 94 0.0017088360840440675\n",
      "EPOCH_LOSS 95 0.0017082141165528897\n",
      "EPOCH_LOSS 96 0.0017075976792610078\n",
      "EPOCH_LOSS 97 0.001706991902631435\n",
      "EPOCH_LOSS 98 0.0017063954248617704\n",
      "EPOCH_LOSS 99 0.0017058079886943076\n",
      "EPOCH_LOSS 100 0.0017052267628655726\n",
      "EPOCH_LOSS 101 0.0017046561445664917\n",
      "EPOCH_LOSS 102 0.001704089084621797\n",
      "EPOCH_LOSS 103 0.0017035345496138898\n",
      "EPOCH_LOSS 104 0.0017029838945242953\n",
      "EPOCH_LOSS 105 0.0017024451481977858\n",
      "EPOCH_LOSS 106 0.0017019073469755179\n",
      "EPOCH_LOSS 107 0.0017013776823018168\n",
      "EPOCH_LOSS 108 0.001700851998000996\n",
      "EPOCH_LOSS 109 0.0017003328107707757\n",
      "EPOCH_LOSS 110 0.0016998196613234198\n",
      "EPOCH_LOSS 111 0.0016993106745931126\n",
      "EPOCH_LOSS 112 0.001698807511235127\n",
      "EPOCH_LOSS 113 0.0016983094725613647\n",
      "EPOCH_LOSS 114 0.00169781441932511\n",
      "EPOCH_LOSS 115 0.0016973237973524167\n",
      "EPOCH_LOSS 116 0.001696835258100682\n",
      "EPOCH_LOSS 117 0.0016963516808505223\n",
      "EPOCH_LOSS 118 0.001695873233775667\n",
      "EPOCH_LOSS 119 0.001695397937940036\n",
      "EPOCH_LOSS 120 0.0016949247150821979\n",
      "EPOCH_LOSS 121 0.0016944559779796713\n",
      "EPOCH_LOSS 122 0.0016939903694437243\n",
      "EPOCH_LOSS 123 0.0016935270486716778\n",
      "EPOCH_LOSS 124 0.0016930657267111254\n",
      "EPOCH_LOSS 125 0.001692609864272608\n",
      "EPOCH_LOSS 126 0.0016921542706412741\n",
      "EPOCH_LOSS 127 0.0016917012031286201\n",
      "EPOCH_LOSS 128 0.0016912500175204673\n",
      "EPOCH_LOSS 129 0.0016908017438782915\n",
      "EPOCH_LOSS 130 0.0016903573714959056\n",
      "EPOCH_LOSS 131 0.0016899129825149986\n",
      "EPOCH_LOSS 132 0.001689472542565431\n",
      "EPOCH_LOSS 133 0.0016890332305972335\n",
      "EPOCH_LOSS 134 0.0016885945636388403\n",
      "EPOCH_LOSS 135 0.0016881595971655165\n",
      "EPOCH_LOSS 136 0.0016877264265143807\n",
      "EPOCH_LOSS 137 0.0016872949113933247\n",
      "EPOCH_LOSS 138 0.0016868655453090157\n",
      "EPOCH_LOSS 139 0.0016864366529610391\n",
      "EPOCH_LOSS 140 0.0016860116655285644\n",
      "EPOCH_LOSS 141 0.0016855860274404464\n",
      "EPOCH_LOSS 142 0.0016851622865927712\n",
      "EPOCH_LOSS 143 0.0016847408806367453\n",
      "EPOCH_LOSS 144 0.0016843209495865845\n",
      "EPOCH_LOSS 145 0.0016839009303663294\n",
      "EPOCH_LOSS 146 0.001683481109753336\n",
      "EPOCH_LOSS 147 0.001683065309447742\n",
      "EPOCH_LOSS 148 0.0016826492827039599\n",
      "EPOCH_LOSS 149 0.0016822356883266303\n",
      "EPOCH_LOSS 150 0.0016818227726384664\n",
      "EPOCH_LOSS 151 0.0016814118995558858\n",
      "EPOCH_LOSS 152 0.0016810007734768513\n",
      "EPOCH_LOSS 153 0.0016805902015937008\n",
      "EPOCH_LOSS 154 0.0016801830728461501\n",
      "EPOCH_LOSS 155 0.0016797736876728525\n",
      "EPOCH_LOSS 156 0.0016793677244996057\n",
      "EPOCH_LOSS 157 0.0016789623263142328\n",
      "EPOCH_LOSS 158 0.0016785580786306698\n",
      "EPOCH_LOSS 159 0.001678155896702294\n",
      "EPOCH_LOSS 160 0.00167775152116525\n",
      "EPOCH_LOSS 161 0.0016773521796808945\n",
      "EPOCH_LOSS 162 0.0016769508182386858\n",
      "EPOCH_LOSS 163 0.0016765495614580858\n",
      "EPOCH_LOSS 164 0.0016761494788536078\n",
      "EPOCH_LOSS 165 0.0016757499875606082\n",
      "EPOCH_LOSS 166 0.00167535350102239\n",
      "EPOCH_LOSS 167 0.0016749560540643253\n",
      "EPOCH_LOSS 168 0.0016745603149026492\n",
      "EPOCH_LOSS 169 0.0016741663112303095\n",
      "EPOCH_LOSS 170 0.0016737707549999863\n",
      "EPOCH_LOSS 171 0.0016733769208978812\n",
      "EPOCH_LOSS 172 0.0016729831560638544\n",
      "EPOCH_LOSS 173 0.0016725915529124135\n",
      "EPOCH_LOSS 174 0.0016721999895892615\n",
      "EPOCH_LOSS 175 0.0016718091719837471\n",
      "EPOCH_LOSS 176 0.0016714194538300369\n",
      "EPOCH_LOSS 177 0.0016710305045547424\n",
      "EPOCH_LOSS 178 0.0016706407795584449\n",
      "EPOCH_LOSS 179 0.0016702513084302002\n",
      "EPOCH_LOSS 180 0.0016698641052285354\n",
      "EPOCH_LOSS 181 0.0016694771140100687\n",
      "EPOCH_LOSS 182 0.0016690903189424506\n",
      "EPOCH_LOSS 183 0.0016687041390032178\n",
      "EPOCH_LOSS 184 0.0016683198249285607\n",
      "EPOCH_LOSS 185 0.0016679350692062887\n",
      "EPOCH_LOSS 186 0.0016675516262767021\n",
      "EPOCH_LOSS 187 0.0016671683266753672\n",
      "EPOCH_LOSS 188 0.0016667850485023005\n",
      "EPOCH_LOSS 189 0.0016664036955784211\n",
      "EPOCH_LOSS 190 0.0016660212585392863\n",
      "EPOCH_LOSS 191 0.0016656400248680411\n",
      "EPOCH_LOSS 192 0.0016652586704220213\n",
      "EPOCH_LOSS 193 0.0016648793104658169\n",
      "EPOCH_LOSS 194 0.0016644987307817495\n",
      "EPOCH_LOSS 195 0.0016641208082637021\n",
      "EPOCH_LOSS 196 0.0016637429728236994\n",
      "EPOCH_LOSS 197 0.0016633667498740772\n",
      "EPOCH_LOSS 198 0.0016629891152715904\n",
      "EPOCH_LOSS 199 0.001662611683014427\n",
      "EPOCH_LOSS 200 0.0016622353471762042\n",
      "EPOCH_LOSS 201 0.0016618589975432498\n",
      "EPOCH_LOSS 202 0.0016614832563289705\n",
      "EPOCH_LOSS 203 0.0016611080509416488\n",
      "EPOCH_LOSS 204 0.0016607351129724738\n",
      "EPOCH_LOSS 205 0.0016603610843680548\n",
      "EPOCH_LOSS 206 0.0016599862379374493\n",
      "EPOCH_LOSS 207 0.0016596146926772196\n",
      "EPOCH_LOSS 208 0.0016592414469441714\n",
      "EPOCH_LOSS 209 0.0016588691442116195\n",
      "EPOCH_LOSS 210 0.0016584967232073466\n",
      "EPOCH_LOSS 211 0.0016581261326563859\n",
      "EPOCH_LOSS 212 0.0016577550261340683\n",
      "EPOCH_LOSS 213 0.0016573827129984422\n",
      "EPOCH_LOSS 214 0.0016570124643489857\n",
      "EPOCH_LOSS 215 0.0016566428690548704\n",
      "EPOCH_LOSS 216 0.0016562738442553965\n",
      "EPOCH_LOSS 217 0.001655905657154957\n",
      "EPOCH_LOSS 218 0.0016555372283815642\n",
      "EPOCH_LOSS 219 0.0016551685585510219\n",
      "EPOCH_LOSS 220 0.001654802274005399\n",
      "EPOCH_LOSS 221 0.0016544364762192025\n",
      "EPOCH_LOSS 222 0.0016540691274972465\n",
      "EPOCH_LOSS 223 0.001653702625089504\n",
      "EPOCH_LOSS 224 0.0016533362624354363\n",
      "EPOCH_LOSS 225 0.0016529713556205177\n",
      "EPOCH_LOSS 226 0.0016526080891645588\n",
      "EPOCH_LOSS 227 0.0016522437025271672\n",
      "EPOCH_LOSS 228 0.0016518801635487972\n",
      "EPOCH_LOSS 229 0.0016515172464213969\n",
      "EPOCH_LOSS 230 0.0016511541912713737\n",
      "EPOCH_LOSS 231 0.001650791182433433\n",
      "EPOCH_LOSS 232 0.001650429203307709\n",
      "EPOCH_LOSS 233 0.001650066921299601\n",
      "EPOCH_LOSS 234 0.0016497065001463175\n",
      "EPOCH_LOSS 235 0.0016493435499027674\n",
      "EPOCH_LOSS 236 0.0016489836074867008\n",
      "EPOCH_LOSS 237 0.0016486222438583584\n",
      "EPOCH_LOSS 238 0.0016482635947386564\n",
      "EPOCH_LOSS 239 0.0016479032002621838\n",
      "EPOCH_LOSS 240 0.0016475441548942967\n",
      "EPOCH_LOSS 241 0.0016471852125334863\n",
      "EPOCH_LOSS 242 0.0016468270251689086\n",
      "EPOCH_LOSS 243 0.0016464694474100376\n",
      "EPOCH_LOSS 244 0.00164611113761591\n",
      "EPOCH_LOSS 245 0.0016457537672936143\n",
      "EPOCH_LOSS 246 0.0016453967566816957\n",
      "EPOCH_LOSS 247 0.0016450388520463236\n",
      "EPOCH_LOSS 248 0.0016446817513270193\n",
      "EPOCH_LOSS 249 0.0016443265637435451\n",
      "EPOCH_LOSS 250 0.001643970738376304\n",
      "EPOCH_LOSS 251 0.001643616200276867\n",
      "EPOCH_LOSS 252 0.0016432614011054455\n",
      "EPOCH_LOSS 253 0.0016429069844558597\n",
      "EPOCH_LOSS 254 0.0016425531438854165\n",
      "EPOCH_LOSS 255 0.0016421989595477174\n",
      "EPOCH_LOSS 256 0.0016418459704919317\n",
      "EPOCH_LOSS 257 0.0016414917478698257\n",
      "EPOCH_LOSS 258 0.0016411399472612608\n",
      "EPOCH_LOSS 259 0.0016407886036083166\n",
      "EPOCH_LOSS 260 0.0016404367575310896\n",
      "EPOCH_LOSS 261 0.0016400855120376203\n",
      "EPOCH_LOSS 262 0.0016397343889938633\n",
      "EPOCH_LOSS 263 0.0016393828574660105\n",
      "EPOCH_LOSS 264 0.0016390333378770812\n",
      "EPOCH_LOSS 265 0.0016386840210615016\n",
      "EPOCH_LOSS 266 0.0016383343181371208\n",
      "EPOCH_LOSS 267 0.0016379855362138615\n",
      "EPOCH_LOSS 268 0.0016376369807696012\n",
      "EPOCH_LOSS 269 0.0016372874412677418\n",
      "EPOCH_LOSS 270 0.0016369392343234378\n",
      "EPOCH_LOSS 271 0.001636592446261923\n",
      "EPOCH_LOSS 272 0.0016362448328032366\n",
      "EPOCH_LOSS 273 0.0016358980061850006\n",
      "EPOCH_LOSS 274 0.001635550945313092\n",
      "EPOCH_LOSS 275 0.0016352044978535777\n",
      "EPOCH_LOSS 276 0.001634857778160242\n",
      "EPOCH_LOSS 277 0.0016345109412454523\n",
      "EPOCH_LOSS 278 0.001634165574989061\n",
      "EPOCH_LOSS 279 0.0016338208941087107\n",
      "EPOCH_LOSS 280 0.0016334762431627363\n",
      "EPOCH_LOSS 281 0.0016331309131057646\n",
      "EPOCH_LOSS 282 0.0016327870343430192\n",
      "EPOCH_LOSS 283 0.0016324430652719926\n",
      "EPOCH_LOSS 284 0.0016321007755537823\n",
      "EPOCH_LOSS 285 0.0016317573160846129\n",
      "EPOCH_LOSS 286 0.0016314137635399324\n",
      "EPOCH_LOSS 287 0.001631072362640359\n",
      "EPOCH_LOSS 288 0.0016307316290519152\n",
      "EPOCH_LOSS 289 0.0016303887795349184\n",
      "EPOCH_LOSS 290 0.0016300477890110025\n",
      "EPOCH_LOSS 291 0.0016297071573414116\n",
      "EPOCH_LOSS 292 0.0016293671420444453\n",
      "EPOCH_LOSS 293 0.0016290275674566309\n",
      "EPOCH_LOSS 294 0.0016286871791445872\n",
      "EPOCH_LOSS 295 0.0016283477205814782\n",
      "EPOCH_LOSS 296 0.001628007735438974\n",
      "EPOCH_LOSS 297 0.0016276685218667491\n",
      "EPOCH_LOSS 298 0.0016273290868396093\n",
      "EPOCH_LOSS 299 0.0016269903705738162\n",
      "EPOCH_LOSS 300 0.0016266510750307702\n",
      "EPOCH_LOSS 301 0.001626312536081524\n",
      "EPOCH_LOSS 302 0.0016259759960081462\n",
      "EPOCH_LOSS 303 0.001625638256127062\n",
      "EPOCH_LOSS 304 0.0016253009960265391\n",
      "EPOCH_LOSS 305 0.0016249649575823342\n",
      "EPOCH_LOSS 306 0.0016246299688912344\n",
      "EPOCH_LOSS 307 0.0016242935307708706\n",
      "EPOCH_LOSS 308 0.0016239588322380044\n",
      "EPOCH_LOSS 309 0.0016236231946179713\n",
      "EPOCH_LOSS 310 0.0016232878667638414\n",
      "EPOCH_LOSS 311 0.0016229534340161964\n",
      "EPOCH_LOSS 312 0.0016226199285941407\n",
      "EPOCH_LOSS 313 0.0016222853287171273\n",
      "EPOCH_LOSS 314 0.0016219510919595174\n",
      "EPOCH_LOSS 315 0.0016216177684195798\n",
      "EPOCH_LOSS 316 0.0016212856698200953\n",
      "EPOCH_LOSS 317 0.0016209532571415915\n",
      "EPOCH_LOSS 318 0.0016206220413490968\n",
      "EPOCH_LOSS 319 0.0016202884657784667\n",
      "EPOCH_LOSS 320 0.0016199566411478031\n",
      "EPOCH_LOSS 321 0.0016196239625397534\n",
      "EPOCH_LOSS 322 0.0016192924396456476\n",
      "EPOCH_LOSS 323 0.0016189618178453057\n",
      "EPOCH_LOSS 324 0.0016186314960281057\n",
      "EPOCH_LOSS 325 0.0016183013242808866\n",
      "EPOCH_LOSS 326 0.001617972827364259\n",
      "EPOCH_LOSS 327 0.0016176436854597053\n",
      "EPOCH_LOSS 328 0.0016173141695546057\n",
      "EPOCH_LOSS 329 0.0016169865091093375\n",
      "EPOCH_LOSS 330 0.0016166583156728758\n",
      "EPOCH_LOSS 331 0.0016163291713136854\n",
      "EPOCH_LOSS 332 0.0016160017047016668\n",
      "EPOCH_LOSS 333 0.001615673885817378\n",
      "EPOCH_LOSS 334 0.0016153447900022241\n",
      "EPOCH_LOSS 335 0.0016150175965735428\n",
      "EPOCH_LOSS 336 0.0016146899136650354\n",
      "EPOCH_LOSS 337 0.001614363096073706\n",
      "EPOCH_LOSS 338 0.0016140371412603281\n",
      "EPOCH_LOSS 339 0.0016137126326973858\n",
      "EPOCH_LOSS 340 0.001613387296935515\n",
      "EPOCH_LOSS 341 0.0016130619736579104\n",
      "EPOCH_LOSS 342 0.001612736954420715\n",
      "EPOCH_LOSS 343 0.0016124129682292673\n",
      "EPOCH_LOSS 344 0.0016120887265554304\n",
      "EPOCH_LOSS 345 0.001611765455642425\n",
      "EPOCH_LOSS 346 0.001611440967634968\n",
      "EPOCH_LOSS 347 0.0016111177762588487\n",
      "EPOCH_LOSS 348 0.0016107946202906758\n",
      "EPOCH_LOSS 349 0.0016104721677028463\n",
      "EPOCH_LOSS 350 0.0016101495084658083\n",
      "EPOCH_LOSS 351 0.0016098276500410966\n",
      "EPOCH_LOSS 352 0.0016095052630642653\n",
      "EPOCH_LOSS 353 0.0016091842675389348\n",
      "EPOCH_LOSS 354 0.0016088624129898732\n",
      "EPOCH_LOSS 355 0.001608541567766164\n",
      "EPOCH_LOSS 356 0.0016082205222175757\n",
      "EPOCH_LOSS 357 0.0016078999776681156\n",
      "EPOCH_LOSS 358 0.0016075798945446526\n",
      "EPOCH_LOSS 359 0.00160725961249555\n",
      "EPOCH_LOSS 360 0.001606939240077679\n",
      "EPOCH_LOSS 361 0.0016066201685719457\n",
      "EPOCH_LOSS 362 0.0016063016132469821\n",
      "EPOCH_LOSS 363 0.0016059825888331272\n",
      "EPOCH_LOSS 364 0.001605665745475309\n",
      "EPOCH_LOSS 365 0.0016053479882065685\n",
      "EPOCH_LOSS 366 0.0016050301369157994\n",
      "EPOCH_LOSS 367 0.0016047127988776198\n",
      "EPOCH_LOSS 368 0.0016043966745173449\n",
      "EPOCH_LOSS 369 0.0016040793241742183\n",
      "EPOCH_LOSS 370 0.001603762598804126\n",
      "EPOCH_LOSS 371 0.001603445942864108\n",
      "EPOCH_LOSS 372 0.0016031304745101269\n",
      "EPOCH_LOSS 373 0.0016028159680216486\n",
      "EPOCH_LOSS 374 0.0016025015845051657\n",
      "EPOCH_LOSS 375 0.001602185120795127\n",
      "EPOCH_LOSS 376 0.0016018700775108823\n",
      "EPOCH_LOSS 377 0.0016015544022440612\n",
      "EPOCH_LOSS 378 0.0016012411930172866\n",
      "EPOCH_LOSS 379 0.001600926113387276\n",
      "EPOCH_LOSS 380 0.0016006126911574615\n",
      "EPOCH_LOSS 381 0.001600299182303826\n",
      "EPOCH_LOSS 382 0.0015999865589866743\n",
      "EPOCH_LOSS 383 0.0015996731914768034\n",
      "EPOCH_LOSS 384 0.0015993601380598363\n",
      "EPOCH_LOSS 385 0.0015990488584446169\n",
      "EPOCH_LOSS 386 0.0015987378197189238\n",
      "EPOCH_LOSS 387 0.001598425644632816\n",
      "EPOCH_LOSS 388 0.0015981134456064432\n",
      "EPOCH_LOSS 389 0.0015978036866158831\n",
      "EPOCH_LOSS 390 0.0015974933577183447\n",
      "EPOCH_LOSS 391 0.0015971833115378659\n",
      "EPOCH_LOSS 392 0.0015968720999491955\n",
      "EPOCH_LOSS 393 0.0015965627945939034\n",
      "EPOCH_LOSS 394 0.0015962529208349166\n",
      "EPOCH_LOSS 395 0.0015959442215628082\n",
      "EPOCH_LOSS 396 0.0015956355440531356\n",
      "EPOCH_LOSS 397 0.0015953264913419942\n",
      "EPOCH_LOSS 398 0.0015950173007777276\n",
      "EPOCH_LOSS 399 0.0015947102116064505\n",
      "EPOCH_LOSS 400 0.001594400750351713\n",
      "EPOCH_LOSS 401 0.0015940930856310118\n",
      "EPOCH_LOSS 402 0.0015937853070610503\n",
      "EPOCH_LOSS 403 0.0015934787244454368\n",
      "EPOCH_LOSS 404 0.0015931717220708718\n",
      "EPOCH_LOSS 405 0.001592865576587951\n",
      "EPOCH_LOSS 406 0.0015925602870905196\n",
      "EPOCH_LOSS 407 0.001592255105124379\n",
      "EPOCH_LOSS 408 0.0015919494668047876\n",
      "EPOCH_LOSS 409 0.0015916447294487392\n",
      "EPOCH_LOSS 410 0.0015913384530717005\n",
      "EPOCH_LOSS 411 0.0015910340095152654\n",
      "EPOCH_LOSS 412 0.001590728056632362\n",
      "EPOCH_LOSS 413 0.0015904231568865692\n",
      "EPOCH_LOSS 414 0.0015901181644236032\n",
      "EPOCH_LOSS 415 0.0015898137943166388\n",
      "EPOCH_LOSS 416 0.0015895095619331404\n",
      "EPOCH_LOSS 417 0.0015892052832422662\n",
      "EPOCH_LOSS 418 0.00158890377365261\n",
      "EPOCH_LOSS 419 0.0015886018005785388\n",
      "EPOCH_LOSS 420 0.0015882998677302657\n",
      "EPOCH_LOSS 421 0.0015879977039494695\n",
      "EPOCH_LOSS 422 0.001587695648151523\n",
      "EPOCH_LOSS 423 0.0015873935504362921\n",
      "EPOCH_LOSS 424 0.0015870914213738894\n",
      "EPOCH_LOSS 425 0.0015867892754121082\n",
      "EPOCH_LOSS 426 0.0015864882519255378\n",
      "EPOCH_LOSS 427 0.0015861880335740104\n",
      "EPOCH_LOSS 428 0.0015858876490683195\n",
      "EPOCH_LOSS 429 0.0015855872970187632\n",
      "EPOCH_LOSS 430 0.0015852871895902748\n",
      "EPOCH_LOSS 431 0.0015849873052357372\n",
      "EPOCH_LOSS 432 0.001584688780590293\n",
      "EPOCH_LOSS 433 0.0015843888593114607\n",
      "EPOCH_LOSS 434 0.0015840908361559052\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "initial_lr = 1e-5\n",
    "layers = 3\n",
    "# cell = LSTMStacked(cells=layers, input_size=2) # passenger and the month index\n",
    "optimizer = optim.Adam(cell.parameters(), lr=initial_lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50, threshold=0.00005)\n",
    "prev_a_c_0 = [(torch.zeros((64)).to(torch.float32), torch.zeros((64)).to(torch.float32)) for _ in range(layers)]\n",
    "month_indexes = dataset.data.Month.apply(lambda x: int(x[-2:]))\n",
    "\n",
    "EPOCH=435\n",
    "sliding_window_length = 12\n",
    "\n",
    "for ep in range(EPOCH):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, split-sliding_window_length):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        passengers = torch.tensor(np.array(dataset.passengers[i:i+sliding_window_length])).to(torch.float32)\n",
    "        months = torch.tensor(np.array(month_indexes[i:i+sliding_window_length])).to(torch.float32)\n",
    "        prev_a_c = prev_a_c_0\n",
    "        if len(passengers)<12:\n",
    "            continue\n",
    "\n",
    "        for p in range(sliding_window_length):\n",
    "            input = torch.FloatTensor((months[p], passengers[p]))\n",
    "            logit, a_c = cell(input, prev_a_c)\n",
    "            prev_a_c = a_c\n",
    "\n",
    "        target = torch.FloatTensor(dataset.passengers[i+sliding_window_length])\n",
    "        loss = criterion(logit, target)\n",
    "        prev_a_c = a_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prev_a_c = [(a.detach(), c.detach()) for a, c in prev_a_c]\n",
    "\n",
    "        # print(\"Batch avg loss\", batch_loss.item() / 4)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(\"EPOCH_LOSS\", ep, epoch_loss / (split-sliding_window_length))\n",
    "    scheduler.step(epoch_loss / (split-sliding_window_length))\n",
    "    if initial_lr != optimizer.param_groups[0]['lr']:\n",
    "        print(f\"LR Changed {optimizer.param_groups[0]['lr']}\")\n",
    "        initial_lr = optimizer.param_groups[0]['lr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1241e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cell.state_dict, 'lstm_stacked_month_ep500.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "595d6623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[159.49811716]]), array([[152.67451139]]), array([[152.02673027]]), array([[157.58637844]]), array([[168.09018527]]), array([[180.1697121]]), array([[187.41641626]]), array([[183.89332369]]), array([[167.82805477]]), array([[146.0090407]]), array([[133.95898812]]), array([[144.88993026]]), array([[166.29185118]]), array([[160.32284795]]), array([[160.95092683]]), array([[168.61654578]]), array([[182.21204522]]), array([[198.25642321]]), array([[208.62359181]]), array([[206.24371549]]), array([[189.19368574]]), array([[164.71095572]]), array([[150.5617635]]), array([[161.49687766]]), array([[183.76279876]]), array([[178.10406819]]), array([[179.93479809]]), array([[189.18337342]]), array([[205.4569149]]), array([[223.12379268]]), array([[235.38727784]]), array([[232.87895295]]), array([[213.25632384]]), array([[184.56414157]]), array([[166.68104087]]), array([[176.7119669]]), array([[199.97266784]]), array([[195.11534956]]), array([[198.25781259]]), array([[210.56331119]]), array([[231.27955553]]), array([[254.63129973]]), array([[269.71658576]]), array([[268.44696641]]), array([[245.79666829]]), array([[212.87237522]]), array([[190.34379488]]), array([[198.62602258]]), array([[221.41789895]]), array([[216.38040069]]), array([[219.6990608]]), array([[231.17925742]]), array([[250.24208045]]), array([[271.62564605]]), array([[285.20516372]]), array([[280.69909722]]), array([[254.64536339]]), array([[219.01461151]]), array([[195.77066809]]), array([[205.51878881]]), array([[230.47125807]]), array([[227.32400939]]), array([[234.26572055]]), array([[250.45780551]]), array([[276.21685094]]), array([[304.37974727]]), array([[323.2206316]]), array([[320.93279427]]), array([[295.76389754]]), array([[257.18906355]]), array([[230.10442975]]), array([[238.82677579]]), array([[264.64936239]]), array([[263.7670725]]), array([[274.08962643]]), array([[296.04450685]]), array([[328.56343162]]), array([[364.25313199]]), array([[388.01095462]]), array([[386.72895432]]), array([[359.13062656]]), array([[312.78282052]]), array([[276.64550686]]), array([[280.57343513]]), array([[303.71316701]]), array([[304.48112601]]), array([[316.95043302]]), array([[341.53836644]]), array([[377.60357106]]), array([[416.08732319]]), array([[440.68905783]]), array([[440.31982124]]), array([[410.66492689]]), array([[360.36486292]]), array([[318.86485016]]), array([[319.37509382]]), array([[341.22465891]]), array([[344.96921933]]), array([[361.02633333]]), array([[388.17511809]]), array([[426.42780018]]), array([[465.49253106]]), array([[488.06456053]]), array([[483.98048091]]), array([[447.64190722]]), array([[389.25065589]]), array([[339.37393731]]), array([[334.54916149]]), array([[353.80332589]]), array([[357.7768414]]), array([[374.7736423]]), array([[404.22321653]]), array([[446.00755906]]), array([[488.7305696]]), array([[515.45114613]]), array([[515.76039219]]), array([[483.76895487]]), array([[432.73807478]]), array([[384.4786458]]), array([[379.54923391]]), array([[398.14512348]]), array([[407.65984309]]), array([[430.41415977]]), array([[463.67228329]]), array([[508.44614851]]), array([[552.96955407]]), array([[584.33138418]]), array([[589.60499287]]), array([[563.72700071]]), array([[513.24594629]]), array([[457.42131996]]), array([[440.16973686]])]\n",
      "[array([[115.00000034]]), array([[126.00000067]]), array([[141.00000165]]), array([[134.99999972]]), array([[124.99999906]]), array([[148.99999909]]), array([[169.99999815]]), array([[169.99999815]]), array([[157.99999814]]), array([[133.00000036]]), array([[113.99999969]]), array([[140.00000004]]), array([[145.00000037]]), array([[150.0000007]]), array([[178.00000331]]), array([[162.99999847]]), array([[172.00000137]]), array([[178.00000331]]), array([[198.99999851]]), array([[198.99999851]]), array([[183.99999753]]), array([[162.00000072]]), array([[145.99999812]]), array([[165.99999943]]), array([[170.99999976]]), array([[179.99999881]]), array([[192.99999657]]), array([[181.00000042]]), array([[183.00000364]]), array([[217.99999821]]), array([[230.00000209]]), array([[242.00000596]]), array([[208.99999917]]), array([[191.00000107]]), array([[172.00000137]]), array([[193.99999818]]), array([[196.0000014]]), array([[196.0000014]]), array([[235.9999963]]), array([[234.9999947]]), array([[229.00000048]]), array([[243.00000757]]), array([[263.99999505]]), array([[271.99999249]]), array([[236.99999791]]), array([[211.00000238]]), array([[179.99999881]]), array([[201.00000173]]), array([[203.99999884]]), array([[187.99999624]]), array([[234.9999947]]), array([[226.99999726]]), array([[233.99999309]]), array([[263.99999505]]), array([[301.99999446]]), array([[292.99999541]]), array([[259.00000244]]), array([[229.00000048]]), array([[202.99999723]]), array([[229.00000048]]), array([[242.00000596]]), array([[232.9999992]]), array([[266.99999988]]), array([[269.0000031]]), array([[270.00000471]]), array([[314.99999994]]), array([[363.99998617]]), array([[347.00000513]]), array([[311.99999511]]), array([[273.99999571]]), array([[236.99999791]]), array([[278.00000215]]), array([[283.99999636]]), array([[277.00000054]]), array([[317.00000316]]), array([[312.99999672]]), array([[318.00000477]]), array([[374.00000226]]), array([[412.99998784]]), array([[405.00000584]]), array([[355.00000256]]), array([[306.00000089]]), array([[271.00000632]]), array([[306.00000089]]), array([[314.99999994]]), array([[300.99999285]]), array([[356.00000417]]), array([[348.00000674]]), array([[355.00000256]]), array([[421.99998689]]), array([[465.00000978]]), array([[467.00001299]]), array([[403.99998879]]), array([[347.00000513]]), array([[304.99999928]]), array([[336.00000286]]), array([[339.99999386]]), array([[318.00000477]]), array([[361.99999839]]), array([[348.00000674]]), array([[363.]]), array([[434.99999237]]), array([[490.99998987]]), array([[505.0000124]]), array([[403.99998879]]), array([[358.99999356]]), array([[310.00000733]]), array([[337.00000447]]), array([[359.99999517]]), array([[341.99999708]]), array([[405.99999201]]), array([[396.00000679]]), array([[420.00001454]]), array([[472.0000056]]), array([[548.00000441]]), array([[559.00000668]]), array([[463.00000656]]), array([[407.00000906]]), array([[361.99999839]]), array([[405.00000584]]), array([[416.99999428]]), array([[391.00001419]]), array([[418.9999975]]), array([[461.00000334]]), array([[472.0000056]]), array([[534.99999893]]), array([[622.]]), array([[606.00000513]]), array([[508.00000179]]), array([[461.00000334]]), array([[389.99999714]]), array([[432.00000298]])]\n",
      "tensor(0.0645, dtype=torch.float64)\n",
      "tensor(17.3741, dtype=torch.float64)\n",
      "tensor(22.0418, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    output = []\n",
    "    labels = []\n",
    "    # input = torch.FloatTensor(test_loader.dataset)[i:i+sliding_window_length,1]\n",
    "    for i in range(len(dataset.passengers) - sliding_window_length):\n",
    "        passengers = torch.FloatTensor(dataset.passengers)[i:i+sliding_window_length]\n",
    "        months = torch.FloatTensor(month_indexes)[i:i+sliding_window_length]\n",
    "        \n",
    "        prev_a_c = [(torch.zeros((64)).to(torch.float32), torch.zeros((64)).to(torch.float32)) for _ in range(layers)]\n",
    "        # logits, a_c_prev = cell(input, a_c_prev)\n",
    "        for p in range(sliding_window_length):\n",
    "            input = torch.FloatTensor((months[p], passengers[p]))\n",
    "            logit, a_c = cell(input, prev_a_c)\n",
    "            prev_a_c = a_c\n",
    "        output.append( dataset.scaler.inverse_transform(logit.reshape(-1, 1)))\n",
    "        labels.append(dataset.scaler.inverse_transform(torch.FloatTensor(dataset.passengers)[sliding_window_length+i].reshape(-1, 1)))\n",
    "    \n",
    "    print(output)\n",
    "    print(labels)\n",
    "\n",
    "    toutput = torch.tensor(output)\n",
    "    tlabels = torch.tensor(labels)\n",
    "    # mape (mean absolute percentage error)\n",
    "    # mean((actual-forecast) / actual)\n",
    "    mape = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)) / torch.squeeze(tlabels))) # 0.1580, 0.1418(12-batch), 0.1343(sliding window), 0.1167(stacked lstm), 0.0730(stackedlstm with month)\n",
    "    print(mape)\n",
    "\n",
    "    # mae (mean absolute error)\n",
    "    mae = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)))) # 82.5248, 71.5019, 69.0149, 35.3759, 20.0789\n",
    "    print(mae)\n",
    "\n",
    "    # rmse (root mean square error)\n",
    "    rmse = torch.sqrt(torch.mean(torch.square((torch.squeeze(toutput)-torch.squeeze(tlabels))))) # 112.3071, 94.6528, 91.1919, 47.2027, 25.8126\n",
    "    print(rmse)\n",
    "\n",
    "finally:\n",
    "    torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "125540f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132,)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(output).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e1477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(output).squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c52319dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, output_size=1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size, bias=False) # Waa\n",
    "        self.linear2 = nn.Linear(input_size, hidden_size, bias=True) # Wax + ba\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.final = nn.Linear(hidden_size, output_size, bias=True) # Wya + by\n",
    "\n",
    "    def forward(self, x, previous_a):\n",
    "        aa = self.linear1(previous_a)\n",
    "        ax = self.linear2(x)\n",
    "        x = torch.add(aa, ax)\n",
    "        a = self.tanh(x)\n",
    "        x = self.final(a)\n",
    "        return x, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bcbdadd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH_LOSS 0 0.22154463422046894\n",
      "EPOCH_LOSS 1 0.14146826034946156\n",
      "EPOCH_LOSS 2 0.096368801813502\n",
      "EPOCH_LOSS 3 0.07025096789018587\n",
      "EPOCH_LOSS 4 0.05593075281630706\n",
      "EPOCH_LOSS 5 0.048546799921620504\n",
      "EPOCH_LOSS 6 0.044928918363019384\n",
      "EPOCH_LOSS 7 0.04317640910677317\n",
      "EPOCH_LOSS 8 0.04225370664756381\n",
      "EPOCH_LOSS 9 0.04165115891442123\n",
      "EPOCH_LOSS 10 0.04114518237969232\n",
      "EPOCH_LOSS 11 0.04065215106662731\n",
      "EPOCH_LOSS 12 0.04014936334832964\n",
      "EPOCH_LOSS 13 0.03963695711121658\n",
      "EPOCH_LOSS 14 0.039121433716587474\n",
      "EPOCH_LOSS 15 0.038609245471135124\n",
      "EPOCH_LOSS 16 0.03810531270013381\n",
      "EPOCH_LOSS 17 0.03761265516181682\n",
      "EPOCH_LOSS 18 0.037132817047926474\n",
      "EPOCH_LOSS 19 0.036666386987010025\n",
      "EPOCH_LOSS 20 0.03621327506760623\n",
      "EPOCH_LOSS 21 0.035773050661572416\n",
      "EPOCH_LOSS 22 0.03534498668482734\n",
      "EPOCH_LOSS 23 0.03492834440597411\n",
      "EPOCH_LOSS 24 0.034522263417341326\n",
      "EPOCH_LOSS 25 0.034125958091062475\n",
      "EPOCH_LOSS 26 0.03373864981391603\n",
      "EPOCH_LOSS 27 0.033359539363623805\n",
      "EPOCH_LOSS 28 0.03298801393036703\n",
      "EPOCH_LOSS 29 0.03262338779523606\n",
      "EPOCH_LOSS 30 0.032265076123963385\n",
      "EPOCH_LOSS 31 0.031912626944099486\n",
      "EPOCH_LOSS 32 0.03156549248287153\n",
      "EPOCH_LOSS 33 0.031223237040730732\n",
      "EPOCH_LOSS 34 0.030885574923454093\n",
      "EPOCH_LOSS 35 0.030552039398272373\n",
      "EPOCH_LOSS 36 0.030222438456784313\n",
      "EPOCH_LOSS 37 0.029896348342323607\n",
      "EPOCH_LOSS 38 0.029573702653491133\n",
      "EPOCH_LOSS 39 0.029254121716102394\n",
      "EPOCH_LOSS 40 0.028937517983529254\n",
      "EPOCH_LOSS 41 0.028623670729141532\n",
      "EPOCH_LOSS 42 0.028312400318994065\n",
      "EPOCH_LOSS 43 0.028003563123575\n",
      "EPOCH_LOSS 44 0.02769705997828666\n",
      "EPOCH_LOSS 45 0.02739282127354311\n",
      "EPOCH_LOSS 46 0.02709061714607291\n",
      "EPOCH_LOSS 47 0.02679043640606104\n",
      "EPOCH_LOSS 48 0.026492161944909245\n",
      "EPOCH_LOSS 49 0.026195703571553697\n",
      "EPOCH_LOSS 50 0.025901025808895\n",
      "EPOCH_LOSS 51 0.02560796784035753\n",
      "EPOCH_LOSS 52 0.02531656030704552\n",
      "EPOCH_LOSS 53 0.02502672618018737\n",
      "EPOCH_LOSS 54 0.02473833417088321\n",
      "EPOCH_LOSS 55 0.024451383249426657\n",
      "EPOCH_LOSS 56 0.024165830831154644\n",
      "EPOCH_LOSS 57 0.023881596444112786\n",
      "EPOCH_LOSS 58 0.023598643831290123\n",
      "EPOCH_LOSS 59 0.023316932981171518\n",
      "EPOCH_LOSS 60 0.02303641661358745\n",
      "EPOCH_LOSS 61 0.02275709643853176\n",
      "EPOCH_LOSS 62 0.022478838181075097\n",
      "EPOCH_LOSS 63 0.02220168605350782\n",
      "EPOCH_LOSS 64 0.021925585600382696\n",
      "EPOCH_LOSS 65 0.021650476497493052\n",
      "EPOCH_LOSS 66 0.021376380011312524\n",
      "EPOCH_LOSS 67 0.021103235632940944\n",
      "EPOCH_LOSS 68 0.020830989568986866\n",
      "EPOCH_LOSS 69 0.020559684739105757\n",
      "EPOCH_LOSS 70 0.02028921810205749\n",
      "EPOCH_LOSS 71 0.02001958552229483\n",
      "EPOCH_LOSS 72 0.019750845308247018\n",
      "EPOCH_LOSS 73 0.01948287104744395\n",
      "EPOCH_LOSS 74 0.019215739338090742\n",
      "EPOCH_LOSS 75 0.018949366608954177\n",
      "EPOCH_LOSS 76 0.018683760438599817\n",
      "EPOCH_LOSS 77 0.018418924185538404\n",
      "EPOCH_LOSS 78 0.018154851017210795\n",
      "EPOCH_LOSS 79 0.017891505128965528\n",
      "EPOCH_LOSS 80 0.01762893181164425\n",
      "EPOCH_LOSS 81 0.017367097717293976\n",
      "EPOCH_LOSS 82 0.017105993035679255\n",
      "EPOCH_LOSS 83 0.016845653456806386\n",
      "EPOCH_LOSS 84 0.016586052310658565\n",
      "EPOCH_LOSS 85 0.016327252201359117\n",
      "EPOCH_LOSS 86 0.016069238808474755\n",
      "EPOCH_LOSS 87 0.01581195460297532\n",
      "EPOCH_LOSS 88 0.015555538600364165\n",
      "EPOCH_LOSS 89 0.015299947902092707\n",
      "EPOCH_LOSS 90 0.015045215846357755\n",
      "EPOCH_LOSS 91 0.014791401894135017\n",
      "EPOCH_LOSS 92 0.014538477114351385\n",
      "EPOCH_LOSS 93 0.014286514178820653\n",
      "EPOCH_LOSS 94 0.014035545467792957\n",
      "EPOCH_LOSS 95 0.013785616093550316\n",
      "EPOCH_LOSS 96 0.01353677244930475\n",
      "EPOCH_LOSS 97 0.013289079038509627\n",
      "EPOCH_LOSS 98 0.013042564276182401\n",
      "EPOCH_LOSS 99 0.012797300410097011\n",
      "EPOCH_LOSS 100 0.012553323758086864\n",
      "EPOCH_LOSS 101 0.012310764845293622\n",
      "EPOCH_LOSS 102 0.01206962123408967\n",
      "EPOCH_LOSS 103 0.011830008242248788\n",
      "EPOCH_LOSS 104 0.011592011139538807\n",
      "EPOCH_LOSS 105 0.011355689959347687\n",
      "EPOCH_LOSS 106 0.011121176115119478\n",
      "EPOCH_LOSS 107 0.010888520405381972\n",
      "EPOCH_LOSS 108 0.01065782469335172\n",
      "EPOCH_LOSS 109 0.010429233907051966\n",
      "EPOCH_LOSS 110 0.010202797810092857\n",
      "EPOCH_LOSS 111 0.009978679427301176\n",
      "EPOCH_LOSS 112 0.009756994365640518\n",
      "EPOCH_LOSS 113 0.009537812503538134\n",
      "EPOCH_LOSS 114 0.009321332180498907\n",
      "EPOCH_LOSS 115 0.009107627104189555\n",
      "EPOCH_LOSS 116 0.008896850585404223\n",
      "EPOCH_LOSS 117 0.00868913138848134\n",
      "EPOCH_LOSS 118 0.008484656435527912\n",
      "EPOCH_LOSS 119 0.008283500390074536\n",
      "EPOCH_LOSS 120 0.008085849831601998\n",
      "EPOCH_LOSS 121 0.007891808075249107\n",
      "EPOCH_LOSS 122 0.007701572366439201\n",
      "EPOCH_LOSS 123 0.007515275634638932\n",
      "EPOCH_LOSS 124 0.007333056048360069\n",
      "EPOCH_LOSS 125 0.007155038173615668\n",
      "EPOCH_LOSS 126 0.006981386216229399\n",
      "EPOCH_LOSS 127 0.0068122266716841295\n",
      "EPOCH_LOSS 128 0.006647685685167343\n",
      "EPOCH_LOSS 129 0.00648788912260987\n",
      "EPOCH_LOSS 130 0.006332930865372393\n",
      "EPOCH_LOSS 131 0.006182924094718036\n",
      "EPOCH_LOSS 132 0.0060379558617846205\n",
      "EPOCH_LOSS 133 0.005898101681668304\n",
      "EPOCH_LOSS 134 0.005763397969555544\n",
      "EPOCH_LOSS 135 0.005633925503882166\n",
      "EPOCH_LOSS 136 0.005509690929148821\n",
      "EPOCH_LOSS 137 0.005390687222698364\n",
      "EPOCH_LOSS 138 0.005276907001557977\n",
      "EPOCH_LOSS 139 0.005168321864020551\n",
      "EPOCH_LOSS 140 0.005064863130968676\n",
      "EPOCH_LOSS 141 0.004966474240753098\n",
      "EPOCH_LOSS 142 0.004873065059858883\n",
      "EPOCH_LOSS 143 0.00478450739945809\n",
      "EPOCH_LOSS 144 0.004700678366311918\n",
      "EPOCH_LOSS 145 0.004621452973374634\n",
      "EPOCH_LOSS 146 0.004546636575788606\n",
      "EPOCH_LOSS 147 0.004476102177846268\n",
      "EPOCH_LOSS 148 0.004409643423518726\n",
      "EPOCH_LOSS 149 0.0043470845426917095\n",
      "EPOCH_LOSS 150 0.004288222450888243\n",
      "EPOCH_LOSS 151 0.0042328681341757355\n",
      "EPOCH_LOSS 152 0.004180819824592777\n",
      "EPOCH_LOSS 153 0.00413187780439667\n",
      "EPOCH_LOSS 154 0.00408584560594954\n",
      "EPOCH_LOSS 155 0.004042521587956547\n",
      "EPOCH_LOSS 156 0.004001742752277668\n",
      "EPOCH_LOSS 157 0.003963294314800813\n",
      "EPOCH_LOSS 158 0.003927024632380244\n",
      "EPOCH_LOSS 159 0.003892759138549457\n",
      "EPOCH_LOSS 160 0.0038603352235369076\n",
      "EPOCH_LOSS 161 0.0038296176152060385\n",
      "EPOCH_LOSS 162 0.0038004599285221268\n",
      "EPOCH_LOSS 163 0.0037727295767632715\n",
      "EPOCH_LOSS 164 0.003746313235054351\n",
      "EPOCH_LOSS 165 0.0037210941939185625\n",
      "EPOCH_LOSS 166 0.0036969719298084714\n",
      "EPOCH_LOSS 167 0.0036738527273953384\n",
      "EPOCH_LOSS 168 0.0036516503436880747\n",
      "EPOCH_LOSS 169 0.0036302930566234747\n",
      "EPOCH_LOSS 170 0.0036096990359584438\n",
      "EPOCH_LOSS 171 0.0035898096442209112\n",
      "EPOCH_LOSS 172 0.0035705704324831096\n",
      "EPOCH_LOSS 173 0.003551923175253056\n",
      "EPOCH_LOSS 174 0.003533822896334125\n",
      "EPOCH_LOSS 175 0.0035162238098100243\n",
      "EPOCH_LOSS 176 0.00349908602221478\n",
      "EPOCH_LOSS 177 0.0034823805545389623\n",
      "EPOCH_LOSS 178 0.0034660782513715547\n",
      "EPOCH_LOSS 179 0.0034501447734878368\n",
      "EPOCH_LOSS 180 0.0034345554509620816\n",
      "EPOCH_LOSS 181 0.003419286271833566\n",
      "EPOCH_LOSS 182 0.0034043259680311263\n",
      "EPOCH_LOSS 183 0.0033896478180513025\n",
      "EPOCH_LOSS 184 0.0033752289274059043\n",
      "EPOCH_LOSS 185 0.0033610695578358907\n",
      "EPOCH_LOSS 186 0.0033471465880116342\n",
      "EPOCH_LOSS 187 0.003333453212793315\n",
      "EPOCH_LOSS 188 0.0033199670892385155\n",
      "EPOCH_LOSS 189 0.0033066918951427924\n",
      "EPOCH_LOSS 190 0.0032936081561774893\n",
      "EPOCH_LOSS 191 0.0032807120570372316\n",
      "EPOCH_LOSS 192 0.003267995143418551\n",
      "EPOCH_LOSS 193 0.0032554500121662376\n",
      "EPOCH_LOSS 194 0.00324306606387284\n",
      "EPOCH_LOSS 195 0.0032308444090152345\n",
      "EPOCH_LOSS 196 0.003218773317818875\n",
      "EPOCH_LOSS 197 0.0032068488405392072\n",
      "EPOCH_LOSS 198 0.0031950702890477787\n",
      "EPOCH_LOSS 199 0.0031834267728917884\n",
      "EPOCH_LOSS 200 0.0031719196783806807\n",
      "EPOCH_LOSS 201 0.003160542664724849\n",
      "EPOCH_LOSS 202 0.0031492866635785387\n",
      "EPOCH_LOSS 203 0.003138154662457914\n",
      "EPOCH_LOSS 204 0.0031271410018592328\n",
      "EPOCH_LOSS 205 0.003116245605818311\n",
      "EPOCH_LOSS 206 0.003105460047128744\n",
      "EPOCH_LOSS 207 0.0030947806028123514\n",
      "EPOCH_LOSS 208 0.003084209708250838\n",
      "EPOCH_LOSS 209 0.003073742255899047\n",
      "EPOCH_LOSS 210 0.003063375445692313\n",
      "EPOCH_LOSS 211 0.0030531077634054914\n",
      "EPOCH_LOSS 212 0.0030429322475063482\n",
      "EPOCH_LOSS 213 0.003032855948532437\n",
      "EPOCH_LOSS 214 0.003022866327209917\n",
      "EPOCH_LOSS 215 0.00301296581035949\n",
      "EPOCH_LOSS 216 0.003003155940899344\n",
      "EPOCH_LOSS 217 0.002993427318365544\n",
      "EPOCH_LOSS 218 0.0029837824624673087\n",
      "EPOCH_LOSS 219 0.0029742192009046013\n",
      "EPOCH_LOSS 220 0.0029647343471592494\n",
      "EPOCH_LOSS 221 0.002955328579052342\n",
      "EPOCH_LOSS 222 0.0029459949068241335\n",
      "EPOCH_LOSS 223 0.0029367372996433523\n",
      "EPOCH_LOSS 224 0.0029275515297816916\n",
      "EPOCH_LOSS 225 0.0029184356213333693\n",
      "EPOCH_LOSS 226 0.0029093886444181514\n",
      "EPOCH_LOSS 227 0.002900408319577896\n",
      "EPOCH_LOSS 228 0.0028914918531165864\n",
      "EPOCH_LOSS 229 0.0028826426213414257\n",
      "EPOCH_LOSS 230 0.002873854707973286\n",
      "EPOCH_LOSS 231 0.002865127386981732\n",
      "EPOCH_LOSS 232 0.0028564613189698694\n",
      "EPOCH_LOSS 233 0.002847852582191473\n",
      "EPOCH_LOSS 234 0.0028392986303818362\n",
      "EPOCH_LOSS 235 0.002830803820461095\n",
      "EPOCH_LOSS 236 0.0028223630132855394\n",
      "EPOCH_LOSS 237 0.0028139762898616773\n",
      "EPOCH_LOSS 238 0.0028056397535710645\n",
      "EPOCH_LOSS 239 0.0027973567364282504\n",
      "EPOCH_LOSS 240 0.0027891208154054432\n",
      "EPOCH_LOSS 241 0.002780934690518155\n",
      "EPOCH_LOSS 242 0.0027727924137550862\n",
      "EPOCH_LOSS 243 0.002764698525932265\n",
      "EPOCH_LOSS 244 0.0027566500746525565\n",
      "EPOCH_LOSS 245 0.0027486461183950936\n",
      "EPOCH_LOSS 246 0.0027406843204717084\n",
      "EPOCH_LOSS 247 0.0027327664896545636\n",
      "EPOCH_LOSS 248 0.002724885615107152\n",
      "EPOCH_LOSS 249 0.002717048121275588\n",
      "EPOCH_LOSS 250 0.002709250418331813\n",
      "EPOCH_LOSS 251 0.002701488963741492\n",
      "EPOCH_LOSS 252 0.0026937667466920842\n",
      "EPOCH_LOSS 253 0.0026860800869939277\n",
      "EPOCH_LOSS 254 0.002678428958914444\n",
      "EPOCH_LOSS 255 0.0026708101152140584\n",
      "EPOCH_LOSS 256 0.002663228686038165\n",
      "EPOCH_LOSS 257 0.002655678041699592\n",
      "EPOCH_LOSS 258 0.002648160115132925\n",
      "EPOCH_LOSS 259 0.002640675592949586\n",
      "EPOCH_LOSS 260 0.002633221327910365\n",
      "EPOCH_LOSS 261 0.0026257964305235145\n",
      "EPOCH_LOSS 262 0.002618401589254168\n",
      "EPOCH_LOSS 263 0.0026110365546794757\n",
      "EPOCH_LOSS 264 0.002603697803709744\n",
      "EPOCH_LOSS 265 0.002596388885959956\n",
      "EPOCH_LOSS 266 0.0025891058786487713\n",
      "EPOCH_LOSS 267 0.0025818488426375165\n",
      "EPOCH_LOSS 268 0.0025746193900410158\n",
      "EPOCH_LOSS 269 0.0025674130930571906\n",
      "EPOCH_LOSS 270 0.0025602340104664747\n",
      "EPOCH_LOSS 271 0.0025530808710790063\n",
      "EPOCH_LOSS 272 0.002545946551044816\n",
      "EPOCH_LOSS 273 0.0025388406533402323\n",
      "EPOCH_LOSS 274 0.0025317537333164318\n",
      "EPOCH_LOSS 275 0.0025246934733575864\n",
      "EPOCH_LOSS 276 0.0025176506066745294\n",
      "EPOCH_LOSS 277 0.0025106313248901405\n",
      "EPOCH_LOSS 278 0.0025036337692235303\n",
      "EPOCH_LOSS 279 0.0024966597674622006\n",
      "EPOCH_LOSS 280 0.002489702732794219\n",
      "EPOCH_LOSS 281 0.0024827688772225322\n",
      "EPOCH_LOSS 282 0.002475854464058522\n",
      "EPOCH_LOSS 283 0.0024689592606265275\n",
      "EPOCH_LOSS 284 0.002462084335877486\n",
      "EPOCH_LOSS 285 0.0024552308456849714\n",
      "EPOCH_LOSS 286 0.0024483939070210907\n",
      "EPOCH_LOSS 287 0.0024415763274945003\n",
      "EPOCH_LOSS 288 0.0024347801333632294\n",
      "EPOCH_LOSS 289 0.002428000991910119\n",
      "EPOCH_LOSS 290 0.0024212391052421877\n",
      "EPOCH_LOSS 291 0.0024144979274421303\n",
      "EPOCH_LOSS 292 0.0024077739725906866\n",
      "EPOCH_LOSS 293 0.0024010678871112154\n",
      "EPOCH_LOSS 294 0.002394381852368458\n",
      "EPOCH_LOSS 295 0.002387712843421418\n",
      "EPOCH_LOSS 296 0.002381061876021179\n",
      "EPOCH_LOSS 297 0.002374430556540411\n",
      "EPOCH_LOSS 298 0.0023678148845654515\n",
      "EPOCH_LOSS 299 0.002361219440712075\n",
      "EPOCH_LOSS 300 0.0023546429738091406\n",
      "EPOCH_LOSS 301 0.002348083475560327\n",
      "EPOCH_LOSS 302 0.002341541926454654\n",
      "EPOCH_LOSS 303 0.002335018982259391\n",
      "EPOCH_LOSS 304 0.0023285135037677917\n",
      "EPOCH_LOSS 305 0.0023220251452486106\n",
      "EPOCH_LOSS 306 0.0023155563133353267\n",
      "EPOCH_LOSS 307 0.002309108076659072\n",
      "EPOCH_LOSS 308 0.0023026776195125827\n",
      "EPOCH_LOSS 309 0.0022962656846732456\n",
      "EPOCH_LOSS 310 0.002289873600744112\n",
      "EPOCH_LOSS 311 0.002283497934110672\n",
      "EPOCH_LOSS 312 0.0022771438300948762\n",
      "EPOCH_LOSS 313 0.0022708067973310703\n",
      "EPOCH_LOSS 314 0.0022644902963207627\n",
      "EPOCH_LOSS 315 0.0022581942747218523\n",
      "EPOCH_LOSS 316 0.002251917258720038\n",
      "EPOCH_LOSS 317 0.0022456594133897015\n",
      "EPOCH_LOSS 318 0.00223942137367839\n",
      "EPOCH_LOSS 319 0.0022332061944702213\n",
      "EPOCH_LOSS 320 0.0022270095501438986\n",
      "EPOCH_LOSS 321 0.002220835637786362\n",
      "EPOCH_LOSS 322 0.0022146811080742093\n",
      "EPOCH_LOSS 323 0.0022085464698166184\n",
      "EPOCH_LOSS 324 0.002202432487706695\n",
      "EPOCH_LOSS 325 0.002196341896016857\n",
      "EPOCH_LOSS 326 0.0021902702645146804\n",
      "EPOCH_LOSS 327 0.00218421863982761\n",
      "EPOCH_LOSS 328 0.002178189995255005\n",
      "EPOCH_LOSS 329 0.0021721851925264233\n",
      "EPOCH_LOSS 330 0.002166199632356568\n",
      "EPOCH_LOSS 331 0.0021602374403978434\n",
      "EPOCH_LOSS 332 0.0021542956620140253\n",
      "EPOCH_LOSS 333 0.002148376692703659\n",
      "EPOCH_LOSS 334 0.0021424814662099026\n",
      "EPOCH_LOSS 335 0.0021366045206296126\n",
      "EPOCH_LOSS 336 0.002130753727038235\n",
      "EPOCH_LOSS 337 0.0021249252906086324\n",
      "EPOCH_LOSS 338 0.0021191178174432175\n",
      "EPOCH_LOSS 339 0.0021133342989415748\n",
      "EPOCH_LOSS 340 0.0021075728312208407\n",
      "EPOCH_LOSS 341 0.002101834653312442\n",
      "EPOCH_LOSS 342 0.0020961183127147145\n",
      "EPOCH_LOSS 343 0.0020904233675824286\n",
      "EPOCH_LOSS 344 0.002084754801076226\n",
      "EPOCH_LOSS 345 0.0020791057630966934\n",
      "EPOCH_LOSS 346 0.0020734817206217067\n",
      "EPOCH_LOSS 347 0.0020678799831275415\n",
      "EPOCH_LOSS 348 0.002062298308916483\n",
      "EPOCH_LOSS 349 0.0020567439892008607\n",
      "EPOCH_LOSS 350 0.002051209312674741\n",
      "EPOCH_LOSS 351 0.002045699381125674\n",
      "EPOCH_LOSS 352 0.0020402102526776464\n",
      "EPOCH_LOSS 353 0.0020347424047244623\n",
      "EPOCH_LOSS 354 0.002029299621814166\n",
      "EPOCH_LOSS 355 0.002023879610174225\n",
      "EPOCH_LOSS 356 0.002018481140088283\n",
      "EPOCH_LOSS 357 0.0020131029957212044\n",
      "EPOCH_LOSS 358 0.00200775021344777\n",
      "EPOCH_LOSS 359 0.0020024171243933644\n",
      "EPOCH_LOSS 360 0.0019971062983462295\n",
      "EPOCH_LOSS 361 0.001991819075922447\n",
      "EPOCH_LOSS 362 0.0019865513680615908\n",
      "EPOCH_LOSS 363 0.001981303985679304\n",
      "EPOCH_LOSS 364 0.001976082441182709\n",
      "EPOCH_LOSS 365 0.001970878893460694\n",
      "EPOCH_LOSS 366 0.001965697617488005\n",
      "EPOCH_LOSS 367 0.001960538648656736\n",
      "EPOCH_LOSS 368 0.001955400152588767\n",
      "EPOCH_LOSS 369 0.001950279602906384\n",
      "EPOCH_LOSS 370 0.0019451818383923532\n",
      "EPOCH_LOSS 371 0.0019401034870751312\n",
      "EPOCH_LOSS 372 0.0019350456555673353\n",
      "EPOCH_LOSS 373 0.001930008090518182\n",
      "EPOCH_LOSS 374 0.0019249903804343356\n",
      "EPOCH_LOSS 375 0.0019199921138899326\n",
      "EPOCH_LOSS 376 0.0019150129799682333\n",
      "EPOCH_LOSS 377 0.0019100545449017069\n",
      "EPOCH_LOSS 378 0.0019051146086772057\n",
      "EPOCH_LOSS 379 0.0019001948972787047\n",
      "EPOCH_LOSS 380 0.001895291752267774\n",
      "EPOCH_LOSS 381 0.0018904091583656466\n",
      "EPOCH_LOSS 382 0.001885544870912556\n",
      "EPOCH_LOSS 383 0.0018806968356765035\n",
      "EPOCH_LOSS 384 0.0018758680132029168\n",
      "EPOCH_LOSS 385 0.0018710556907014537\n",
      "EPOCH_LOSS 386 0.0018662627669066391\n",
      "EPOCH_LOSS 387 0.0018614859560255478\n",
      "EPOCH_LOSS 388 0.0018567286677261994\n",
      "EPOCH_LOSS 389 0.0018519854481099812\n",
      "EPOCH_LOSS 390 0.0018472611672239899\n",
      "EPOCH_LOSS 391 0.0018425506014594969\n",
      "EPOCH_LOSS 392 0.0018378608594440348\n",
      "EPOCH_LOSS 393 0.0018331848595712823\n",
      "EPOCH_LOSS 394 0.001828523990246482\n",
      "EPOCH_LOSS 395 0.0018238804292024693\n",
      "EPOCH_LOSS 396 0.0018192506705606562\n",
      "EPOCH_LOSS 397 0.0018146392574035185\n",
      "EPOCH_LOSS 398 0.0018100407872212701\n",
      "EPOCH_LOSS 399 0.001805456896747767\n",
      "EPOCH_LOSS 400 0.001800890402927134\n",
      "EPOCH_LOSS 401 0.0017963364055092895\n",
      "EPOCH_LOSS 402 0.0017917994771411172\n",
      "EPOCH_LOSS 403 0.0017872729414027443\n",
      "EPOCH_LOSS 404 0.0017827626527460608\n",
      "EPOCH_LOSS 405 0.0017782669317429453\n",
      "EPOCH_LOSS 406 0.0017737836766740594\n",
      "EPOCH_LOSS 407 0.001769315217828072\n",
      "EPOCH_LOSS 408 0.0017648582750705586\n",
      "EPOCH_LOSS 409 0.0017604168026580807\n",
      "EPOCH_LOSS 410 0.0017559882755935835\n",
      "EPOCH_LOSS 411 0.001751569505893112\n",
      "EPOCH_LOSS 412 0.0017471652267036161\n",
      "EPOCH_LOSS 413 0.0017427739343910769\n",
      "EPOCH_LOSS 414 0.0017383944420753536\n",
      "EPOCH_LOSS 415 0.0017340263915928665\n",
      "EPOCH_LOSS 416 0.001729669288418503\n",
      "EPOCH_LOSS 417 0.0017253267955870844\n",
      "EPOCH_LOSS 418 0.001720994557241792\n",
      "EPOCH_LOSS 419 0.0017166736436332796\n",
      "EPOCH_LOSS 420 0.0017123639613176164\n",
      "EPOCH_LOSS 421 0.0017080660546581127\n",
      "EPOCH_LOSS 422 0.0017037784740894786\n",
      "EPOCH_LOSS 423 0.001699499408775247\n",
      "EPOCH_LOSS 424 0.0016952325036580283\n",
      "EPOCH_LOSS 425 0.001690976526751002\n",
      "EPOCH_LOSS 426 0.0016867317956479448\n",
      "EPOCH_LOSS 427 0.0016824960092371372\n",
      "EPOCH_LOSS 428 0.0016782701947135926\n",
      "EPOCH_LOSS 429 0.0016740547010997019\n",
      "EPOCH_LOSS 430 0.001669848050481746\n",
      "EPOCH_LOSS 431 0.0016656522174065603\n",
      "EPOCH_LOSS 432 0.0016614647474893056\n",
      "EPOCH_LOSS 433 0.0016572873436795877\n",
      "EPOCH_LOSS 434 0.0016531177227479235\n",
      "EPOCH_LOSS 435 0.0016489577533812915\n",
      "EPOCH_LOSS 436 0.0016448077791337854\n",
      "EPOCH_LOSS 437 0.0016406654903022824\n",
      "EPOCH_LOSS 438 0.0016365331212004706\n",
      "EPOCH_LOSS 439 0.0016324071712084762\n",
      "EPOCH_LOSS 440 0.0016282900534292093\n",
      "EPOCH_LOSS 441 0.0016241810523198769\n",
      "EPOCH_LOSS 442 0.0016200812313214151\n",
      "EPOCH_LOSS 443 0.0016159891696143891\n",
      "EPOCH_LOSS 444 0.001611904022302391\n",
      "EPOCH_LOSS 445 0.0016078257289820538\n",
      "EPOCH_LOSS 446 0.0016037572516199433\n",
      "EPOCH_LOSS 447 0.0015996962905355833\n",
      "EPOCH_LOSS 448 0.001595643322539033\n",
      "EPOCH_LOSS 449 0.0015915960142366204\n",
      "EPOCH_LOSS 450 0.001587554508526681\n",
      "EPOCH_LOSS 451 0.0015835237357463255\n",
      "EPOCH_LOSS 452 0.0015794982889584944\n",
      "EPOCH_LOSS 453 0.0015754805404457218\n",
      "EPOCH_LOSS 454 0.0015714679767037916\n",
      "EPOCH_LOSS 455 0.0015674641372594763\n",
      "EPOCH_LOSS 456 0.0015634653458997322\n",
      "EPOCH_LOSS 457 0.0015594735403194721\n",
      "EPOCH_LOSS 458 0.0015554896377667474\n",
      "EPOCH_LOSS 459 0.0015515112881013579\n",
      "EPOCH_LOSS 460 0.0015475397624678832\n",
      "EPOCH_LOSS 461 0.001543573513364664\n",
      "EPOCH_LOSS 462 0.0015396147291514995\n",
      "EPOCH_LOSS 463 0.0015356627552088667\n",
      "EPOCH_LOSS 464 0.0015317150240153907\n",
      "EPOCH_LOSS 465 0.0015277747452812293\n",
      "EPOCH_LOSS 466 0.0015238395675900593\n",
      "EPOCH_LOSS 467 0.0015199113502551388\n",
      "EPOCH_LOSS 468 0.0015159897641348918\n",
      "EPOCH_LOSS 469 0.0015120717623784627\n",
      "EPOCH_LOSS 470 0.0015081602412630832\n",
      "EPOCH_LOSS 471 0.0015042548038335475\n",
      "EPOCH_LOSS 472 0.0015003576161462192\n",
      "EPOCH_LOSS 473 0.0014964631929245311\n",
      "EPOCH_LOSS 474 0.001492576204093496\n",
      "EPOCH_LOSS 475 0.0014886955516513636\n",
      "EPOCH_LOSS 476 0.0014848202196535178\n",
      "EPOCH_LOSS 477 0.0014809482124743222\n",
      "EPOCH_LOSS 478 0.001477084956362507\n",
      "EPOCH_LOSS 479 0.0014732256802630307\n",
      "EPOCH_LOSS 480 0.0014693730526735666\n",
      "EPOCH_LOSS 481 0.001465524909424204\n",
      "EPOCH_LOSS 482 0.0014616837914169277\n",
      "EPOCH_LOSS 483 0.0014578477421677595\n",
      "EPOCH_LOSS 484 0.0014540170701456655\n",
      "EPOCH_LOSS 485 0.001450192628712949\n",
      "EPOCH_LOSS 486 0.0014463747604976293\n",
      "EPOCH_LOSS 487 0.0014425618168257266\n",
      "EPOCH_LOSS 488 0.0014387548456536443\n",
      "EPOCH_LOSS 489 0.0014349526383603348\n",
      "EPOCH_LOSS 490 0.0014311580952076715\n",
      "EPOCH_LOSS 491 0.0014273703401010917\n",
      "EPOCH_LOSS 492 0.0014235856600963639\n",
      "EPOCH_LOSS 493 0.0014198090077450422\n",
      "EPOCH_LOSS 494 0.0014160372165990243\n",
      "EPOCH_LOSS 495 0.0014122721059782065\n",
      "EPOCH_LOSS 496 0.0014085130828993921\n",
      "EPOCH_LOSS 497 0.0014047600818461075\n",
      "EPOCH_LOSS 498 0.0014010149349451143\n",
      "EPOCH_LOSS 499 0.0013972744431267766\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "initial_lr = 1e-5\n",
    "layers = 3\n",
    "cell = RNNCell(input_size=2) # passenger and the month index\n",
    "optimizer = optim.Adam(cell.parameters(), lr=initial_lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50, threshold=0.00005)\n",
    "a0 = torch.zeros((64)).to(torch.float32)\n",
    "c0 = torch.zeros((64)).to(torch.float32)\n",
    "month_indexes = dataset.data.Month.apply(lambda x: int(x[-2:]))\n",
    "\n",
    "EPOCH=500\n",
    "sliding_window_length = 12\n",
    "\n",
    "for ep in range(EPOCH):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, split-sliding_window_length):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        passengers = torch.tensor(np.array(dataset.passengers[i:i+sliding_window_length])).to(torch.float32)\n",
    "        months = torch.tensor(np.array(month_indexes[i:i+sliding_window_length])).to(torch.float32)\n",
    "        prev_a = a0\n",
    "        if len(passengers)<12:\n",
    "            continue\n",
    "\n",
    "        for p in range(sliding_window_length):\n",
    "            input = torch.FloatTensor((months[p], passengers[p]))\n",
    "            logit, a = cell(input, prev_a)\n",
    "            prev_a= a\n",
    "\n",
    "        target = torch.FloatTensor(dataset.passengers[i+sliding_window_length])\n",
    "        loss = criterion(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(\"Batch avg loss\", batch_loss.item() / 4)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(\"EPOCH_LOSS\", ep, epoch_loss / (split-sliding_window_length))\n",
    "    scheduler.step(epoch_loss / (split-sliding_window_length))\n",
    "    if initial_lr != optimizer.param_groups[0]['lr']:\n",
    "        print(f\"LR Changed {optimizer.param_groups[0]['lr']}\")\n",
    "        initial_lr = optimizer.param_groups[0]['lr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9ac3d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[158.62280507]]), array([[126.57203333]]), array([[144.01082052]]), array([[134.48662223]]), array([[121.63055696]]), array([[150.47571816]]), array([[157.8769836]]), array([[154.30711509]]), array([[151.96948378]]), array([[150.66313066]]), array([[147.99340563]]), array([[149.2534305]]), array([[169.52430385]]), array([[148.42136686]]), array([[172.20141578]]), array([[168.45384116]]), array([[157.04119949]]), array([[196.95142859]]), array([[198.61706877]]), array([[187.38562596]]), array([[179.38090461]]), array([[173.02867061]]), array([[168.34261324]]), array([[169.09336317]]), array([[186.39561248]]), array([[171.88517648]]), array([[204.55716509]]), array([[193.31335676]]), array([[181.5801301]]), array([[218.91072416]]), array([[231.0734095]]), array([[218.74782658]]), array([[212.85168111]]), array([[198.3284319]]), array([[190.25329965]]), array([[187.56924081]]), array([[203.66071892]]), array([[195.13933188]]), array([[229.08181977]]), array([[227.77728057]]), array([[231.87848049]]), array([[275.75356847]]), array([[277.19227588]]), array([[258.49837297]]), array([[245.20377171]]), array([[224.56641352]]), array([[209.33859211]]), array([[198.28986877]]), array([[210.31136179]]), array([[203.31264728]]), array([[230.6322028]]), array([[227.61483067]]), array([[227.50929922]]), array([[275.57551116]]), array([[287.89476466]]), array([[282.6432935]]), array([[266.87165165]]), array([[243.7039777]]), array([[224.77895844]]), array([[213.34363121]]), array([[226.40134245]]), array([[232.90256476]]), array([[273.72228867]]), array([[265.74285412]]), array([[273.58219242]]), array([[325.32496214]]), array([[343.73952162]]), array([[342.59136534]]), array([[320.98356855]]), array([[291.1747613]]), array([[263.62402767]]), array([[242.43440467]]), array([[255.67582339]]), array([[272.4618547]]), array([[321.68159521]]), array([[313.63733816]]), array([[326.3939544]]), array([[385.01689327]]), array([[409.44069409]]), array([[401.73242104]]), array([[376.21495867]]), array([[335.21737814]]), array([[295.89397478]]), array([[268.8658812]]), array([[276.28945398]]), array([[301.17522508]]), array([[351.63990021]]), array([[345.70238197]]), array([[363.77820957]]), array([[427.67126822]]), array([[458.36437225]]), array([[452.67583168]]), array([[428.88722646]]), array([[380.36381447]]), array([[332.20117927]]), array([[297.08694643]]), array([[298.29161978]]), array([[327.52096117]]), array([[376.23965883]]), array([[359.86885273]]), array([[372.89522564]]), array([[437.87966108]]), array([[470.45087302]]), array([[471.12926304]]), array([[454.55412483]]), array([[392.56328762]]), array([[342.01155984]]), array([[303.10077274]]), array([[301.05065906]]), array([[339.51060641]]), array([[394.10056412]]), array([[386.76273179]]), array([[411.786654]]), array([[488.84218347]]), array([[516.89903891]]), array([[521.46702564]]), array([[501.89023077]]), array([[438.18473899]]), array([[380.81462336]]), array([[338.95528495]]), array([[339.3077563]]), array([[391.61958694]]), array([[445.62893641]]), array([[414.99455702]]), array([[458.50099504]]), array([[538.75161433]]), array([[572.00782359]]), array([[581.21042573]]), array([[549.40809906]]), array([[477.14631581]]), array([[418.04613888]]), array([[364.9966687]])]\n",
      "[array([[115.00000034]]), array([[126.00000067]]), array([[141.00000165]]), array([[134.99999972]]), array([[124.99999906]]), array([[148.99999909]]), array([[169.99999815]]), array([[169.99999815]]), array([[157.99999814]]), array([[133.00000036]]), array([[113.99999969]]), array([[140.00000004]]), array([[145.00000037]]), array([[150.0000007]]), array([[178.00000331]]), array([[162.99999847]]), array([[172.00000137]]), array([[178.00000331]]), array([[198.99999851]]), array([[198.99999851]]), array([[183.99999753]]), array([[162.00000072]]), array([[145.99999812]]), array([[165.99999943]]), array([[170.99999976]]), array([[179.99999881]]), array([[192.99999657]]), array([[181.00000042]]), array([[183.00000364]]), array([[217.99999821]]), array([[230.00000209]]), array([[242.00000596]]), array([[208.99999917]]), array([[191.00000107]]), array([[172.00000137]]), array([[193.99999818]]), array([[196.0000014]]), array([[196.0000014]]), array([[235.9999963]]), array([[234.9999947]]), array([[229.00000048]]), array([[243.00000757]]), array([[263.99999505]]), array([[271.99999249]]), array([[236.99999791]]), array([[211.00000238]]), array([[179.99999881]]), array([[201.00000173]]), array([[203.99999884]]), array([[187.99999624]]), array([[234.9999947]]), array([[226.99999726]]), array([[233.99999309]]), array([[263.99999505]]), array([[301.99999446]]), array([[292.99999541]]), array([[259.00000244]]), array([[229.00000048]]), array([[202.99999723]]), array([[229.00000048]]), array([[242.00000596]]), array([[232.9999992]]), array([[266.99999988]]), array([[269.0000031]]), array([[270.00000471]]), array([[314.99999994]]), array([[363.99998617]]), array([[347.00000513]]), array([[311.99999511]]), array([[273.99999571]]), array([[236.99999791]]), array([[278.00000215]]), array([[283.99999636]]), array([[277.00000054]]), array([[317.00000316]]), array([[312.99999672]]), array([[318.00000477]]), array([[374.00000226]]), array([[412.99998784]]), array([[405.00000584]]), array([[355.00000256]]), array([[306.00000089]]), array([[271.00000632]]), array([[306.00000089]]), array([[314.99999994]]), array([[300.99999285]]), array([[356.00000417]]), array([[348.00000674]]), array([[355.00000256]]), array([[421.99998689]]), array([[465.00000978]]), array([[467.00001299]]), array([[403.99998879]]), array([[347.00000513]]), array([[304.99999928]]), array([[336.00000286]]), array([[339.99999386]]), array([[318.00000477]]), array([[361.99999839]]), array([[348.00000674]]), array([[363.]]), array([[434.99999237]]), array([[490.99998987]]), array([[505.0000124]]), array([[403.99998879]]), array([[358.99999356]]), array([[310.00000733]]), array([[337.00000447]]), array([[359.99999517]]), array([[341.99999708]]), array([[405.99999201]]), array([[396.00000679]]), array([[420.00001454]]), array([[472.0000056]]), array([[548.00000441]]), array([[559.00000668]]), array([[463.00000656]]), array([[407.00000906]]), array([[361.99999839]]), array([[405.00000584]]), array([[416.99999428]]), array([[391.00001419]]), array([[418.9999975]]), array([[461.00000334]]), array([[472.0000056]]), array([[534.99999893]]), array([[622.]]), array([[606.00000513]]), array([[508.00000179]]), array([[461.00000334]]), array([[389.99999714]]), array([[432.00000298]])]\n",
      "tensor(0.0591, dtype=torch.float64)\n",
      "tensor(16.7096, dtype=torch.float64)\n",
      "tensor(22.7312, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    output = []\n",
    "    labels = []\n",
    "    # input = torch.FloatTensor(test_loader.dataset)[i:i+sliding_window_length,1]\n",
    "    for i in range(len(dataset.passengers) - sliding_window_length):\n",
    "        passengers = torch.FloatTensor(dataset.passengers)[i:i+sliding_window_length]\n",
    "        months = torch.FloatTensor(month_indexes)[i:i+sliding_window_length]\n",
    "        \n",
    "        prev_a = torch.zeros((64)).to(torch.float32)\n",
    "        # logits, a_c_prev = cell(input, a_c_prev)\n",
    "        for p in range(sliding_window_length):\n",
    "            input = torch.FloatTensor((months[p], passengers[p]))\n",
    "            logit, prev_a = cell(input, prev_a)\n",
    "            \n",
    "        output.append( dataset.scaler.inverse_transform(logit.reshape(-1, 1)))\n",
    "        labels.append(dataset.scaler.inverse_transform(torch.FloatTensor(dataset.passengers)[sliding_window_length+i].reshape(-1, 1)))\n",
    "    \n",
    "    print(output)\n",
    "    print(labels)\n",
    "\n",
    "    toutput = torch.tensor(output)\n",
    "    tlabels = torch.tensor(labels)\n",
    "    # mape (mean absolute percentage error)\n",
    "    # mean((actual-forecast) / actual)\n",
    "    mape = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)) / torch.squeeze(tlabels))) # 0.1580, 0.1418(12-batch), 0.1343(sliding window), 0.1167(stacked lstm), 0.0730(stackedlstm with month)\n",
    "    print(mape)\n",
    "\n",
    "    # mae (mean absolute error)\n",
    "    mae = torch.mean(torch.abs((torch.squeeze(toutput)-torch.squeeze(tlabels)))) # 82.5248, 71.5019, 69.0149, 35.3759, 20.0789\n",
    "    print(mae)\n",
    "\n",
    "    # rmse (root mean square error)\n",
    "    rmse = torch.sqrt(torch.mean(torch.square((torch.squeeze(toutput)-torch.squeeze(tlabels))))) # 112.3071, 94.6528, 91.1919, 47.2027, 25.8126\n",
    "    print(rmse)\n",
    "\n",
    "finally:\n",
    "    torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f17296e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGfCAYAAAB1KinVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaEFJREFUeJzt3Qd8W+W5P/DHe2/HK44dJ3H2ToAEwiikCSuMpNBSVu/l0pYyCpRRWkhbKDeU9sItbUpb/lxCy05LKElZIYuRHbLIdKbtOLbjeG/L1v/zvDrvsSTLtnQknXMk/b6fj5Fsy4mshJzHz3rDrFarlQAAAABMJNzoJwAAAADgDAEKAAAAmA4CFAAAADAdBCgAAABgOghQAAAAwHQQoAAAAIDpIEABAAAA00GAAgAAAKaDAAUAAABMBwEKAAAAmE6kp19w6tQpevTRR+nDDz+k1tZWGjVqFL3yyis0c+ZM8XnenP+LX/yCXnrpJaqvr6cLLriAXnzxRSouLlZ/jdraWrr33ntp5cqVFB4eTosWLaLf//73lJiY6NZz6OnpoYqKCkpKSqKwsDBPvwUAAAAwAMcITU1NlJeXJ67/gz3YbbW1tdbCwkLr9773PeuWLVusx44ds3788cfWI0eOqI955plnrCkpKdb33nvPunv3bus111xjLSoqsra1tamPufzyy61Tpkyxbt682fr5559bR40aZb3pppvcfh5lZWV8fhDe8IY3vOENb3ijwHvj6/hgwjw5LPCnP/0pffnll/T555/3GxlxVPSTn/yEHnroIfGxhoYGys7OpmXLltF3vvMdOnDgAI0fP562bdumZl0++ugjuvLKK6m8vFx8/WD410xNTaWysjJKTk529+kDAACAgRobG2nYsGGiwpKSkuK7Es/7779P8+fPpxtuuIE2bNhAQ4cOpR/96Ed05513is8fP36cKisrae7cuerX8BM477zzaNOmTSJA4VsOLmRwwvjxnOrZsmULXX/99X1+346ODvEmcXqIcXCCAAUAACCwuNOe4VGT7LFjx9R+ko8//pjuuusuuu++++jVV18Vn+fghHHGxB6/Lz/Ht1lZWQ6fj4yMpPT0dPUxzpYsWSICHfnG0RcAAAAEL48CFG5OnT59Ov33f/83TZs2jb7//e+L7Mmf//xn/z1DInrsscdEWUe+cWkHAAAAgpdHAUpubq7oH7E3btw4Ki0tFfdzcnLEbVVVlcNj+H35Ob6trq52+LzFYhGTPfIxzmJiYtRyDso6AAAAwc+jAIVHhg8dOuTwscOHD1NhYaG4X1RUJIKMNWvWODTEcG/J7Nmzxft8y80xO3bsUB+zdu1akZ3hXhUAAAAAj5pkH3jgATr//PNFiefGG2+krVu30l//+lfxJpte7r//fvr1r38t+lQ4YHniiSfEZM51112nZlwuv/xytTTU1dVF99xzj2igdWeCBwAAAIKfR2PGbNWqVaInpKSkRAQgDz74oDrFY7+ojYMWzpTMmTOH/vSnP9Ho0aPVx3A5h4MS+0VtL7zwgtuL2jgrw82y3I+Ccg8AAEBg8OT67XGAYgYIUAAAAAKPJ9dvnMUDAAAApoMABQAAAEwHAQoAAACYDgIUAAAAMB0EKAAAAGA6CFAAAACCUHePlV7bfJL2ljdQIEKAAgAAEITe2lZKj7/3NT3yzz0UiBCgAAAABJn2rm7649oj4v6R6iaRTQk0CFAAAACCzFtbS+l0Q7u439VtpVN1bRRoEKAAAAAEkbbOblq6/qjDx47VNFOgQYACAAAQRF7bfJLONHXQ0NQ4umxslvjY8ZoWCjQIUAAAAIJES4eFXtxgy578+LJiGp2TJO4jQAEAAADD/G3TSapt6aThGfG0cPpQKspMEB9HgAIAAACGWXewWtzeedEIiowIpxFKgHLsDAIUAAAAMMiJs7ZAZGJeiriVGZSKhjYxehxIEKAAAAAEgdZOC1U3dYj7wzNsgUl6QjQlxUaS1UpUWttKgQQBCgAAQBA4edYWgKTGR1FKfJS4HxYWFrBlHgQoAAAAQeCkUt4pVLInUqA2yiJAAQAACALHa2wZlKKMeIePF2UmKp8PrGVtCFAAAACCOYMyBBkUAAAAMHiCZ3imYwZF9qAgQAEAAADDmmQLnTIow5UApaa5kxrauihQIEABAAAIggMCTyunFxc5BSiJMZGUlRQj7p8IoCwKAhQAAIAAV6rsOEmOjRRjxs6GB2CZBwEKAABA0PSfJIjdJ87UXSgIUAAAAMDoCR7nXSgo8QAAAIBuTigNsnyKsSuBuKwNAQoAAECAO6EEHvIMHmcj7HahWPlgngCAAAUAACBIRoyHO+1AkYalx1N4GFFzh4XONNsOFDQ7BCgAAAABrL2rmyoa2gbsQYmJjKChaXHi/gllJb7ZIUABAAAIYOV1rcRVG953kpEQ3e/jhiTadqHUtiCDAgAAAH52oqa3vONqxFhKibPtR2lss1AgQIACAAAQBDtQCvsp7zgHKIGy7h4BCgAAQDAsactw3SArJSNAAQAAAKMPCey3xNOOAAUAAAB0y6AkDPg4lHgAAABAF52WHjpV1zbgDhQpORYBCgAAAOjgdEMb9ViJYqPC1THiwXpQGhGgAAAAgD81KMFGWnz0gCPGDCUeAAAA0EVTu22nSVJs5KCPTY6zPaYBe1AAAADAn5qUiZwkpb9kIJjiAQAAAF00epBBkQEKN9by+T1mhwAFAAAg4Es8UYM+NiE6UpxoHCh9KAhQAAAAAr7EEznoY8PDwwJqkgcBCgAAQAg0yQbaJA8CFAAAgADPoCS7UeKxfxwCFAAAAPCbRmVkGBkUAAAAMI2mDvd7UBxGjRGgAAAAgN97UGLcLPGoGRTzL2tDgAIAAOChlbsraMEfvqAdJ+sCqkk2Wd0miwwKAABAUHlnWxnd99ZO2nuqgV7ffDJgNskG2jZZBCgAAABu+vvmk/TIP/eQ1Wp7f+uJ2oDZJMvQJAsAABBkXt14gp5472tx/5ZZBRQRHkbldW1UUd9myPPpsHSLtfUMY8YAAAAhiM+uefrfB8T9uy4ZSU9dO5Em5iWL97cZlEVpUrInLBFTPAAAAKHnSHUzdXb3UGp8FD0yfwyFhYXROcPTxee2HDc2QEmIjhDZHHcgQAEAAAgiJdVN4nZ0VpIITti5RbYAZathAUqXw+iwZ2PGCFAAAAAC3uGqZnFbnJ2ofkxmUDi7cra5w/QjxvYZlJbObrJ02/pXzAoBCgAAwCBKqpQMSnaS+rG0hGgarQQs207UmX7EmCXbBTNyAsisEKAAAABoyKDYZ1GMaJRt1JBBiYwIFz0rgVDmQYACAAAwgLbObiqra+2TQTG6D6VJDVDcz6AEUqMsAhQAAIABcI8JL2ZLT4imzMQYlwHKvooGau6wGFTiifTo6wKlURYBCgAAwAAOK/0nxVmO5R2WmxJHw9LjqMdKup/L06ShxBO0Acovf/lLMV5l/zZ27Fj18+3t7XT33XdTRkYGJSYm0qJFi6iqqsrh1ygtLaWrrrqK4uPjKSsrix5++GGyWMzdqAMAAKHrsBwxdirv9OlD0bnM0yTHjLWWeNqDLIMyYcIEOn36tPr2xRdfqJ974IEHaOXKlbR8+XLasGEDVVRU0MKFC9XPd3d3i+Cks7OTNm7cSK+++iotW7aMFi9e7LvvCAAAwIdKlAZZObHj7DyD+lCaNGZQAuU8nkiPvyAyknJycvp8vKGhgV5++WV644036NJLLxUfe+WVV2jcuHG0efNmmjVrFn3yySe0f/9++vTTTyk7O5umTp1KTz31FD366KMiOxMdHe2b7woAAMDXJZ5+MigzCm0Byp5T9WS1WtVFbv7WFOQBiscZlJKSEsrLy6MRI0bQzTffLEo2bMeOHdTV1UVz585VH8vln4KCAtq0aZN4n28nTZokghNp/vz51NjYSPv27ev39+zo6BCPsX8DAADwt5YOizgQcKAST05KrLht7+qhtq5u/ZtkYzwr8ciSUGObJXgClPPOO0+UZD766CN68cUX6fjx43ThhRdSU1MTVVZWigxIamqqw9dwMMKfY3xrH5zIz8vP9WfJkiWUkpKivg0bNsyTpw0AAKB5godlJkaLKR5XeK9IpHIWTl2rflkJ7RmUyIAYM/bou7riiivU+5MnTxYBS2FhIb3zzjsUFxdH/vLYY4/Rgw8+qL7PGRQEKQAAoN8Ej+vsCeOSTmp8NNU0d1BdSycNTfXf9dD1ojYPm2Tjg7TEY4+zJaNHj6YjR46IvhRufq2vr3d4DE/xyJ4VvnWe6pHvu+prkWJiYig5OdnhDQAAwN9Kql1vkHWWplz063XNoHRpGzOODYEApbm5mY4ePUq5ubk0Y8YMioqKojVr1qifP3TokOhRmT17tnifb/fu3UvV1dXqY1avXi0CjvHjx3vzVAAAAHRvkJXS4m3ln7rWTl2eV6elhzosPUE9ZuxR2PXQQw/RggULRFmHR4h/8YtfUEREBN10002iN+SOO+4QpZj09HQRdNx7770iKOEJHjZv3jwRiNx666307LPPir6Txx9/XOxO4SwJAACAKUeMXSxps5eqZlA6dc2esMQgneLx6LsqLy8XwcjZs2dpyJAhNGfOHDFCzPfZ888/T+Hh4WJBG0/e8ITOn/70J/XrOZhZtWoV3XXXXSJwSUhIoNtvv52efPJJ339nAAAAXuDV9afqB57gcc6g6FXiaVL6T7hBN0Jp0PV0kyw3yfb0WCncw683ZYDy1ltvDfj52NhYWrp0qXjrD2dfPvjgA09+WwAAAN2VKOUdPn8nrZ8JHik1IUrXKZ4mjQ2y9hkUXs/f0mnR9GvoAWfxAAAADNAg298GWdcZlE5TN8iy2KgIio4MN32ZBwEKAACAC9WN7eI2P23wsWE5xaNXk2yjxh0ogTTJgwAFAABggCBAlkQGwntQ9C3xdIlbreWZ3mVt5t0miwAFAADABblp1Z0xXv1LPBavMiiBMMmDAAUAAMDLIKC3xGP+JlmHXSgIUAAAAAKLXGQmx3LdKfHw13TzeIxOJZ5krT0oAbCsDQEKAACACzK74E6WQmYkrFZ9yiZNKPEAAACEJhkEuJOl4LHdxJhI3SZ5mjq8bZJFgAIAABD0JR691903+WjMGD0oAAAAAcbTXSPqgYEtXaYqPw2UQalHgAIAABA42ru6xYnBmjIoAdCDkqas7tdr6kgLBCgAAAD9lHfCwogSoz3LoOhR4mn0MkBJV84Oqm3pILNCgAIAAKbS1d1Dlm5b9sIoMkPBja/unvar57r7JnXMWFuJJz0hRrdylFbaQi8AAAAf4VLKH9eW0O7yBjpxtoXK69pEj8RH919IWUmxpt8iq/e6+05LD3Uo5SfNGRTluTZ3WKjD0k0xkRFkNsigAACAodYdqqYX1h6hDYfP0MmzrWLRWW1LJ328r8qw5yRLKO72n9hnUPxd4mmyW64mR5s9lRwXSRFKZsisWRQEKAAAYKgj1c3idtaIdHrjzvPoBxePEO9vOFRt2HPqPYwv0vPGUz9f8JuU4Ck+OoIiI7RdxsPCwtSeGQ4GzQgBCgAAGOpETYu4PX9kpnhbMDlPvL/x6FlRfjCCPOVXW4nH3xkUi1flnb6NsghQAAAA+uC+EzY8M0HcTshLpiFJMdTa2U3bT9QZvKQtUkOJx98ZlC6vdqBI6UrGp1anE5g9hQAFAAAMdbymVdwWZSSo5YeLRw8R99cbVObRMiWjLmrz8wW/0WcZFCVAaTbnqDECFAAAMAwHAjXKBXJ4Zrz68UvG2AIUbpw1tsTjfhAgF7XxhE1bZ7fpMyhpsgfFpMvaEKAAAIBheGqHZSZGO1xw54zKJB4yOVzVTBX1bcZlUDyY4uGJmkg5GePHLEqTjzIoGWpTL0o8AAAADo4rDbLDlfKOfcPptII0cX/9If2zKFrKKFyaklkUPQKUZC8DFDl1hCZZAADQ3Z7yerrvzZ10pLqJzDzBIxtk7V2i9KFsOFwdEIva7Cd5GvxYNmnydZMsAhQAANDbXz87Ru/vrqDb/28bnTVhM+RxZYKnyEWAcrHSh/LlkbPqwX16UbMUHpR4HNfd+y9AqVWyMzJboxUCFAAAMHwJ2qn6Nrrrta90v9C7nUFxKvGwiXkpojeF17HvOFlnyJixp30eeuxCqWpsF7c5ybE+apJFgAIAADrq6bGqO0aiI8Np64laeuK9r8lqtZJZnFCaZO0neCQ+pO+iYmXcWOcyj9YSjx7r7k83KAFKincBSkZib5Osmf5OSAhQAACCVEVDG7V39VBURBi9ePN0MRXz9vYyWrbxBJlBQ1uXWl5wlUFh0wttjbJHlUyQHvgk5RZlTNjzEo9/Dwy0Wq1UqQQouSlxXv1a8rlaeqxqU7CZIEABAAhSx87YsieFGQl02bhs+tmV48T7//tpiSl+YpblnaykGEro59A7dRRWx10dXFKSzFbiaeqwiA27vijxxEZFiPN8zDpqjAAFACBIHT1jyzqMHGLLTtx+/nBR6uHMRWmtrbRiihFjFw2yfQ7g07FPQi5pi4uKoCgPD+Pz97r7SiV7khIXRXFKcOGLRtmzCFAAAEDvDMqIIYnili+243KSxP29pxrILAGKXHE/YMlExwuo1gZZPTIop9XyjnfZE+cABRkUAADQzbEaWwZlhF2GYsLQFNMEKM6HBLqSppy4y1mf7h6rzgcFej7G6+8MSpUSoGR7Wd4JhAMDEaAAAASpo9WOGRQ2SQlQ9p1qJLP0oBS5mOCRUuNsF1COTeRkjRnP4fF1Saq9q1vsr3GeBvJ5BkWOGiODAgAAemjpsFClsi9D9qDYByicQTGyUZZ/b3d6ULhnhs+40bMPxZtNrXJ5mrcZn+Xby8QGYG5otlfZ2OaTEeM+ARUCFAAA0IO8+PMUjOyLYKOzk8TYMV9Ay+v0P4RP4qkcOdpamN5/gGJf5tErQJHPS0uJR2Z8rF5mfE4o+2H2VTTo0oOCJlkAANB1gmeEXfZEZiTGKI2yXxvYhyIDKL7QDjaN0tso26XvScYaSjz8+iYo30+9FwHKWeVYgqNKo7PzFI+ve1CQQQEAAF3IC9tIu/4TV2UeM664N3olu+xB0XoYny8mec4qAQP3htj3ociynbdL2iQ0yQIAgK6O9ZNBYRPNEKCoEzz9N8jquT7e9RSP5xkU+5KUN8/3TFNHn2CTG2fldJCvelDMfGAgAhQAgGDegZKZ6PIQPravotGwRtnjnmRQdN4m21vi0ZZB8UVJ6qxdwCDLdbK8wwvktJSfBsxOIUABAAA9DgmUAYCrDAr3oESGh4mLUoVy0dNbRb2tQbcg3Z0Mir59Er0lHo0ZFC9LPPznV2v3vcpg075BNiwsjHxBHiXQ1G6hrm5znXSNAAUAIMicbmyntq5uMa0zzEUAwGew8DSPkY2yslQhsyMD0XvdvTeL2nwxGVPvNKJ8VGZQfDxiLL9HPkTSjI2yCFAAAIK0/4SzE/2dJTNxaLKxAYoy4SKzDe70oOg3xSMXtUV5lZWobe70aoLH+c+zsqHD5wFKRHiY2tRrtkZZBCgAAEF+Bo8rRk7ycAlDNpDKxWb+LJlozqBoLPGkJ3qXQalRAptEZUEdH+zI5ZfKBiWD4qMR4z6NshoDKn9BgAIAELSnGPcfoMhJHs6g6N0o29RhEavr5am8ZgpQ+LVQMyhxXmZQWhwzIe6qUTIo43KTRENsV7eVympbfb6krc+6e2RQAABAnwxK/xMy43KTRXqff1qvatR2IdVKZk/44sv9MIPp3STb5fdgqrWzW+3/0FriSU+I8WoyRpZ4hiTFUJFyDAD/mVYpO1ByfLQDxezL2hCgAAAEGdmzYH8GjzMODIqzEg3pQ1EbZN0o79geZ7uAcuAg19D7u7zDU06xUeGGNMnKr8tIiFGDTD6ZWmZQfF3iSTPpunsEKAAAQYSXecnRYVc7UOzJn85PKSO/epGlmhQ3GmRlMMXZFj2WtdmXd7SO8tqP7nZaejT3oGQmcoBi+zM8WNlEZ5p93yTL0mWGCgEKAAD4i+xfiI4IH7QBNSvJVoqobtJ3FwofVOhJBsWhDOHnZW3ygD9vFqFxXw2Xz7T2zcg/w4zEaDULtuVYrTiAkEfHZQDkK7IkhQwKAAD4vXzCwclgGYAspVRQrXMPivxJ3Z0JHkk+1t8/5csSj9ZzeFh4eJgafJ3VMBkje1AyRYCS6JDl4kMC+df3SwYFTbIAAOD//o7Bf8oekigzKDoHKGoQ5X4mQK9Jnt4ST6RhZ9zITEZmYm+TrOTr/hPHpl599sy4CwEKAEAQkRdwd7ITQ5KNCVC0lHhkI6e/z4zpLfFoz6A4Nsp6/trWKH8eGYkxlBAT6TBW7Ov+E4cxY41j0f6CAAUAIIjIJlJ3MiiyB+WMzj0oahAV50kGRZ4Q7OcelHbvzuGReAJHS0DV1tlNLZ3dtl9DWfhmPy7u6x0oDmPcLf4f4/YEAhQAgCAiyyfyojOQrKRYtaRg0fGgOPs+GY9P3fVziad3i6xvMiieBigy4xIdGU5JyiZZ+2ks7kHxNRlMdXb3UHOHf8e4PYEABQAgKEs8g2cneBqEp034h2Y9Jzh619xryaD4u8Tj3RZZb3ehqCPGCdFqk7P9PptcHy9pY3HRvDAvXJcSmicQoAAABBE1O+HGBZanQXhSRO9Jnnoz96CoUzxelngStZ1vIyd4MpQGZuczlfzRgyJ+PxOOGiNAAQAIwgyKOz0o9mUePXehaBkzlt+Pv3tQvD3J2OsSj7qkLVr9mH0Pir8ClEylH0k26JqBdyEiAACYiqf9Hb3L2vS5MNmvq/ekxNO7qE2fKR5vMyhap3jOuMig5KXE0czCNHGisT/GjGVJyb7EZAYIUAAAgnGKx81to1ly1FinEo8cMXa3DNV3UZtt0kTrGnq3m2S97EHRWjKRGZQMuwwKl+KW/3C2uO+v75t3rthvsTUDlHgAAIJxisfNDMoQnUs8MoDiCZXIiHCPSzw8acInDvsDBz5nlEySzIBoJb+eM1qeTEjJjMsQuwyKDEz8FZywzCQl44MABQAg8D327h5a8IcvTDP5YCufeLalVe8Sj7pF1o0xaHvx0RFi9Jb56/Uur2sTPSh8jtHwjP5PgnaHfYDoyflB9ufw6ElmfMxU4kGAAgCgAS/UentbGe091UDPfHiAzIDLJ3LPFh9YZ8YApV7DkjbG2QN/L2vbV9EobouzE9VgSCvODsmylCcBlVriSXDMoPibbJKVPTBmgAAFAECDA5WN1KMEA+9sL6ftJ2qNfkpqAymXT6LcLJ/IAwPPNLabdkmbXsva9p+2BSjjc5N98utpaZRV96Ak6hygKBkblHgAAAKc/Glbevy9r3XdxjpgdsKD8skQu5+c9Vhz7skiuf5Hjf0UoCh/puPzfBOg8CI8TzIoPT1W9TycTJ1LPLLnJWhKPM8884xIu91///3qx9rb2+nuu++mjIwMSkxMpEWLFlFVVZXD15WWltJVV11F8fHxlJWVRQ8//DBZLOZZrwsAMJj9FQ3i9rvnFYhswMHKJlq28UTAnGTsfGHq6rZ61Cuh50GBvjgh2B0H/JRBcff5cvAms3JpXjbpekqONfOfT6fF2EDb6wBl27Zt9Je//IUmT57s8PEHHniAVq5cScuXL6cNGzZQRUUFLVy4UP18d3e3CE46Oztp48aN9Oqrr9KyZcto8eLF3n0nAAA6+vqU7WI2Z1QmPXbFWHH/+dWH6XRDm2HPSW1A9SBA4V4LGSzoMcnTe1Cg5wGKOmrsh0CKszKn6m1/duN8lEFJl6PGbmYl5Egy/3lEeTDh5Av858HHHtiehznKPJpegebmZrr55pvppZdeorS0NPXjDQ0N9PLLL9Nzzz1Hl156Kc2YMYNeeeUVEYhs3rxZPOaTTz6h/fv302uvvUZTp06lK664gp566ilaunSpCFoAAMyOF2YdqmwS9yfmpdANM4bR9IJUcQrtXz87ZoKTjD27+KvbZHXYhdLbg2KuEo/sPylIj/d6i6zWEk/vBI++/Sdy14p8vu4GVKYMULiEw1mQuXPnOnx8x44d1NXV5fDxsWPHUkFBAW3atEm8z7eTJk2i7Oxs9THz58+nxsZG2rdvn8vfr6OjQ3ze/g0AwCglVc1iHwdvGx2WHif+cb/9/OHic7vL6gNmzX2fZW06TPKoZSgPx4xtX+O/Eo/af+Kj8o6WEo/s/5CBgt5kYGSWSR6PN8m+9dZb9NVXX4kSj7PKykqKjo6m1NRUh49zMMKfk4+xD07k5+XnXFmyZAn96le/8vSpAgD4xT6l/4QvZnJ5lrywcS8KNzty0KI3Wfpwd8S4T6OsHgFKm7YxY+bPMWNfN8ja7zJxt2QiJ2jkyK/eZGOuWc7j8SiDUlZWRj/+8Y/p9ddfp9hY/5wH4Mpjjz0mykfyjZ8HAIDREzwTh6aoHyvKTBD9HLzltLS2NTBLPHr0oLR4MWbszwyKjxtktWRQ1IMCDcqgDEk014nGHgUoXMKprq6m6dOnU2RkpHjjRtgXXnhB3OdMCPeR1Nc7pjh5iicnJ0fc51vnqR75vnyMs5iYGEpOTnZ4AwAwOoMywe6nbV7MNSY7yeFipzd58fd0AkTPZW1yisdMPSjtXd10pLrZ5xmU9ADqQTHjicYeBSiXXXYZ7d27l3bt2qW+zZw5UzTMyvtRUVG0Zs0a9WsOHTokxopnz7YddMS3/GtwoCOtXr1aBB3jx4/35fcGAOBzXL6R5YAJeb0ZFDYuN8lhXFVv9Rov/rIH5Yyfm2R5fLW5w6J9zNhHi9pW76+iB9/ZpQYOHJxYeqziOeWm+K46ILfBcumN/96YdUmbJHtfagKxByUpKYkmTpzo8LGEhASx80R+/I477qAHH3yQ0tPTRdBx7733iqBk1qxZ4vPz5s0Tgcitt95Kzz77rOg7efzxx0XjLWdKAADM7MTZFjGtExMZTiOHOJ7XIssDhgUoJi/xyP4TbttJ0jApIxfQtXf1iKMG4qIjND2PP6wtoT3lDWTpttILN01z6D/x5YF8shGYz0jizNFgmS3Zq6L3OTxSZiCXeNzx/PPP09VXXy0WtF100UWibPPuu++qn4+IiKBVq1aJWw5cbrnlFrrtttvoySef9PVTAQDwW//J2NzkPqfxjlMDFNsIcsBM8ehU4mmwa+KVOzc8YVvhH+Z1FkU2A7+/u4LWHqxyaHr2pZjICPGc3b3oy8yF3ltk+5zH0xSgUzzO1q9f7/A+N8/yThN+609hYSF98MEH3v7WAACGBSj2/ScSBy2MF35xNkNLn4U3fRScWdDSgCpLPNzgyyWYROWi6rdFchqWtDHObvBP+acb2kWfxNDUOI9/DV7nb7/n4/EVX6uZDV/2n0jpidHU1GFxqw+lpsmYgwL7TPEE8h4UAIBQJX/a5gVtzjgzIC+aemdRZPYkMjzM4wAjPjpS/ZpqPx4aqJ4V5EXgJssQWn/K52CBd9gw/rOqaGhXg87xuX3/TH23rG3g59vSYaG2rm6Dx4xj1OfKZSmjIUABAPDgp++BMij2P4Xr3YdiP76rpY9iiA5lnt6zgrRvarU/3FALmT3hgOw3i3qPauER8RFOPUU+XXc/SAZFlndio8IpQWNvja+mjjg28deBjJ5AgAIA4CYuLXCqnvsnxuTYJnac9fah6BugeJud0CVAafM+g6KeuqvxecplaNyIOqc4k741I1+8PzYnyS/n36gZlGb3AhT+c/Blo64n+PuXwaMZyjz+KTQCAAQhOe1RnJVIsVGuf8odn2vMLhQ5Yqw1O6E2yvqxxNN7mKFxGRTndfKLF4wXPTHzJrjew+WLHhR3MihnmowdMZZ4Bwv/OXHANIZcB+F6QYACAOCmCuWk4sKM+H4fIzMofF4PHyqo16m06inBGrMTctTYnxMc6kGBGtbc+2otf+8or+3X4YMBH7/afzu43D0wsHeCx9gAJTMxmo5Um2MXCko8AABukhcZ2VfgyrC0eNHfwI2Yx8606PbcvO3v0OPAQHVPi4aDAn0WoKjL0PSZsEoPuAAlxjQlHgQoAAAeByj9X2D5kEDuZ9C7D6WuRdsOlL67UPw5xaPtMEPfNsl26DrKKwMUd5tkhxi0A6VvgIIMCgBAUGVQ7Ms8evahqCcZa+5BUbbJ+nHdvdZFcr4cM65R/gz12tY6xM3eHrkDxagRY0lmlmQgZyQEKAAAHgYosq+gP0ZM8vSuudeYQZHn8TTr0IPigyZZXirHu0O0T/HoEwjkpcSpGRReptcflHj6QoACAOBhgDLYmSpG7ELpzU54N8XDQUSHpf8L6WBe/uI4vb7l5IBjxt5kUHhHSJwyQaWlDKEeyOfhic9acTDG5zaxqgGyKOYLUJBBAQAIugzKmOwk9WIoe0PMepKxxH0hvKzMm/IJ9688tWo//XzF130uxt6s4rfHO0K8aZTVO4PCzzdP2S5cUT9QgKJv825/ZOlL654ZX0KAAgDg5hZZNUsxSIDCp+zKIEaOJus3xROt/cKf6N0kT1VD79dtOHTG5fPTsorfmdYAxdLdo/bq6HlicG6Krb/ndD9/Fzh44zOQzNGDEqP26vDfeSMhQAEAcPMMl65u2z/Y6W4EAe781OwrPT1Wux4U7dkJddRYY6Os/QTQukPVDp+TGRWtq/jtyUDK034ZeQIy//belJk8lav0ofAmYldkoMUZLHn6sVFk8Ndp6RF/542EAAUAwA1yVTn3P3CGxNufmn2pqd0izk/xZorHvg/ljMZRY/vMyxclNWJRnfTxvkpxO3VYKnlLawZF7kDhAJOPK9BLXurAfxd6R4yNW3Mv8YZkmeEyusyDAAUAwIOfvuVeCzNlUGTpKT46gmIitR805+15PPYBA//0veNknZrh+deuCnH/+mm2s2+MaOSUAYqe5R2WI4PVfv4umKX/RJKvz2C7W/wNAQoAgAcZFHcvbvKn5op6/2dQfLFfxBe7UJyXvK1X+lC2nqilU/Vtonxx2bgsMiyD0qLvkjbnUeOKfko8Zpng6RMAIoMCABA4GRR3g4DevgP/Byi+2C/ii22yMrCZWZgmbtcrfSjv7Twlbq+clNvvIYt6BChqpkLnRtTcwUo8TWYLUKJNMWqMAAUAwIcjxkaUeHyxX8QX5/HIr/vWjHziFo+DlU10oqaF/r33tPj4ddOGki9o70GRGRR9SykyWOVAsq2zu/8MSpI5SjyZahMySjwAAKannnXjdoBi+6m5srGdumUHq9+em68yKLE+6UEpzk5Sm2Gf+NfXook3LyWWzitKJ18GKJwR8WQUVu+DAqXk2EjRH9RfFqW3B8UcGZQM5XkYve4eAQoAgBtkw6C7TbJ8sedJEQ5OtC4+c1d5XZtPLnCyxMMXJk+DKg4U5PfJv843xth6TT4vqRG3104bKg5S9AUZYPCJ0Y1tFs97UHQOBHgyp3eqq29GTY5LmyVAmVaQSjedO4zO9VFAqRUCFAAADzIo7gYoHJzkJNsuStwg6k+bjp0Vt9OV3g+t+MLNMQTHJp7+9NzQ1iUCBpnhuEQJUKSFPirvMJ5U4qwEO9PsfglNZir0LvE4lvzaTN+D8o0xWbRk4WS6dqrv/sy0QIACAOCHDIpeu1C4N0ae+TN7RIZXvxYHVTK74GmZRz6eAwduhJ2Ql6xecPk+l318SctItOz10DuDwtzJoAwxSQ+KWSBAAQDwYJTXkwBF/tTc3/4LX9h09Kx6/o+8aBsxyaOWd5SsEZdzrpyUI+7fdG4B+ZqWRlmjelAG2ibLa+65R8dMGRSzMHanLgBAgO1B8SiDkur/Es+XR209HueP8i57Yh+g7NMwISMDGhngsMeuGEcLpuSpY8e+NERp6HX3ebZ2Wqitq9sEGZQ2l5m5qIgwcWAj9EKAAgAwiA5Lt3ouiSf9C0NlBsWPJR6ZQblgZKZPfj2ty9rk4+0DFD4S4Jzh/mm09PQ8Hpk9iYkMpwQ3jirwtdx+smmy/4SXxxm95t5sUOIBAHBzERr3aCTHRnmc1vfXLhRuuDxe0yIaW88d4ZtAQOsuFJnJ8EWZyaNR4yb3dnXYb2s1IhDgMWtXp1ubbQeKmSBAAQBw86dvPinYk1FZfzfJfnnEVt6ZnJ/qUeDkjx4UGdDIDIy/qT0oHmZQ9D6HxzmDwv0mzXanBJttzb2ZIEABAPDTWTeyxMPjrdwM6Y2G1i56ePlucUpwn/KOj/pPvDkwUO1BUTIw/iYbXd3tQek9h8eYAIVPCOaziFilXcBqtiVtZoIABQDADyPGcrNrbJTtn9nKfg6Kcxevi1++o5zu/Nt2MVbMi9HUBlkf9Z/YN5963INiUInH3QBF3YFiYCAgm6btS37y+SNA6QsBCgCAj5e0SdzroC7o8rLMU9Vou6jxJMoP/r6Dviqtp6rGDoqODKcZPpySkSUevnB6skbefousngFKbYt7W2+NLvH0d4Bkb4kHPSjOEKAAAPgpg8LyfNQoa99rUVrbSv+5bJu4zyO8vjgh2PnCz1theTusO+x3ecgMjL/x1Iu69VYp3wxEPkZO/xhBns9k/3dBBih6ZZ4CCQIUAAA/ZVAcGmW93IUiMxS89IzLRjJ4OH+k7/pPGAc7ch+Hu30oshzEI7xyBb2/8URVeoL7kzzmzaB0Gh44mRUCFAAAN9bJa86gqCUeLzMoSrBw8egh9MzCyerHzx/lu/6TPpM8bvah2DfI6jnC68kkj7rmXglqjJDjYt1975gxAhRnWNQGAKbD2YHbXt5Cnd1WOq8oXZyqOmtEhqYAwRdkeUBbgCLT+r7JoPBFmXtOeFSVMxzThqWSr3GgUVLd7PaocW//iT7lHS2TPDUmyKDIcp8MULq6e9QdO2iS7QsBCgCYzpoDVbS7vEHc54mVZRtPiPLBqnvn+PzQOXfUtXR5UeLxfpssN6vKLIHMbtwyq5D8Rd0m626JRwZPOl9k3Z3k6emximZaowMBOcXD5T7+M5VlJy5XpWLNfR8o8QCA6WxU9nvMHZdNt88uFBflDksPrT90JvCaZFO9b5LlNfudlh7dLrDelHj0JAMpOeHUn/q2LtFMq2WXjS/JfqSWzm5qbLfYlZ2iPVoAGCqQQQEAU+GfLOUCsltnF4qei5yUOPrNRwdpV1m9Ic9Hy0nGziUeLsk0tndp2vgqMwS86IvPt/G33mVtnpZ49A1Qhrp5GONZJRDg5l8eyzZKfHSkeA5cwnxq1X7sQBkEMigAYCpltW3ighMZHqaegjutwNZnsbO0Tvfn09hmUfdsaPnpmy9KvLDN1UFxZj3nJitZW4lH7x6UoWm27NSpuoEDFN4XY5ZR3oL0eHH7jx3ltOGwLSM4YkiCwc/KnJBBAQBT2XTMth116rBUSlBWg0/OTxE7L3gShjeyymkIPdQq2RM+AVfrvhHuQ+FmSG6UHZPjeQ+N+pO2XgGKehCfZ2PGegcAQ1Pj3cqgnKpvVR5vC2iMtHjBeHr3q1NiHDs1PlqUd745Ptvop2VKCFAAwFRkeWe23X4PzkKMyUkWDbO7yuro8pRc3Z6PbK5M92L6g0+y5eeudZus7hkUD8/j0XvNvXMGhUsmXELj825cKVcyLPLxRjpneLp4g8GhxAMApsH9HhtdBCiOZR59+1Bq5QSPF82VvY2yGgMUuW1Up14FGWjwRb+1s/fkXVe4/CWDOL2bZDkgkUvlBirzyM/lmyBAAfchQAEA0zhW0yJ+GudGxukFjufLyH0fO3VulFUzKAneByiD9UqYJYPCF/44pZw12CQPN6Byiw6X4IxYgibLNrKM40q5EhiaocQD7kOAAgCmIbMnMwr6ni8zTQlY9pTXk6XbNnKr74ix9ouvLC1oHTVWAxSdMii8DTZbyYZUDjLCK8s7fEow7/PQmzuNssigBCYEKABgGpv7Ke+wEZkJlBQbSe1dPXSwssmAc3iifPBTfmBkUNgwZdqkrLb/zITDDhSDJmTkayuzJM44mJVBVn6a7XuCwIAABQBMgbd9bjrWf4DCi6x4skfvMo9PMijKRZQvlFqyP0aceOtugGLUDhQpf5AMCr/m3CcTHRGOA/kCDAIUADCFw9VN4lA+7n2Yku/6fBlZ5tmlY6OsLzIofPGOiggTF8oqNydjJP4aGSTpGaDIfR0nB8ugNBqzA8Xd7JSc4OE189jWGlgQoACAKWw8YsuezBye1u+2T3WSp0y/hW3yYDdvMih8YZRn8njaKMtbbDlI4UOC9TwssVAJUEoHCVBOnFV2jBjU3zFYDwr6TwIXAhQAMIWvlC2xfGpxf6YqmZVjZ1qoXlmg5k/HzjSLfhf+wXvKsBSvfi2tpxrLEgqPOUdFhJuuxHPkTLO4Lc5KJCMzKNys22Hp7vN5mVnBBE/gQYACAKYgSwWFGf03MqYlRFNRpm0tuB7n8qzYeUrcXjR6iNclDHe3npqhQZYVKH8ONc2dYh9Kf3trjlbbApRRBgUonFWKjQrv9yiB8jq5RRYNsoEGAQoAmIJsBB3s4DR1H4oP+1DWH6qmO5Ztc8gWcNMuryRni6bn+6wUIXsizB6g8KGGacoZQv1lUbgBlYMXHi8uzDDmPBkeiR6oD0V+DCWewIMABQBMwd0LsexD2V3uuwDl/748QWsOVtPPVuwVWQG25XituLjxaLMvzkoZqrXEo/MWWVeNsv31oRxRsiec9TLylOChyviwqz6UUyZacw+eQYACAIZr7+qmJqWMMFgGZVxusrgtqbJdHH2hWtmT8XlJDa3eXyXu//OrcnF79eRczYcE+rLEo9dBga76UEqVRtj+ApRRQ4wp7wy2C4WzYHI5HjIogQcBCgCYprzDuyr4lNeBjFQuhnyhH+ycGE+DAPbUv/eLBtwP954W7y/0QXnHedpEZmk82oFiQAZF9gMNlkEpzjY2QOlvFwpnnzq7e0QJKifZmDFo0A4BCgAYTs0SJEaLnoKBcKMsH1Evp3m81dXdQ7XKRFBqfBSV1bbR7a9so5bOblHimFnoeCaQVrkptgtkW1c31bfaDiA0cw+KOyWeEoMbZAc7j0f2+3BwEqnjBBT4Bv7EAMBwPCniyUVYZlGOKiOu3jjb3Emc0OCfsn+5YIL42G5lQmjh9KGDBkzu4jKRLF95UuYxMkAZNkiAok7wDEkiI6nZKafXVZ3gQXknICFAAYCAmeCRRmYlOFwgfZW9uXZqHp0zvDdjsnCab8o73kzyqE2yBgQocjKHL/S8LM55w67ccCv/PIzOoPCYsf3zVCd4sAMlICFAAQDD1ahBgmcZFLkkzBvysDsOADhb8strJlBCdATNG5+t7gLxFU8neXjxmCwHGdGDwqURXtHf1W3tc6qxfO05OIiPHrhvyN+yuYQTHkaWHqv658kwwRPYjP1bBQCgIUswUul5OFrtfQ9K72F3tuBhQl4Kbf35XJ9M7nh7qjGXnxgHCSlx2s8C0orLXnwC8PGaFjp5tsVhG6ucojK6/4SJJtiUWJGZ4qBEHisgM1WY4AlMyKAAgIlKPO6dNSPHWvnCqeV0YHu8It05Q5EQEykuer6Wp1zg3c2g9JafYgw76E42yjova1NHjE0QoPQX/PWuuccW2UCEAAUADFfT1OnRrg++GMVEhosRUk83s/abQUn2fwnF0wyKkQ2yg03yyBKPaQIUp/4eHuVGiSewIUABANOUeNztQeFswggfTfLY96AYffKumbbIOgcoJ52WtRl9Bo+zfKfgr7alU4x02x/UCIEFAQoAmKZJ1pMgQV4YZanB+x4U/TIoPP3C23N93TzsD7JR2L7E09JhUQMBo7fI9hf8yefHf64xkb7vJwKTBSgvvvgiTZ48mZKTk8Xb7Nmz6cMPP1Q/397eTnfffTdlZGRQYmIiLVq0iKqqbGujpdLSUrrqqqsoPj6esrKy6OGHHyaLxTfbIAEguNfc2xs5JMFHGRT9yijc6MoTQu6WeYwcMe6TQbELUORrzj1DvDjPDJyPEkB5J8QClPz8fHrmmWdox44dtH37drr00kvp2muvpX379onPP/DAA7Ry5Upavnw5bdiwgSoqKmjhwoXq13d3d4vgpLOzkzZu3EivvvoqLVu2jBYvXuz77wwAAoLMYLiz5t7XGRTuU3Ce4vH7ybselHnM0IMil7XxuHNDW5fDay7Hvc1Avq4cPN3zxle07lC1eJ+nkCAEApQFCxbQlVdeScXFxTR69Gh6+umnRaZk8+bN1NDQQC+//DI999xzInCZMWMGvfLKKyIQ4c+zTz75hPbv30+vvfYaTZ06la644gp66qmnaOnSpSJoAYDQo541o+whcVfvNtkWj862sdfYbqEOS4+uQYAnkzxmCFASYyLV6SpZ5jHbBA8rTI8Xp07zX4VVe07TO9tthz3aj0ZDiPSgcDbkrbfeopaWFlHq4axKV1cXzZ07V33M2LFjqaCggDZt2iTe59tJkyZRdnbv0eXz58+nxsZGNQvjSkdHh3iM/RsABNeae3dHjKWizATieIZ/qpe/hqdkAJAUG+mXvSfeTvLIsgrv+DCS88p7MwYo3Dj90m0z6YP7LqQbZuSLjBybkp9i9FMDvRa17d27VwQk3G/C2ZMVK1bQ+PHjadeuXRQdHU2pqakOj+dgpLKyUtznW/vgRH5efq4/S5YsoV/96leePlUACMI19xIHFMPS4sVFk9P6WrIMek7wSO6WePi5cQDFQdiYbGPPuuE+lJ2l9eKMIj5ccadyVlFxlrHPy5Xxecn02xum0KNXjBWTR9MLHK9JEMQBypgxY0QwwiWdf/zjH3T77beLfhN/euyxx+jBBx9U3+cMyrBhw/z6ewKAPuyXkXmKG2VlgDJrRIapJ3g8zaDsr2hUM0W8OM7o8gn7y2fH1I/xavmxueYLUCT++2Tk9BN4z+O/9ZwlGTVqlLjPfSbbtm2j3//+9/Ttb39b9JHU19c7ZFF4iicnJ0fc59utW7c6/Hpyykc+xpWYmBjxBgDB3YPiKS4xrDt0RnOjbG+PR6z5ApTTtgBlfG4yGe2conRxy9mciXkpNHtkBl0+MQcBAPiV12F5T0+P6BHhYCUqKorWrFkjxovZoUOHxFgxl4QY33JjbXV1tRgxZqtXrxYjy1wmAoDQ4+ma+/4aZbUwIoMi94pwkyyPWPfX+yIzKHw2kNEuLB5C6x+6hNLioyklXv8zgSA0RXpaauHJG258bWpqojfeeIPWr19PH3/8MaWkpNAdd9whSjHp6eki6Lj33ntFUDJr1izx9fPmzROByK233krPPvus6Dt5/PHHxe4UZEgAQrzEozGDYr/V1Mw7UCTeCpsWH0V1rV0i8zNxaMqAAQr3VJjB8Ezb3hkAUwYonPm47bbb6PTp0yIg4aVtHJx885vfFJ9//vnnKTw8XGRQOKvCEzp/+tOf1K+PiIigVatW0V133SUCl4SEBNHD8uSTT/r+OwOAgCAncLSsc5cZFC6XtHZaKD460vQZFB6lHpOTRJuP1dLByiaXAQpvaj1+tsU0JR4AI3j0fzPvORlIbGys2GnCb/0pLCykDz74wJPfFgCCmLrOXUOQwFtMMxKixer4o9UtNMnDkVIjpngYT+VwgHK4qsnl5w9WNop9Hhw4GbkDBcBIOIsHIARxtiGQ19zbK862ZVH6u9gPRM8tsvbG5NiyIpxBGbj/BNkTCF0IUABCzD92lNP4xR/T7z4+FLBr7u2NVnaEHK72LEDptPSIPhBDMig5tud8qLJx4AkeBCgQwhCgAIQQXgn/18+Oivt/XHeEXvnyeECuuXcZoPSTjRjs946KCKPUOH0nU0YrWZ+qxg6qb+27BXefiSZ4AIyCAAUghOwpb6DDVc1inwV7ctV+WrWnwgRL2qK9zkbw96VlgodLS7wmXU9JsVHqPpRDToGVpbtHLf2gQRZCGQIUgBDyzvYycXvNlDy6fXahaMR88O3dtPFojcHn8GgvsYxW1q3zJE9Tu61kY9YJHpdlHqfeGd7pwuUnPqSPV8wDhCoEKAAhoq2zm97fZcuW3DhzGC1eMIGunJRDnd09dO8bO8VFMZC2yEq8OCw7OcbjLIpREzx9+1AcA5T9pxvE7bjcJN0zOwBmggAFIER8vK9STMxwaWH2iAyKCA+j526cKsorPKa77URtwBwU2F8fSokHkzxGrLm3N7afAGXfKfOsuAcwEgIUgBAr79wwM1/9yZzXrH9jjO3YiTUHqgOyB4XJ036dyyVm2yLrKqji58zNy84TPGiQhVCHAAXAh8pqWzUfXOfv57Xx6FnRHPutGfkOn7tsXLa4XXOwyuFCqWsGxcsgQZ3k0ZBBMaoHhbfg8onATe0WOt1gKzfx648RYwAbY8/wBggCvFTr33sr6NP91eKnYS6d/Pu+OTRWWcZllt0n7IKRmZSf5th4eWFxpthDcvJsq2jQlOfbmH3Nvb3RarmkOWAyKNGR4TRiSILom+EyT15qHFU0tFN9a5cIXOQCOoBQhQwKgBd4JflVf/iclq47qpYXunushpRLBvLuznK1vOMsISaSZo3MEPfXHKgKmIMC7RUrQRVnZGpb+u4VGWjFvlEZFOcyD3tjy0lxOzY3iWIiXZ9yDBAqEKAAeOHAaduZKflpcfS/355KD35ztPj45mNnySx4nXxZbZu4f8loW7+Js8vGKn0oB30fWP2/z4/Rkg8PUE+Ptc/zavZyzb19kDUsPc7tMg+XUnqbZI0LUOwbZTnY/cuGY+L9e75RbNhzAjALBCgAXpC9A+cOT6frpg2leRNs/RzbT9QZMrbrSnWj7UIcExlOyXGuq7qXKgHKjpN1LjebasVByNMfHBAX3uU7bE269lNFLCk2UvOae1f7UNwJUBrausR4tdEBisygcKD703/uJUuPleZPyKbLJ+YY9pwAzAIBCoAXqpQAJSclVr1IpsVHUVtXN+09VU9mIPd9ZCX3v05+WHq8mITh8tSGw2d89ntXNrSLDBN79qNDIjBgvFDt1/8+IO7/4KIRmtfcu+5DGTxAkYfxcXnHyFKK7FPizbG7yuopKSaSfnXNRMOeD4CZIEAB8EJlo2OAwuO7s0bY+jk2HTVHmUc2g2YPsu/jsnG+HzeuqLeVlhjvWnl+9WFx//eflogSy/CMeLrzohE++b3kqHGJG8vaNpTYgrA5xZlkJC4Nxkf3BkiPXDFW/bsEEOoQoAB4mSFgOcm9FxUZoGw+pv/iM1eqG3szKO4EKOsPVVOXUv7wFk+l2O85+fvmk/T+7gp6ZeMJ8f4vr5ngswyGnHpx3iviyoZDtgDl4tFDyEgc0BYrgdWMwjS6+dwCQ58PgJkgQAHwYQaFzVYmYrafrDVFH4rMoGQNkkGZOiyN0hOiqbHdInpofJlB4R6XyyfkiBLSfW/uFLfca3GJsiTOV3tFeP8cl5Hk9+xKVWO7KKlwVenCYmMDFPZfc4ro3KJ0+u23JmO1PYAdBCgAGvGps3ISxD5A4ZHXjIRoau/qod3lxvehuLvvg/e3XKSUPDb5aArpdIMtQOEdHz+/apxo1GWxUeH0xNXjyZd4K+7wzIRB+1Bkj83koSkiIDPagil59M4PZtOIIdh7AmAPAQqARmeaO4gnZ3mpVmZC78WfGz7VMo8J+lB6MyiDT6tMK0gTt1+fsh1Y561T9bYMU15KnGjElWPYD80b02dhnC/7UAaa5JEBitHlHQAYGAIUAC9HjLOTY/uk5meNSPdpJsI3PSiDN19Oyred/7KnvMEna+9P1/dmUNgPLh5JOx6fS/91oW8aY53Jfo4Dp10HKFxa+qKkRty/eAwCFAAzQ4AC4OWIcbaL5lPZh8J7RTos3WQkT86c4RN0udTDG1llf41WHODIHpTc1N7gKMPLpWwD4bIN21XmuoeGS27co8J7V6bkp/rteQCA9xCgAHiZQclNsWUHnBs2eTtqh6WHdpUa14fCTbo83utugMJ9HHJ5GGdRvMHNti2d3WqJRw/TCmxBB58p5GrhnJze4fHiyAj88wdgZvg/FEAjngaRJR5ntj6UdMPHjeVpwdwnkxYf7VEWYq+XAYrMnvDiuji7XR/+xNkZ3q3Cdpb1DQzRfwIQOBCgAHidQXHd2yHLPGsP6nsAX38TPO6OsKp9KF42ytpP8OhputLou/OkY5mnrqWT9ihTVRchQAEwPQQoABrJHo3sfgKUeeNzRD/H7vIGOl7TQoY2yHpw3sxkJUDZW17vVaOsnOBxVQLzp2mFaS4zKF8cqRFTVzzpo/dzAgDPIUAB8HKLbH8ZFM5aXDDKtlfk/V0VZGwGxf316WNykigqIozqWruovK53Vb3WCZ6hdg2yepiu9KFw74/9Ccqr99syWReNNna9PQC4BwEKgAacWVC3yA4wvnvtlDxx+69dp3wytqt5B8oga+7t8ep5eYjdXi/KPL0TPPpmKzhDwufbNHVYqKTadi5PS4dFDVCunJSr6/MBAG0QoABowNkFucZ+oIv//Ik5YmvqsZoWry72Wp1p8rzE47wPxdtzePTuQeHpHFmm+qrU1ofyyf5KccJ0YUY8TR2G8WKAQIAABcCL8g6vtB/osLvEmEiaOy5b3P+XAWWe6kb3zuHpd5LnVL3XGZQ8A07nlY2yXymNsu/ttL32104dKiasAMD8EKAAaFDZ2NbviLGz66YOFbcrd1eITaZmXXPvy42y/H3KMWy9MygOAUppnRi15gZZdt1UW8kNAMwPAQqABpUNHQM2yNrjkdbU+CgRLGzS+Wyealni8aAHhfGytujIcGpqt9DJs60e/74cFHR1W8UUk6fBka8Xtr2xpVQETFz2wYF8AIEDAQqAH0aM7fGFXjZmcrOsXviiXNMst8h6VmaJiggXa++17kM5pZR3spNiDNnYar+w7cX1R9XyDgAEDgQoABpUKkvIct0o8diXeT76upLau/Q5m6e2pVMEKdxykZno3hbZ/vaheOp0vXHlHecyDzfH8o66BVMwvQMQSBCgAGhQqTSfupNBYTML08ReFB593eViBbs/yzsZCdqyGJOVw/R40VygjBi7WtjGeB+Np1kkADAWAhQAbzIobgYovGb+nOFp6gnHZm6QlaaojbL16ki1uyrUNffGBQVyYRtDeQcg8CBAAfBizHigJW3OZhSmO4y+6rbm3sMGWWlUViKlJ0RTe1ePx+PGvSPGxmVQeGFbUWaCCCLnT7CNegNA4ECAAuCh1k4LNbZbxP0cD3Z8zFBKDjtK6xxWsPt/B4q2AIX3hZxXpO1EZnmQopE9KFzWWnXvHPrkgYsoKTbKsOcBANogQAHQmD1JiI7w6MLHUzExkeFU39olNsvqV+LRXmaZNcJ2IvPmY2e19aAYsKTNXkJMJIITgACFAAVAa3nHw4svjxtPUdas7zjpWUZCzx0o9s4bYcugbD9RR13d7vWh8JSSHG8eamAGBQACGwIUAA+phwRqyA6oZR4d+lC8bZJlo7OSKC0+Sozqunsujwzg+AwiXlAHAKAFAhQAD8n+ipxkz7MDPG7MtusRoCg9KEO8KPHw9NF5RZ6VeXoneOJw7g0AaIYABcBD8oyZnBTPMxPTlOVhx860iEVq/sLn55zxQQbFvsyz5bh7ZalSZTW+kRM8ABD4EKBAwPjHjnL67PAZQ59DXUunmknwZMRY4rHdEUMSxP2dpd5nUY7XtFBLh22iyF5DWxd1Kj0jvCDOG7JRdvuJ2kH7UOpbO+mFNSUOBw4CAGiBAAUCAjeVPrR8N/3X37ZTQ2uXLr8nN3van+R78mwLLXpxIx2uaqak2Ej6xtgsTb+ur8o8e8sb6Bu/W08X/3Y9fbD3tMNzlf0nKXFRFBsV4fU+Ee4lae3spr0DnMvDv/9P/7mXKhraxTk4d39jlFe/LwCENgQoEBDe2lombnmj6fu7/X/g3utbTtLYJz6iC59dRz9bsVeciLvwTxvFeDBPpvzzrvMpP812GJ1RjbJbT9SqJwf/6PWv6Pt/30Flta3iNZJlKF+cJMx9KOcOV8o8A+xDeXNrGX20r5KiIsLoDzdNp8SYSK9/bwAIXfgXBEyvucNC/957Wn3/ne3ldOvs4X79PdcdtJWSyuvaRHAiTRyaTP93+zmUpaG847xRdneZbYU8jx9rcbiySdyOzUmiI9XNtHp/lXiz582IsXOZ55P9VaK8ddclI/t8vqSqiZ5ctU/cf3j+GJR3AMBryKCA6a3aXSHKC5y54J/Oucxw4HSjX39PLuew+y4rpttnF1JxViJdPTmX3v7+bK+CEzYiM0GUTDosPbTfi+/jUJUtQLn30mJadd8cNTNjjw/J8wX7PhSLiz6UR/65R6zEv7A4k/5rzgif/J4AENqQQQHTe3u7rbxz6+xCkXX48OtKWr69nBYvGO+X34/X0JfW2iZRvjU9nwoytJVyBiqZTC9Io7UHq8UFf6qyvM3T58hZCzYmJ5FGZSWJslNbZ7dojuVm1vCwMNGU6wucpeF+Fm6+3XOqQTx/6XBVE+0srRfB4+9umCK+PwAAbyGDAqZWolz8IsLDaOH0oXTjzGHi4yt2lnt8wq67qpraRXYjMjzMb6fxnquccfN5SY2mrz9V30Ytnd0UHRFOhRm2qSAWFx0hAonMxBifBSeMgw7OjrB3vyp3+NyKnbaeoEvGZFG2l9klAAAJAQqY2tvbbNmTy8ZmiTNl+CKZnRxDda1dtOaAY7+Fr5xU9njkp8WJA+f8gb8ftunoWdFj4ynOWjAeWY7y03N09t3zCsTtiq9Oqc+ZMzn/UgKU66cN1eV5AEBoQIACpsUZkneVi9+3z7FlTjhgWDQ9X9x/Ryn9+Kv/pMAuM+Fro7ISxSgul2M2HDqjuf+ESy96mT0ig0YOSRCZG5k14UkiHivmsetLNY5dAwC4ggAFTIszJLxtlUdlLx49RP34DUqZZ8PhM+q5L/7IoHAA4S+8Av6b47PF/dX7Kz3++kPKBM9oHQMUfs43n1co7r+++aTYe8LZFHbVpFyv960AANhDgAKmxQEIu27aUIdSS1FmgtjL0WMlevy9r0WZwR8BSkG6/wIU9s3xOeKWm2XdPSnYOUDhJWp6WjQjXxwCeLCyib48clYsiJN/RgAAvoQABUyrrK6134vw41ePE/tDPj1QRS+sta1W95WTtbYSz3A/lngYjwVzI2tju4W2uXnODeNghs/yYaN1DlC4AffaKbZghDf7NnVYxPi3XOQGAOArCFDAtHhJmmxWdTY5P5Wevm6iuP+/n5b0WVCmFZctTtbYAqNCP5Z4GE8myWZZXoLmSY8M964kREeI4EBvt8yylXkqlW21107Nw2gxAPgcAhQwpe4eK1XU2wKUYf2UWrgXhZeosQff3kVHzzR7/fvydBBnBcLC+v99fam3D6XK4SydgRyqtH2fxdlJhgQGvCV2it3uFkzvAIA/IEABU+KzZLq6rWIXyUC7NR6/erwoL3BQ8cg/9nj9+55QJnj4pGI9mj4vLB4iejp4r8mB07a+EncnePTuP7EnA8PJ+SkiUAIA8DUEKGDq8k5eapwohfSHd4D8/qapIuPBh++VK30rWpWe1ae8Y79Ybc4o24TSJ/1M82w7UUsnamyBk/0ZPHpO8DjjrMnS704XbwAA/oAABUxJBhqu+k+c5abE0TnKAXwffe35yK6rDEphun8bZO3NsyvzOPv6VAPd+JdNdN2fvqRqpefjsAE7UFyNHF81OVeXMhgAhCYEKGBKZbVK/0maexfAyyfm+CRAUTMomfpdeC8blyWyRPsqGulgpePhga9vKSVuTalv7aKfrdhL7V3dahCl9wQPAICeEKBAwGdQ7AOUHaV1aqYhUDIoGYkxNH+CLYvy6saT6sdbOiz0/i7bIjQuYX16oJp++/Ehsf+Fx5MzE3131g4AgNkgQAHddVi6qaG1y70R43T3AhTuVeHJEs42fLxPexZFnmKsVw+KdPvs4eL2vZ2n1Nfm33tPi7XyvNH2J98cLT728hfHxe3o7ERRZgEACFYeBShLliyhc845h5KSkigrK4uuu+46OnTokMNj2tvb6e6776aMjAxKTEykRYsWUVWVY229tLSUrrrqKoqPjxe/zsMPP0wWi+cHpkFg+t7/baOLfruOypRgYKAlbe6WeNgVShblQzfLPJyh+H+fH6MzTR3i/ab2Lqpp7jQkQOHTjbmnpK2rWz1jSB6UeOM5w+iHF4+kKfkp6uONnOABADBdgLJhwwYRfGzevJlWr15NXV1dNG/ePGpp6Z0weOCBB2jlypW0fPly8fiKigpauHCh+vnu7m4RnHR2dtLGjRvp1VdfpWXLltHixYt9+52BaQ8A3HL8LDW0ddFrm3vLGfYs3T10WjljJ19DgLLleK04w2cwf1p/hH797wN075tf2Ra0Kf0nGQnRlBQbRXribMj3zrdlUf62+YRYZc9TSdyb8q3p+WLV/+9umCK25xo9wQMAYLoA5aOPPqLvfe97NGHCBJoyZYoILDgbsmPHDvH5hoYGevnll+m5556jSy+9lGbMmEGvvPKKCEQ4qGGffPIJ7d+/n1577TWaOnUqXXHFFfTUU0/R0qVLRdACwY1LKPLonLe3l4mmT2e8oZQXtUVFhImDAt1VmJFA43KTxde6cwDfp/urxe3mY7XiPBxZ3inQOXsiXTt1qFglzw3C97+9S3yMN81mKXtgeN/IczdOobnjsunqSXmGPEcAgIDoQeGAhKWn20Y8OVDhrMrcuXPVx4wdO5YKCgpo06ZN4n2+nTRpEmVn25oC2fz586mxsZH27dvn8vfp6OgQn7d/g8Bkv8+DJ1NW7q7od4KH17h7uin1CjeneXgxmlx4xpZ8eJCOVjfrcgbPQDtRvnOO7aTmA6dtf8e/c67tfenqyXn0/26fSSnx+mZ4AAACJkDp6emh+++/ny644AKaONF2JkplZSVFR0dTamrvGmzGwQh/Tj7GPjiRn5ef66/3JSUlRX0bNszxH23QjksJv/noIC16cSOtO2TLKPiTnJLhDbHs7y7KPHKCR8uODRmgfHGkhhrb+2/EXXfQ9r2Oz02mtPgoOlLdTK9sPKHLKcaDnXMjYzLeZntRsW2JGwBAqNEcoHAvytdff01vvfUW+dtjjz0msjXyrazM1jwI2nGG4fL//Yzm/+9n9OL6o6Lf4cmV+6lH1l/85LiSQeFzdKIjwmlPeQPtKqt3+5DAwXAZpDgrUazJf+ULW8AxUIDCy8buu6xY3Jd9K8N13IHijIMyeT4PN8dy7wkAQCjS9K/fPffcQ6tWraJ169ZRfn6++vGcnBzRR1Jf73jB4Ske/px8jPNUj3xfPsZZTEwMJScnO7yBd375/j46WNkk+jz4gpgUEymCh8+P1OiSQZlekEpXT84V9/+mZC76BijaAgUZcLy44Ygo5Tjjvpcvj9q+z0vHZtHN5xWKUV6pQMcdKK78ZtFkevZbk+meb4wy9HkAAARMgMKTDhycrFixgtauXUtFRUUOn+em2KioKFqzZo36MR5D5kba2bNni/f5du/evVRd3VtO4IkgDjrGjx/v/XcEbu0h4UZUtu6hS+il22bSohm2QPPvm/rPOvjCiRpb+aYoM4FuVQ6cW7XnNJ1tto362o8Ya8mgMA58eGy3vauHlnxwoM/nNx07Kz6XmxIrRnt5MubRy8eqn7cPVoyQGh9NN3KGSZnYAQAIReGelnV4+uaNN94Qu1C4Z4Tf2tpsP6Vyf8gdd9xBDz74oMiucNPsf/zHf4igZNasWeIxPJbMgcitt95Ku3fvpo8//pgef/xx8WtzpgT8r1IZ4eVTdLkRlclgYc3B6gH3k3iDMxcVDba/K8MzE2jqsFRxGm5nd4+Y6JFOeZlB4ZHdXy6YIHo5OPjZcuysy/LOJWOy1GVnvIn2R5eMpAfmjhabXQEAIIAClBdffFH0gFxyySWUm5urvr399tvqY55//nm6+uqrxYK2iy66SJRt3n33XfXzERERojzEtxy43HLLLXTbbbfRk08+6dvvDPolAwAOTuQFeuSQRLqwOFNsYn1ti+v9JN7iMV7+9bmcxLtG+Pe+dZYtMHpza6nof+kSO1DkOTzaMihsfF4y3XRugbj/y5X7xeixzALySLEs70j8XB65fCz9eK6tPAQAAAFW4nH1xrtRpNjYWLHTpLa2Vixw4+DEubeksLCQPvjgA2ptbaUzZ87Q7373O4qMjPTddwUDkn0ZvB7engwW3tnmej+JrxpkOXsiAyMem02KjRSjxTx5c7q+XexJ4fJGppeZjJ/MG0PJsZFiZPf51YfFAjie1uEeF/71LxiV4ZPvCwAAfA9F7hBUUW8r8cjyjnTZuGzxsbrWLlEa8dcOFA5Q7Hd/LJw2VM2i2B8S6OkOFGd8oN5D88eI+39cd4Qu//3n9MLaI+L9WSMyKD4aQTEAgFkhQAlBFf1kUHit+s2zbGWRv206IbJjvnRCWSVf5NSE+t3zbJmb1furaKcycqy1/8QZZ4Wevn6iCFY4eyIXw106BvtFAADMDAFKCJKNqs4BCvu2Mj3C+0k4YNDq/744Tt96cSPV2E3nyAwKr6S3NyYniWYUppGlx0ovfX7MqwkeZ1xK4jFinla688IiMVYdFxVB35zgeqQdAADMAQFKiDfJOuMJljvm2MbHn/r3fk29KNyQ+sLaEtp+sk49mdd+B4p9iUf6rtLQyuvvPT3F2B18xs3PrxpPnz9yKX10/4Uuv3cAADAPBCghhss2skm2v4s0LwjjNevcuPrSZ7aMhif2VTSogYY8E6ets1s9oZh3oDjjja7c0Cr5KoPiLCcltk8GBwAAzAcBSojhde4dlh7iIZrsFNdTMgkxkfTYlbbFZUvXH1F7Vtz1eUnvNlouFXHj68laW/aEgxA++8ZZbFQELZye7/cABQAAAgMClBCd4BmSGEMxkRH9Pu6aKXl07nDbNtanXWxjHchnh8+IWzmEw1kU2X9SZDdi7Ozm82xlHq0HBQIAQPBAgBJi+tuB4oyDiF9cM14EGf/ec5q+dPOMnpYOC31VWifu3zZ7uBqgHFdW3LvqP7E/6O/X102kp66d4PUOFAAACGwIUEKM2n/iRgllQl6KmIBhP3lnt3ra70C2HD8rThIelh5HP7h4hPjYjtI62nrctm5++CD9H7fMKqRblcAGAABCFwKUECP7SdydYvnpFWNpRGaCOFzw4eW7B92N8tlhW6ZlzqghlJsSJ87b4S9Zd8hW9hmeidINAAAMDgFKqC5pS4l16/HcMPvH704Xu1H4IMGXvzg+4ON5XT27qDhT3F4x0XHfyGAZFAAAAIYAJcT0t0V2sIP3nrh6vLj/m48O0m5l26urX5u3tXLfyvkjZYCS6/AYVyPGAAAAzhCgBBEuv7y1tZQ2HbX1e7hySp7D4+EY7y3nFdCVk3JEf8ldr+2gslpb06u9L5Tx4sn5qZSijBIXZMTThLxkcT81PopS46M9+n0BACA0IUAJIrvK6umn7+6lm17aLE7v7eFjge3wVli5et7TTao81bNk4WTRj1LR0E7f+evmPkHK507lHUmWeVDeAQAAdyFACUA89rv+ULXLAEX6/ZoSuvNv26mx3bbRlclNrvHREWL1u6f4a978/iwRpPA0EAcppcoBgPWtneoo8pxix4P4bp01nK6fNpTun1vs8e8JAAChCefNBxjOWtzz5lcUFRFOuxZ/k+Kje/8I955qELfnDE+j3eUNoqn1uj9+Sct/OFucsWPff9LfsrTBZCfHiiDlpr9upmM1LXTN0i8oIiyMziojyAnRETStINXha7jc8/y3p3rxXQMAQKhBBiXAbDtRK8Z2Oy09Yo28va+VAOWHF4+kf/xwNuWmxIog4tWNJzxa0uZOkPIWZ1KGJIgzd2RwwmWjn8wbI4InAAAAbyCDEmC2nbBtaWW8sXXWiAz1MD6eoGETh6aIIOLnV42je97YSW9uK6N7Lyv2eAfKQLKSY+ndu86njUfPipOHOVjhkWQAAABfwBUlwOw4Wave/+pkb8/J/tONxD2xQ5JiRHDC5o3PESvjzzR10Cf7quhUnQxQ3NuBMhieyLlykuMYMQAAgC8gFx9AuBH1cJUtS8J2ltapm11leWfS0BT187xc7aZzh4n7r20+SRUNvinxAAAA+BsClAAiD+Hjc26iI8JF70epMuorA5SJys4R6aZzC8TitE3HztKeMttjEKAAAIDZIUAJwP6T2SMyaMJQWyCy42SdwwQP95/Y42DksnHZ4n5Th8VnPSgAAAD+hADFDm9gfezdPfS3TbapF19xXpim1Q4lQJk5PJ2mF6SpWRVewFaiNMhOyncMUNits2wnEjOeLs5x8xweAAAAoyBAsXOsppne3FpG6w72XYKm1a9W7qMpv/pEBD2DnQQ8kA5LN+0qtzXFzixM6w1QTtbTwcom6u6xUkZCNOUoDbL25ozKpOEZtlOEs5NiMQYMAACmhyuVnSJlFfsJZTuqtzYcPkOvfHlClFYW/2sf3fvmTmpWyiye+vpUo9h9wkEIH7g3vdC2DO1gZSNtPX5WLe+4WsAWHh5GN59XqJ6NAwAAYHYYM7ZTNMQWoHDjaVd3j1eZhtZOC/18xV5xf0ZhmjgBeNWe07S/opGe+/ZUmjrMcdvqYLafqFV/LQ5CclPixCI2Xl//xpbSPhM8zm47v1BkYS4a7biGHgAAwIyQQbHD5Y+4qAhRLilXdoZoxYf18a/BDal/+89z6e0f9G52vW7pl/Sfy7aJoMVd25Vm2JnDbaUdJss8MuMzUWmcdSUmMoLuubRYnDQMAABgdghQnEohhUoJ5HhN774RT+0tb6CXvzgu7v/6uoliwypnPj6470JaOH2oGPtde7Carl36Jd3y/7aI/pQTNS39/nrcuyKndbhBVnI+88Z5ggcAACBQocTjhFe2c9PpsTMtdOlYz7+esy8/fXeP2Oq6YEoefWNslvq5tIRoeu7GqXTvpcX0x7VH6L1dp+iLIzXijRWkx4uJm9vPHy6WrEmcdalt6aSYyHCamNcbhHDQo/7a8VEYHwYAgKCBDIoTbkBlJ872n9EYyO7yetpX0UiJMZG0+Orx/f4e/3PjFFr7k4vp0cvH0qwR6RQVESZ6X57+4AB98/kN9NHXlWJ8+MsjNfT7T0vE100ZluoQuEzIS1Hf769BFgAAIBAhg+JkuDLJc3yAkstASqqa1PILn4szkMKMBLrrkpHijad7Vu2uoP9ZfZhOnm2lH762gyLDw8hit0OFx4XtcXDCjbFc/kF5BwAAggkCFBclHnaiRtuocYlyVs6orESPvo4zLt85t4CunpJHf15/lF76/Bh1WHooKymG5hRn0kXFQ1wezHfnhUXU2tlN35qRr+n5AgAAmBEClH4yKKfq20SJJTYqwqOvP3LGFqAUZyVp+v05UHlo/hi6Y04R1bd1iQVrA5VuLp+YK94AAACCCXpQnKQnRFNybGSfPpQdJ2vpm89toPWHqv2SQXHGDbXcq4K+EgAACEUIUJxwQFA0xBZc2I/+/t+XJ8R5N7wRlpe4udLSYRGZF1bsZYACAAAQyhCguFCk7ELh8V552N9GZRSYJ23+tavC5dcdVco7mYnRIgMCAAAA2iBAcaEo0zGDwmPDda1d6uf/uLaELC6yKL4q7wAAAIQ6BCguDM+U22RtAcrnR86I2wtGZYiFaLxafuWeCp83yAIAAIANAhQXRigZFDVAOWwr78wbn0P/deEIcf8Pa4+IrbGuMijF2cigAAAAeAMBygAZlJrmTqpubFfPweF9JLyGPjU+SqzCX+WURTlSbVvSNkppsgUAAABtEKC4kBQbRZmJti2w72wvo87uHnHOzYjMBLGn5L/mFKlZFD7Ij/HOFG6gZaOQQQEAAPAKApR+cDDC3thSqq6ZlztJOIuSEB1BR6qb1ewKl4O44pMSF0VDlOAGAAAAtEGAMkiZp6KhXS3v2GdYrlDWzv/zq1PilnekyP0nWK4GAADgHQQog4waM443LnA6qG/h9KHi9t97KkR554hySCAaZAEAALyHAKUfvGZempCXLFbg25tVlEF5KbHU2G6htQer1QzKSDTIAgAAeA0BihsBypxRQ/p8Pjw8jK6bZsuivPtVuehHYcXZ2IECAADgLQQo/SgUpwjb7l9o13/iqsyz/tAZdWcKzuABAADwnu3YXugjNiqC7rxwBJXXtdK5RekuHzMqK4mm5KfQ7vIG8T5P9uSmxOr8TAEAAIIPApQB/OzKcYM+5vppQ9UAZVR2EiZ4AAAAfAAlHi8tmJJHkeG2oATlHQAAAN9AgOKljMQYunRsljrtAwAAAN5DiccHliycJBppb5g5zOinAgAAEBQQoPgoi3Lr7OFGPw0AAICggRIPAAAAmA4CFAAAADAdBCgAAABgOghQAAAAwHQQoAAAAIDpIEABAAAA00GAAgAAAKaDAAUAAABMBwEKAAAAmA4CFAAAAAj8AOWzzz6jBQsWUF5eHoWFhdF7773n8Hmr1UqLFy+m3NxciouLo7lz51JJSYnDY2pra+nmm2+m5ORkSk1NpTvuuIOam5u9/24AAAAgNAOUlpYWmjJlCi1dutTl55999ll64YUX6M9//jNt2bKFEhISaP78+dTe3q4+hoOTffv20erVq2nVqlUi6Pn+97/v3XcCAAAAQSPMyikPrV8cFkYrVqyg6667TrzPvxRnVn7yk5/QQw89JD7W0NBA2dnZtGzZMvrOd75DBw4coPHjx9O2bdto5syZ4jEfffQRXXnllVReXi6+3llHR4d4kxobG2nYsGHi1+YsDAAAAJgfX79TUlLcun779DTj48ePU2VlpSjrSPxEzjvvPNq0aZMIUPiWyzoyOGH8+PDwcJFxuf766/v8ukuWLKFf/epXLr9RAAAACAzyuu1ObsSnAQoHJ4wzJvb4ffk5vs3KynJ8EpGRlJ6erj7G2WOPPUYPPvig+v6pU6dEFoazKAAAABBYmpqaRAJDtwDFX2JiYsSblJiYSGVlZZSUlCTKTL4ky0f866N8hNfDFbwmfeE16QuvSV94TfoKtdfEarWK4MRVO4dfA5ScnBxxW1VVJaZ4JH5/6tSp6mOqq6sdvs5isYjJHvn1g+FyUH5+PvkT/0UJhb8s7sLr0Rdek77wmvSF16QvvCah/ZqkDJI58cselKKiIhFkrFmzxiE65N6S2bNni/f5tr6+nnbs2KE+Zu3atdTT0yN6VQAAAAA8zqDwvpIjR444NMbu2rVL9JAUFBTQ/fffT7/+9a+puLhYBCxPPPGESOXISZ9x48bR5ZdfTnfeeacYRe7q6qJ77rlHNNC6k/IBAACA4OdxgLJ9+3b6xje+ob4vm1dvv/12MUr8yCOPiF0pvNeEMyVz5swRY8SxsbHq17z++usiKLnssstEuWbRokVid4oZcK/LL37xC4eel1CG16MvvCZ94TXpC69JX3hN+sJr4qc9KAAAAAD+gLN4AAAAwHQQoAAAAIDpIEABAAAA00GAAgAAAKaDAAUAAABMBwGKnaVLl9Lw4cPFSDQvjdu6dSuFCj6Q8ZxzzhHHB/BZSby35tChQw6PaW9vp7vvvpsyMjLEcQM8Hs5bgkPBM888I45V4D0/ofx68DlYt9xyi/ie4+LiaNKkSWL1gMRDgYsXLxabpPnzfBBoSUkJBavu7m6x64l3PvH3O3LkSHrqqaccDkIL9tfks88+owULFog9Vvz/yHvvvefweXe+f94kfvPNN4tNqnyY7B133CF2bgXja8K7vx599FHx/05CQoJ4zG233UYVFRVB/ZpogQBF8fbbb4udLjyP/tVXX9GUKVNo/vz5fdbyB6sNGzaIi+3mzZtp9erV4n+iefPmiZ020gMPPEArV66k5cuXi8fz/1ALFy6kYLdt2zb6y1/+QpMnT3b4eKi9HnV1dXTBBRdQVFQUffjhh7R//376n//5H0pLS1Mf8+yzz4qdRryEkTdI8z/A/P8RB3PB6De/+Q29+OKL9Mc//pEOHDgg3ufX4A9/+EPIvCb8bwT/e8k/4LnizvfPF+J9+/aJf3tWrVolLvC8SysYX5PW1lZxjeHAlm/fffdd8cPgNddc4/C4m4PsNdGE96CA1Xruueda7777bvX97u5ua15ennXJkiXWUFRdXc0/Alo3bNgg3q+vr7dGRUVZly9frj7mwIED4jGbNm2yBqumpiZrcXGxdfXq1daLL77Y+uMf/zhkX49HH33UOmfOnH4/39PTY83JybH+9re/VT/Gr1NMTIz1zTfftAajq666yvqf//mfDh9buHCh9eabbw7J14T//q9YsUJ9353vf//+/eLrtm3bpj7mww8/tIaFhVlPnTplDbbXxJWtW7eKx508eTIkXhN3IYNCRJ2dneJsIE49Srzhlt/ftGkThaKGhgZxy0cYMH59OKti/xqNHTtWHG8QzK8RZ5Wuuuoqh+87VF+P999/n2bOnEk33HCDKANOmzaNXnrpJYdjLyorKx1eEz4UjMulwfqanH/++eLsscOHD4v3d+/eTV988QVdccUVIfua2HPn++dbLmHw3y2JH8//BnPGJVT+veVSEL8ODK+JH04zDlQ1NTWilpydne3wcX7/4MGDFGr44EbuteB0/sSJE8XH+B+Z6Oho9X8g+9eIPxeM3nrrLZGC5RKPs1B8PY4dOybKGVwK/dnPfiZel/vuu0+8DnzUhfy+Xf1/FKyvyU9/+lNxICoHpxEREeLfkaefflqk51kovib23Pn++ZYDXnuRkZHih6NQeI241MU9KTfddJN6mnGovyYSAhRwmTX4+uuvxU+CoaqsrIx+/OMfi/qv/TlSoYwDV/6J7r//+7/F+5xB4b8n3FvAAUooeuedd8TZYm+88QZNmDBBHJzKwT03PobqawLu4yzsjTfeKBqJOfgHRyjxEFFmZqb46cd5AoPfz8nJoVDChzhyQ9a6desoPz9f/Ti/DlwK4wMgQ+E14hION0hPnz5d/OTCb9wIy81+fJ9/Agyl14PxFMb48eMdPsank5eWlor78vsOpf+PHn74YZFF4dPYeSrj1ltvFc3TPBUXqq+JPXe+f751HkawWCxiiiWYXyMZnJw8eVL8ICSzJ6H8mjhDgEIkUtQzZswQtWT7nxb5/dmzZ1Mo4Aieg5MVK1bQ2rVrxdikPX59eHrD/jXiznO+OAXja8Qnbe/du1f8RCzfOHvAqXt5P5ReD8YlP+fRc+69KCwsFPf57wz/42n/mnD5g2vmwfqa8EQG9wXY4x92+N+PUH1N7Lnz/fMtB/r8Q4HE/wbxa8i9KsEcnPC49aeffirG9u2F4mviktvttEHurbfeEp3ly5YtEx3U3//+962pqanWyspKayi46667rCkpKdb169dbT58+rb61traqj/nhD39oLSgosK5du9a6fft26+zZs8VbqLCf4gnF14MnDSIjI61PP/20taSkxPr6669b4+Pjra+99pr6mGeeeUb8f/Ovf/3LumfPHuu1115rLSoqsra1tVmD0e23324dOnSoddWqVdbjx49b3333XWtmZqb1kUceCZnXhCfddu7cKd74kvLcc8+J+3IixZ3v//LLL7dOmzbNumXLFusXX3whJuduuukmazC+Jp2dndZrrrnGmp+fb921a5fDv7cdHR1B+5pogQDFzh/+8AdxwYmOjhZjx5s3b7aGCv6fyNXbK6+8oj6G/0H50Y9+ZE1LSxMXpuuvv178TxWqAUoovh4rV660Tpw4UQTzY8eOtf71r391+DyPlT7xxBPW7Oxs8ZjLLrvMeujQIWuwamxsFH8n+N+N2NhY64gRI6w///nPHS40wf6arFu3zuW/HRy8ufv9nz17Vlx8ExMTrcnJydb/+I//EBf5YHxNOJDt799b/rpgfU20COP/uM6tAAAAABgDPSgAAABgOghQAAAAwHQQoAAAAIDpIEABAAAA00GAAgAAAKaDAAUAAABMBwEKAAAAmA4CFAAAADAdBCgAAABgOghQAAAAwHQQoAAAAACZzf8HNYZs6BS0LNkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(output).squeeze())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
