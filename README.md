# sequence-models-from-scratch

The motivation behind this repo is to get the intuition behind recurrent architectures and to understand the improvements introduced at each stage of their evolution from simple RNN cell to Transformers.

Worklog:
- Simple RNN Cell for name generation
    - trained on Turkish Names and Dinosaur Name lists
- Simple RNN Cell for time series-forecasting
    - trained on known Airline Passengers dataset for 1-to-1 and 12-to-1 forecasting
- LSTM Cell and Stacked LSTM for time series-forecasting
    - trained on known Airline Passengers dataset for 12-to-1 forecasting
- LSTM Cell and Stacked LSTM for sequence transduction
    - trained on Turkish-to-English sentence pairs from Tatoeba Project for neural machine translation

